/**
 * Embedded SpiteDB runtime for standalone CLI binaries.
 *
 * This file is auto-generated by scripts/embed-runtime.ts
 * DO NOT EDIT MANUALLY
 */

// SpiteDB runtime bundle (278.9 KB)
export const SPITEDB_JS = "// @bun\nvar __commonJS = (cb, mod) => () => (mod || cb((mod = { exports: {} }).exports, mod), mod.exports);\nvar __require = import.meta.require;\n\n// ../../node_modules/.bun/detect-libc@2.1.2/node_modules/detect-libc/lib/process.js\nvar require_process = __commonJS((exports, module) => {\n  var isLinux = () => process.platform === \"linux\";\n  var report = null;\n  var getReport = () => {\n    if (!report) {\n      if (isLinux() && process.report) {\n        const orig = process.report.excludeNetwork;\n        process.report.excludeNetwork = true;\n        report = process.report.getReport();\n        process.report.excludeNetwork = orig;\n      } else {\n        report = {};\n      }\n    }\n    return report;\n  };\n  module.exports = { isLinux, getReport };\n});\n\n// ../../node_modules/.bun/detect-libc@2.1.2/node_modules/detect-libc/lib/filesystem.js\nvar require_filesystem = __commonJS((exports, module) => {\n  var fs = __require(\"fs\");\n  var LDD_PATH = \"/usr/bin/ldd\";\n  var SELF_PATH = \"/proc/self/exe\";\n  var MAX_LENGTH = 2048;\n  var readFileSync = (path) => {\n    const fd = fs.openSync(path, \"r\");\n    const buffer = Buffer.alloc(MAX_LENGTH);\n    const bytesRead = fs.readSync(fd, buffer, 0, MAX_LENGTH, 0);\n    fs.close(fd, () => {});\n    return buffer.subarray(0, bytesRead);\n  };\n  var readFile = (path) => new Promise((resolve, reject) => {\n    fs.open(path, \"r\", (err, fd) => {\n      if (err) {\n        reject(err);\n      } else {\n        const buffer = Buffer.alloc(MAX_LENGTH);\n        fs.read(fd, buffer, 0, MAX_LENGTH, 0, (_, bytesRead) => {\n          resolve(buffer.subarray(0, bytesRead));\n          fs.close(fd, () => {});\n        });\n      }\n    });\n  });\n  module.exports = {\n    LDD_PATH,\n    SELF_PATH,\n    readFileSync,\n    readFile\n  };\n});\n\n// ../../node_modules/.bun/detect-libc@2.1.2/node_modules/detect-libc/lib/elf.js\nvar require_elf = __commonJS((exports, module) => {\n  var interpreterPath = (elf) => {\n    if (elf.length < 64) {\n      return null;\n    }\n    if (elf.readUInt32BE(0) !== 2135247942) {\n      return null;\n    }\n    if (elf.readUInt8(4) !== 2) {\n      return null;\n    }\n    if (elf.readUInt8(5) !== 1) {\n      return null;\n    }\n    const offset = elf.readUInt32LE(32);\n    const size = elf.readUInt16LE(54);\n    const count = elf.readUInt16LE(56);\n    for (let i = 0;i < count; i++) {\n      const headerOffset = offset + i * size;\n      const type = elf.readUInt32LE(headerOffset);\n      if (type === 3) {\n        const fileOffset = elf.readUInt32LE(headerOffset + 8);\n        const fileSize = elf.readUInt32LE(headerOffset + 32);\n        return elf.subarray(fileOffset, fileOffset + fileSize).toString().replace(/\\0.*$/g, \"\");\n      }\n    }\n    return null;\n  };\n  module.exports = {\n    interpreterPath\n  };\n});\n\n// ../../node_modules/.bun/detect-libc@2.1.2/node_modules/detect-libc/lib/detect-libc.js\nvar require_detect_libc = __commonJS((exports, module) => {\n  var childProcess = __require(\"child_process\");\n  var { isLinux, getReport } = require_process();\n  var { LDD_PATH, SELF_PATH, readFile, readFileSync } = require_filesystem();\n  var { interpreterPath } = require_elf();\n  var cachedFamilyInterpreter;\n  var cachedFamilyFilesystem;\n  var cachedVersionFilesystem;\n  var command = \"getconf GNU_LIBC_VERSION 2>&1 || true; ldd --version 2>&1 || true\";\n  var commandOut = \"\";\n  var safeCommand = () => {\n    if (!commandOut) {\n      return new Promise((resolve) => {\n        childProcess.exec(command, (err, out) => {\n          commandOut = err ? \" \" : out;\n          resolve(commandOut);\n        });\n      });\n    }\n    return commandOut;\n  };\n  var safeCommandSync = () => {\n    if (!commandOut) {\n      try {\n        commandOut = childProcess.execSync(command, { encoding: \"utf8\" });\n      } catch (_err) {\n        commandOut = \" \";\n      }\n    }\n    return commandOut;\n  };\n  var GLIBC = \"glibc\";\n  var RE_GLIBC_VERSION = /LIBC[a-z0-9 \\-).]*?(\\d+\\.\\d+)/i;\n  var MUSL = \"musl\";\n  var isFileMusl = (f) => f.includes(\"libc.musl-\") || f.includes(\"ld-musl-\");\n  var familyFromReport = () => {\n    const report = getReport();\n    if (report.header && report.header.glibcVersionRuntime) {\n      return GLIBC;\n    }\n    if (Array.isArray(report.sharedObjects)) {\n      if (report.sharedObjects.some(isFileMusl)) {\n        return MUSL;\n      }\n    }\n    return null;\n  };\n  var familyFromCommand = (out) => {\n    const [getconf, ldd1] = out.split(/[\\r\\n]+/);\n    if (getconf && getconf.includes(GLIBC)) {\n      return GLIBC;\n    }\n    if (ldd1 && ldd1.includes(MUSL)) {\n      return MUSL;\n    }\n    return null;\n  };\n  var familyFromInterpreterPath = (path) => {\n    if (path) {\n      if (path.includes(\"/ld-musl-\")) {\n        return MUSL;\n      } else if (path.includes(\"/ld-linux-\")) {\n        return GLIBC;\n      }\n    }\n    return null;\n  };\n  var getFamilyFromLddContent = (content) => {\n    content = content.toString();\n    if (content.includes(\"musl\")) {\n      return MUSL;\n    }\n    if (content.includes(\"GNU C Library\")) {\n      return GLIBC;\n    }\n    return null;\n  };\n  var familyFromFilesystem = async () => {\n    if (cachedFamilyFilesystem !== undefined) {\n      return cachedFamilyFilesystem;\n    }\n    cachedFamilyFilesystem = null;\n    try {\n      const lddContent = await readFile(LDD_PATH);\n      cachedFamilyFilesystem = getFamilyFromLddContent(lddContent);\n    } catch (e) {}\n    return cachedFamilyFilesystem;\n  };\n  var familyFromFilesystemSync = () => {\n    if (cachedFamilyFilesystem !== undefined) {\n      return cachedFamilyFilesystem;\n    }\n    cachedFamilyFilesystem = null;\n    try {\n      const lddContent = readFileSync(LDD_PATH);\n      cachedFamilyFilesystem = getFamilyFromLddContent(lddContent);\n    } catch (e) {}\n    return cachedFamilyFilesystem;\n  };\n  var familyFromInterpreter = async () => {\n    if (cachedFamilyInterpreter !== undefined) {\n      return cachedFamilyInterpreter;\n    }\n    cachedFamilyInterpreter = null;\n    try {\n      const selfContent = await readFile(SELF_PATH);\n      const path = interpreterPath(selfContent);\n      cachedFamilyInterpreter = familyFromInterpreterPath(path);\n    } catch (e) {}\n    return cachedFamilyInterpreter;\n  };\n  var familyFromInterpreterSync = () => {\n    if (cachedFamilyInterpreter !== undefined) {\n      return cachedFamilyInterpreter;\n    }\n    cachedFamilyInterpreter = null;\n    try {\n      const selfContent = readFileSync(SELF_PATH);\n      const path = interpreterPath(selfContent);\n      cachedFamilyInterpreter = familyFromInterpreterPath(path);\n    } catch (e) {}\n    return cachedFamilyInterpreter;\n  };\n  var family = async () => {\n    let family2 = null;\n    if (isLinux()) {\n      family2 = await familyFromInterpreter();\n      if (!family2) {\n        family2 = await familyFromFilesystem();\n        if (!family2) {\n          family2 = familyFromReport();\n        }\n        if (!family2) {\n          const out = await safeCommand();\n          family2 = familyFromCommand(out);\n        }\n      }\n    }\n    return family2;\n  };\n  var familySync = () => {\n    let family2 = null;\n    if (isLinux()) {\n      family2 = familyFromInterpreterSync();\n      if (!family2) {\n        family2 = familyFromFilesystemSync();\n        if (!family2) {\n          family2 = familyFromReport();\n        }\n        if (!family2) {\n          const out = safeCommandSync();\n          family2 = familyFromCommand(out);\n        }\n      }\n    }\n    return family2;\n  };\n  var isNonGlibcLinux = async () => isLinux() && await family() !== GLIBC;\n  var isNonGlibcLinuxSync = () => isLinux() && familySync() !== GLIBC;\n  var versionFromFilesystem = async () => {\n    if (cachedVersionFilesystem !== undefined) {\n      return cachedVersionFilesystem;\n    }\n    cachedVersionFilesystem = null;\n    try {\n      const lddContent = await readFile(LDD_PATH);\n      const versionMatch = lddContent.match(RE_GLIBC_VERSION);\n      if (versionMatch) {\n        cachedVersionFilesystem = versionMatch[1];\n      }\n    } catch (e) {}\n    return cachedVersionFilesystem;\n  };\n  var versionFromFilesystemSync = () => {\n    if (cachedVersionFilesystem !== undefined) {\n      return cachedVersionFilesystem;\n    }\n    cachedVersionFilesystem = null;\n    try {\n      const lddContent = readFileSync(LDD_PATH);\n      const versionMatch = lddContent.match(RE_GLIBC_VERSION);\n      if (versionMatch) {\n        cachedVersionFilesystem = versionMatch[1];\n      }\n    } catch (e) {}\n    return cachedVersionFilesystem;\n  };\n  var versionFromReport = () => {\n    const report = getReport();\n    if (report.header && report.header.glibcVersionRuntime) {\n      return report.header.glibcVersionRuntime;\n    }\n    return null;\n  };\n  var versionSuffix = (s) => s.trim().split(/\\s+/)[1];\n  var versionFromCommand = (out) => {\n    const [getconf, ldd1, ldd2] = out.split(/[\\r\\n]+/);\n    if (getconf && getconf.includes(GLIBC)) {\n      return versionSuffix(getconf);\n    }\n    if (ldd1 && ldd2 && ldd1.includes(MUSL)) {\n      return versionSuffix(ldd2);\n    }\n    return null;\n  };\n  var version = async () => {\n    let version2 = null;\n    if (isLinux()) {\n      version2 = await versionFromFilesystem();\n      if (!version2) {\n        version2 = versionFromReport();\n      }\n      if (!version2) {\n        const out = await safeCommand();\n        version2 = versionFromCommand(out);\n      }\n    }\n    return version2;\n  };\n  var versionSync = () => {\n    let version2 = null;\n    if (isLinux()) {\n      version2 = versionFromFilesystemSync();\n      if (!version2) {\n        version2 = versionFromReport();\n      }\n      if (!version2) {\n        const out = safeCommandSync();\n        version2 = versionFromCommand(out);\n      }\n    }\n    return version2;\n  };\n  module.exports = {\n    GLIBC,\n    MUSL,\n    family,\n    familySync,\n    isNonGlibcLinux,\n    isNonGlibcLinuxSync,\n    version,\n    versionSync\n  };\n});\n\n// ../../node_modules/.bun/node-gyp-build-optional-packages@5.2.2/node_modules/node-gyp-build-optional-packages/node-gyp-build.js\nvar require_node_gyp_build = __commonJS((exports, module) => {\n  var fs = __require(\"fs\");\n  var path = __require(\"path\");\n  var url = __require(\"url\");\n  var os = __require(\"os\");\n  var runtimeRequire = typeof __webpack_require__ === \"function\" ? __non_webpack_require__ : __require;\n  var vars = process.config && process.config.variables || {};\n  var prebuildsOnly = !!process.env.PREBUILDS_ONLY;\n  var versions = process.versions;\n  var abi = versions.modules;\n  if (versions.deno || process.isBun) {\n    abi = \"unsupported\";\n  }\n  var runtime = isElectron() ? \"electron\" : isNwjs() ? \"node-webkit\" : \"node\";\n  var arch = process.env.npm_config_arch || os.arch();\n  var platform = process.env.npm_config_platform || os.platform();\n  var libc2 = process.env.LIBC || (isMusl(platform) ? \"musl\" : \"glibc\");\n  var armv = process.env.ARM_VERSION || (arch === \"arm64\" ? \"8\" : vars.arm_version) || \"\";\n  var uv = (versions.uv || \"\").split(\".\")[0];\n  module.exports = load;\n  function load(dir) {\n    return runtimeRequire(load.resolve(dir));\n  }\n  load.resolve = load.path = function(dir) {\n    dir = path.resolve(dir || \".\");\n    var packageName = \"\";\n    var packageNameError;\n    try {\n      packageName = runtimeRequire(path.join(dir, \"package.json\")).name;\n      var varName = packageName.toUpperCase().replace(/-/g, \"_\");\n      if (process.env[varName + \"_PREBUILD\"])\n        dir = process.env[varName + \"_PREBUILD\"];\n    } catch (err) {\n      packageNameError = err;\n    }\n    if (!prebuildsOnly) {\n      var release = getFirst(path.join(dir, \"build/Release\"), matchBuild);\n      if (release)\n        return release;\n      var debug = getFirst(path.join(dir, \"build/Debug\"), matchBuild);\n      if (debug)\n        return debug;\n    }\n    var prebuild = resolve(dir);\n    if (prebuild)\n      return prebuild;\n    var nearby = resolve(path.dirname(process.execPath));\n    if (nearby)\n      return nearby;\n    var platformPackage = (packageName[0] == \"@\" ? \"\" : \"@\" + packageName + \"/\") + packageName + \"-\" + platform + \"-\" + arch;\n    var packageResolutionError;\n    try {\n      var prebuildPackage = path.dirname(__require(\"module\").createRequire(url.pathToFileURL(path.join(dir, \"package.json\"))).resolve(platformPackage));\n      return resolveFile(prebuildPackage);\n    } catch (error) {\n      packageResolutionError = error;\n    }\n    var target2 = [\n      \"platform=\" + platform,\n      \"arch=\" + arch,\n      \"runtime=\" + runtime,\n      \"abi=\" + abi,\n      \"uv=\" + uv,\n      armv ? \"armv=\" + armv : \"\",\n      \"libc=\" + libc2,\n      \"node=\" + process.versions.node,\n      process.versions.electron ? \"electron=\" + process.versions.electron : \"\",\n      typeof __webpack_require__ === \"function\" ? \"webpack=true\" : \"\"\n    ].filter(Boolean).join(\" \");\n    let errMessage = \"No native build was found for \" + target2 + `\n    attempted loading from: ` + dir + \" and package:\" + \" \" + platformPackage + `\n`;\n    if (packageNameError) {\n      errMessage += \"Error finding package.json: \" + packageNameError.message + `\n`;\n    }\n    if (packageResolutionError) {\n      errMessage += \"Error resolving package: \" + packageResolutionError.message + `\n`;\n    }\n    throw new Error(errMessage);\n    function resolve(dir2) {\n      var tuples = readdirSync2(path.join(dir2, \"prebuilds\")).map(parseTuple);\n      var tuple = tuples.filter(matchTuple(platform, arch)).sort(compareTuples)[0];\n      if (!tuple)\n        return;\n      return resolveFile(path.join(dir2, \"prebuilds\", tuple.name));\n    }\n    function resolveFile(prebuilds) {\n      var parsed = readdirSync2(prebuilds).map(parseTags);\n      var candidates = parsed.filter(matchTags(runtime, abi));\n      var winner = candidates.sort(compareTags(runtime))[0];\n      if (winner)\n        return path.join(prebuilds, winner.file);\n    }\n  };\n  function readdirSync2(dir) {\n    try {\n      return fs.readdirSync(dir);\n    } catch (err) {\n      return [];\n    }\n  }\n  function getFirst(dir, filter) {\n    var files = readdirSync2(dir).filter(filter);\n    return files[0] && path.join(dir, files[0]);\n  }\n  function matchBuild(name) {\n    return /\\.node$/.test(name);\n  }\n  function parseTuple(name) {\n    var arr = name.split(\"-\");\n    if (arr.length !== 2)\n      return;\n    var platform2 = arr[0];\n    var architectures = arr[1].split(\"+\");\n    if (!platform2)\n      return;\n    if (!architectures.length)\n      return;\n    if (!architectures.every(Boolean))\n      return;\n    return { name, platform: platform2, architectures };\n  }\n  function matchTuple(platform2, arch2) {\n    return function(tuple) {\n      if (tuple == null)\n        return false;\n      if (tuple.platform !== platform2)\n        return false;\n      return tuple.architectures.includes(arch2);\n    };\n  }\n  function compareTuples(a, b) {\n    return a.architectures.length - b.architectures.length;\n  }\n  function parseTags(file) {\n    var arr = file.split(\".\");\n    var extension = arr.pop();\n    var tags = { file, specificity: 0 };\n    if (extension !== \"node\")\n      return;\n    for (var i = 0;i < arr.length; i++) {\n      var tag = arr[i];\n      if (tag === \"node\" || tag === \"electron\" || tag === \"node-webkit\") {\n        tags.runtime = tag;\n      } else if (tag === \"napi\") {\n        tags.napi = true;\n      } else if (tag.slice(0, 3) === \"abi\") {\n        tags.abi = tag.slice(3);\n      } else if (tag.slice(0, 2) === \"uv\") {\n        tags.uv = tag.slice(2);\n      } else if (tag.slice(0, 4) === \"armv\") {\n        tags.armv = tag.slice(4);\n      } else if (tag === \"glibc\" || tag === \"musl\") {\n        tags.libc = tag;\n      } else {\n        continue;\n      }\n      tags.specificity++;\n    }\n    return tags;\n  }\n  function matchTags(runtime2, abi2) {\n    return function(tags) {\n      if (tags == null)\n        return false;\n      if (tags.runtime !== runtime2 && !runtimeAgnostic(tags))\n        return false;\n      if (tags.abi !== abi2 && !tags.napi)\n        return false;\n      if (tags.uv && tags.uv !== uv)\n        return false;\n      if (tags.armv && tags.armv !== armv)\n        return false;\n      if (tags.libc && tags.libc !== libc2)\n        return false;\n      return true;\n    };\n  }\n  function runtimeAgnostic(tags) {\n    return tags.runtime === \"node\" && tags.napi;\n  }\n  function compareTags(runtime2) {\n    return function(a, b) {\n      if (a.runtime !== b.runtime) {\n        return a.runtime === runtime2 ? -1 : 1;\n      } else if (a.abi !== b.abi) {\n        return a.abi ? -1 : 1;\n      } else if (a.specificity !== b.specificity) {\n        return a.specificity > b.specificity ? -1 : 1;\n      } else {\n        return 0;\n      }\n    };\n  }\n  function isNwjs() {\n    return !!(process.versions && process.versions.nw);\n  }\n  function isElectron() {\n    if (process.versions && process.versions.electron)\n      return true;\n    if (process.env.ELECTRON_RUN_AS_NODE)\n      return true;\n    return typeof window !== \"undefined\" && window.process && window.process.type === \"renderer\";\n  }\n  function isMusl(platform2) {\n    if (platform2 !== \"linux\")\n      return false;\n    const { familySync, MUSL } = require_detect_libc();\n    return familySync() === MUSL;\n  }\n  load.parseTags = parseTags;\n  load.matchTags = matchTags;\n  load.compareTags = compareTags;\n  load.parseTuple = parseTuple;\n  load.matchTuple = matchTuple;\n  load.compareTuples = compareTuples;\n});\n\n// ../../node_modules/.bun/node-gyp-build-optional-packages@5.2.2/node_modules/node-gyp-build-optional-packages/index.js\nvar require_node_gyp_build_optional_packages = __commonJS((exports, module) => {\n  var runtimeRequire = typeof __webpack_require__ === \"function\" ? __non_webpack_require__ : __require;\n  if (typeof runtimeRequire.addon === \"function\") {\n    module.exports = runtimeRequire.addon.bind(runtimeRequire);\n  } else {\n    module.exports = require_node_gyp_build();\n  }\n});\n\n// ../../node_modules/.bun/msgpackr-extract@3.0.3/node_modules/msgpackr-extract/index.js\nvar require_msgpackr_extract = __commonJS((exports, module) => {\n  var __dirname = \"/Users/ryanwible/projects/spitestack/node_modules/.bun/msgpackr-extract@3.0.3/node_modules/msgpackr-extract\";\n  module.exports = require_node_gyp_build_optional_packages()(__dirname);\n});\n\n// src/infrastructure/storage/segments/segment-header.ts\nvar SEGMENT_MAGIC = 1397770580;\nvar SEGMENT_VERSION = 1;\nvar SEGMENT_HEADER_SIZE = 32;\n\nclass InvalidSegmentHeaderError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"InvalidSegmentHeaderError\";\n  }\n}\nfunction encodeSegmentHeader(header) {\n  if (!Number.isSafeInteger(header.basePosition) || header.basePosition < 0) {\n    throw new InvalidSegmentHeaderError(\"Base position must be a non-negative safe integer\");\n  }\n  const buffer = new ArrayBuffer(SEGMENT_HEADER_SIZE);\n  const view = new DataView(buffer);\n  view.setUint32(0, SEGMENT_MAGIC, false);\n  view.setUint8(4, SEGMENT_VERSION);\n  view.setUint8(5, header.flags);\n  view.setUint16(6, 0, false);\n  view.setBigUint64(8, header.segmentId, false);\n  view.setBigUint64(16, BigInt(header.basePosition), false);\n  return new Uint8Array(buffer);\n}\nfunction decodeSegmentHeader(data) {\n  if (data.length < SEGMENT_HEADER_SIZE) {\n    throw new InvalidSegmentHeaderError(`Header too short: expected ${SEGMENT_HEADER_SIZE} bytes, got ${data.length}`);\n  }\n  const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n  const magic = view.getUint32(0, false);\n  if (magic !== SEGMENT_MAGIC) {\n    throw new InvalidSegmentHeaderError(`Invalid magic number: expected 0x${SEGMENT_MAGIC.toString(16).toUpperCase()}, got 0x${magic.toString(16).toUpperCase()}`);\n  }\n  const version = view.getUint8(4);\n  if (version !== SEGMENT_VERSION) {\n    throw new InvalidSegmentHeaderError(`Unsupported version: expected ${SEGMENT_VERSION}, got ${version}`);\n  }\n  const basePosition = Number(view.getBigUint64(16, false));\n  if (!Number.isSafeInteger(basePosition)) {\n    throw new InvalidSegmentHeaderError(\"Base position exceeds safe integer range\");\n  }\n  return {\n    magic,\n    version,\n    flags: view.getUint8(5),\n    segmentId: view.getBigUint64(8, false),\n    basePosition\n  };\n}\nfunction isValidSegmentHeader(data) {\n  if (data.length < SEGMENT_HEADER_SIZE) {\n    return false;\n  }\n  const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n  const magic = view.getUint32(0, false);\n  if (magic !== SEGMENT_MAGIC) {\n    return false;\n  }\n  const version = view.getUint8(4);\n  if (version !== SEGMENT_VERSION) {\n    return false;\n  }\n  return true;\n}\n\n// src/infrastructure/storage/support/crc32.ts\nvar CRC32_TABLE = new Uint32Array(256);\nvar POLYNOMIAL = 3988292384;\nfor (let i = 0;i < 256; i++) {\n  let crc = i;\n  for (let j = 0;j < 8; j++) {\n    crc = crc & 1 ? crc >>> 1 ^ POLYNOMIAL : crc >>> 1;\n  }\n  CRC32_TABLE[i] = crc;\n}\nfunction crc32(data, initial = 4294967295) {\n  let crc = initial;\n  for (let i = 0;i < data.length; i++) {\n    const tableIndex = (crc ^ data[i]) & 255;\n    crc = CRC32_TABLE[tableIndex] ^ crc >>> 8;\n  }\n  return (crc ^ 4294967295) >>> 0;\n}\n\nclass CRC32Calculator {\n  crc = 4294967295;\n  update(data) {\n    for (let i = 0;i < data.length; i++) {\n      const tableIndex = (this.crc ^ data[i]) & 255;\n      this.crc = CRC32_TABLE[tableIndex] ^ this.crc >>> 8;\n    }\n  }\n  finalize() {\n    const result = (this.crc ^ 4294967295) >>> 0;\n    this.crc = 4294967295;\n    return result;\n  }\n  reset() {\n    this.crc = 4294967295;\n  }\n}\n\n// src/infrastructure/storage/batch/batch-record.ts\nvar BATCH_MAGIC = 1111577667;\nvar BATCH_HEADER_SIZE = 28;\n\nclass InvalidBatchError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"InvalidBatchError\";\n  }\n}\n\nclass BatchChecksumError extends InvalidBatchError {\n  constructor(expected, actual) {\n    super(`Checksum mismatch: expected 0x${expected.toString(16).padStart(8, \"0\")}, ` + `got 0x${actual.toString(16).padStart(8, \"0\")}`);\n    this.name = \"BatchChecksumError\";\n  }\n}\nfunction encodeBatchRecord(batchId, eventCount, compressedPayload, uncompressedLength) {\n  const totalSize = BATCH_HEADER_SIZE + compressedPayload.length;\n  const buffer = new ArrayBuffer(totalSize);\n  const view = new DataView(buffer);\n  const bytes = new Uint8Array(buffer);\n  view.setUint32(0, BATCH_MAGIC, false);\n  view.setUint32(8, compressedPayload.length, false);\n  view.setUint32(12, uncompressedLength, false);\n  view.setBigUint64(16, batchId, false);\n  view.setUint32(24, eventCount, false);\n  bytes.set(compressedPayload, BATCH_HEADER_SIZE);\n  const checksumData = bytes.subarray(8);\n  const checksum = crc32(checksumData);\n  view.setUint32(4, checksum, false);\n  return bytes;\n}\nfunction decodeBatchHeader(data, validateChecksum = true) {\n  if (data.length < BATCH_HEADER_SIZE) {\n    throw new InvalidBatchError(`Header too short: expected at least ${BATCH_HEADER_SIZE} bytes, got ${data.length}`);\n  }\n  const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n  const magic = view.getUint32(0, false);\n  if (magic !== BATCH_MAGIC) {\n    throw new InvalidBatchError(`Invalid magic number: expected 0x${BATCH_MAGIC.toString(16).toUpperCase()}, ` + `got 0x${magic.toString(16).toUpperCase()}`);\n  }\n  const storedChecksum = view.getUint32(4, false);\n  const compressedLength = view.getUint32(8, false);\n  const uncompressedLength = view.getUint32(12, false);\n  const batchId = view.getBigUint64(16, false);\n  const eventCount = view.getUint32(24, false);\n  if (validateChecksum) {\n    const expectedSize = BATCH_HEADER_SIZE + compressedLength;\n    if (data.length < expectedSize) {\n      throw new InvalidBatchError(`Incomplete batch: expected ${expectedSize} bytes, got ${data.length}`);\n    }\n    const checksumData = data.subarray(8, expectedSize);\n    const computedChecksum = crc32(checksumData);\n    if (computedChecksum !== storedChecksum) {\n      throw new BatchChecksumError(storedChecksum, computedChecksum);\n    }\n  }\n  return {\n    magic,\n    checksum: storedChecksum,\n    compressedLength,\n    uncompressedLength,\n    batchId,\n    eventCount\n  };\n}\nfunction extractBatchPayload(data, header) {\n  const h = header ?? decodeBatchHeader(data, false);\n  return data.subarray(BATCH_HEADER_SIZE, BATCH_HEADER_SIZE + h.compressedLength);\n}\nfunction isValidBatchMagic(data) {\n  if (data.length < 4) {\n    return false;\n  }\n  const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n  return view.getUint32(0, false) === BATCH_MAGIC;\n}\n\n// src/infrastructure/storage/segments/segment-index-file.ts\nfunction compareStreamIds(a, b) {\n  const minLen = Math.min(a.length, b.length);\n  for (let i = 0;i < minLen; i++) {\n    const diff = a.charCodeAt(i) - b.charCodeAt(i);\n    if (diff !== 0)\n      return diff;\n  }\n  return a.length - b.length;\n}\nvar INDEX_MAGIC = 1229215811;\nvar INDEX_VERSION = 1;\nvar INDEX_HEADER_SIZE = 32;\nvar INDEX_ENTRY_SIZE = 24;\n\nclass IndexCorruptedError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"IndexCorruptedError\";\n  }\n}\n\nclass SegmentIndexFile {\n  data = null;\n  header = null;\n  entries = [];\n  entriesByGlobalPosition = null;\n  stringTable = new Map;\n  loaded = false;\n  async load(fs, path) {\n    try {\n      this.data = await fs.mmap(path);\n    } catch {\n      this.data = await fs.readFile(path);\n    }\n    this.parse();\n  }\n  async loadIntoMemory(fs, path) {\n    this.data = await fs.readFile(path);\n    this.parse();\n  }\n  parse() {\n    if (!this.data) {\n      throw new Error(\"No data loaded\");\n    }\n    if (this.data.length < INDEX_HEADER_SIZE) {\n      throw new IndexCorruptedError(\"File too small for header\");\n    }\n    const view = new DataView(this.data.buffer, this.data.byteOffset, this.data.byteLength);\n    this.header = {\n      magic: view.getUint32(0, true),\n      version: view.getUint32(4, true),\n      segmentId: view.getBigUint64(8, true),\n      entryCount: view.getUint32(16, true),\n      stringTableOffset: view.getUint32(20, true),\n      checksum: view.getUint32(24, true)\n    };\n    if (this.header.magic !== INDEX_MAGIC) {\n      throw new IndexCorruptedError(`Invalid magic: expected 0x${INDEX_MAGIC.toString(16)}, got 0x${this.header.magic.toString(16)}`);\n    }\n    if (this.header.version !== INDEX_VERSION) {\n      throw new IndexCorruptedError(`Unsupported version: expected ${INDEX_VERSION}, got ${this.header.version}`);\n    }\n    if (!this.validateChecksum()) {\n      throw new IndexCorruptedError(\"Checksum mismatch\");\n    }\n    this.parseStringTable();\n    this.parseEntries();\n    this.entriesByGlobalPosition = null;\n    this.loaded = true;\n  }\n  parseStringTable() {\n    if (!this.data || !this.header)\n      return;\n    const decoder = new TextDecoder;\n    let offset = this.header.stringTableOffset;\n    while (offset < this.data.length) {\n      const start = offset;\n      while (offset < this.data.length && this.data[offset] !== 0) {\n        offset++;\n      }\n      const str = decoder.decode(this.data.subarray(start, offset));\n      this.stringTable.set(start - this.header.stringTableOffset, str);\n      offset++;\n    }\n  }\n  parseEntries() {\n    if (!this.data || !this.header)\n      return;\n    const view = new DataView(this.data.buffer, this.data.byteOffset, this.data.byteLength);\n    const entriesStart = INDEX_HEADER_SIZE;\n    this.entries = [];\n    for (let i = 0;i < this.header.entryCount; i++) {\n      const entryOffset = entriesStart + i * INDEX_ENTRY_SIZE;\n      const stringOffset = view.getUint32(entryOffset, true);\n      const revision = view.getUint32(entryOffset + 4, true);\n      const globalPosition = Number(view.getBigUint64(entryOffset + 8, true));\n      if (!Number.isSafeInteger(globalPosition)) {\n        throw new IndexCorruptedError(\"Global position exceeds safe integer range\");\n      }\n      const batchOffset = view.getUint32(entryOffset + 16, true);\n      const streamId = this.stringTable.get(stringOffset);\n      if (streamId === undefined) {\n        throw new IndexCorruptedError(`Invalid string table offset ${stringOffset} at entry ${i}`);\n      }\n      this.entries.push({\n        streamId,\n        revision,\n        globalPosition,\n        batchOffset\n      });\n    }\n  }\n  validateChecksum() {\n    if (!this.data || !this.header)\n      return false;\n    const before = this.data.subarray(0, 24);\n    const after = this.data.subarray(28);\n    const combined = new Uint8Array(before.length + after.length);\n    combined.set(before);\n    combined.set(after, before.length);\n    const calculated = crc32(combined);\n    return calculated === this.header.checksum;\n  }\n  isLoaded() {\n    return this.loaded;\n  }\n  getSegmentId() {\n    return this.header?.segmentId ?? 0n;\n  }\n  getEntryCount() {\n    return this.entries.length;\n  }\n  getAllEntries() {\n    return [...this.entries];\n  }\n  findBatchOffsetForGlobalPosition(position) {\n    if (this.entries.length === 0) {\n      return null;\n    }\n    const sorted = this.getEntriesSortedByGlobalPosition();\n    let left = 0;\n    let right = sorted.length - 1;\n    let matchIndex = sorted.length;\n    while (left <= right) {\n      const mid = Math.floor((left + right) / 2);\n      const entry = sorted[mid];\n      if (entry.globalPosition >= position) {\n        matchIndex = mid;\n        right = mid - 1;\n      } else {\n        left = mid + 1;\n      }\n    }\n    if (matchIndex >= sorted.length) {\n      return null;\n    }\n    return sorted[matchIndex].batchOffset;\n  }\n  getAllStreamIds() {\n    const ids = new Set;\n    for (const entry of this.entries) {\n      ids.add(entry.streamId);\n    }\n    return ids;\n  }\n  findByStream(streamId, fromRevision, toRevision) {\n    let left = 0;\n    let right = this.entries.length - 1;\n    let firstIndex = -1;\n    while (left <= right) {\n      const mid = Math.floor((left + right) / 2);\n      const entry = this.entries[mid];\n      const cmp = compareStreamIds(entry.streamId, streamId);\n      if (cmp < 0) {\n        left = mid + 1;\n      } else if (cmp > 0) {\n        right = mid - 1;\n      } else {\n        firstIndex = mid;\n        right = mid - 1;\n      }\n    }\n    if (firstIndex === -1) {\n      return [];\n    }\n    const results = [];\n    for (let i = firstIndex;i < this.entries.length; i++) {\n      const entry = this.entries[i];\n      if (entry.streamId !== streamId)\n        break;\n      if (fromRevision !== undefined && entry.revision < fromRevision)\n        continue;\n      if (toRevision !== undefined && entry.revision > toRevision)\n        continue;\n      results.push(entry);\n    }\n    return results;\n  }\n  findByGlobalPosition(position) {\n    return this.entries.find((e) => e.globalPosition === position);\n  }\n  getStreamMaxRevision(streamId) {\n    const entries = this.findByStream(streamId);\n    if (entries.length === 0)\n      return -1;\n    return Math.max(...entries.map((e) => e.revision));\n  }\n  getEntriesSortedByGlobalPosition() {\n    if (this.entriesByGlobalPosition) {\n      return this.entriesByGlobalPosition;\n    }\n    this.entriesByGlobalPosition = [...this.entries].sort((a, b) => a.globalPosition < b.globalPosition ? -1 : a.globalPosition > b.globalPosition ? 1 : 0);\n    return this.entriesByGlobalPosition;\n  }\n  static async write(fs, path, segmentId, entries) {\n    const encoder = new TextEncoder;\n    const sorted = [...entries].sort((a, b) => {\n      const cmp = compareStreamIds(a.streamId, b.streamId);\n      if (cmp !== 0)\n        return cmp;\n      return a.revision - b.revision;\n    });\n    const stringTable = new Map;\n    const encodedStrings = [];\n    let stringOffset = 0;\n    for (const entry of sorted) {\n      if (!stringTable.has(entry.streamId)) {\n        stringTable.set(entry.streamId, stringOffset);\n        const encoded = encoder.encode(entry.streamId);\n        encodedStrings.push(encoded);\n        stringOffset += encoded.length + 1;\n      }\n    }\n    const entriesSize = sorted.length * INDEX_ENTRY_SIZE;\n    const stringTableStart = INDEX_HEADER_SIZE + entriesSize;\n    const totalSize = stringTableStart + stringOffset;\n    const buffer = new Uint8Array(totalSize);\n    const view = new DataView(buffer.buffer);\n    view.setUint32(0, INDEX_MAGIC, true);\n    view.setUint32(4, INDEX_VERSION, true);\n    view.setBigUint64(8, segmentId, true);\n    view.setUint32(16, sorted.length, true);\n    view.setUint32(20, stringTableStart, true);\n    view.setUint32(24, 0, true);\n    view.setUint32(28, 0, true);\n    for (let i = 0;i < sorted.length; i++) {\n      const entry = sorted[i];\n      if (!Number.isSafeInteger(entry.globalPosition) || entry.globalPosition < 0) {\n        throw new Error(\"Global position must be a non-negative safe integer\");\n      }\n      const entryOffset = INDEX_HEADER_SIZE + i * INDEX_ENTRY_SIZE;\n      view.setUint32(entryOffset, stringTable.get(entry.streamId), true);\n      view.setUint32(entryOffset + 4, entry.revision, true);\n      view.setBigUint64(entryOffset + 8, BigInt(entry.globalPosition), true);\n      view.setUint32(entryOffset + 16, entry.batchOffset, true);\n      view.setUint32(entryOffset + 20, 0, true);\n    }\n    let strWriteOffset = stringTableStart;\n    for (const encoded of encodedStrings) {\n      buffer.set(encoded, strWriteOffset);\n      strWriteOffset += encoded.length;\n      buffer[strWriteOffset] = 0;\n      strWriteOffset++;\n    }\n    const beforeChecksum = buffer.subarray(0, 24);\n    const afterChecksum = buffer.subarray(28);\n    const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n    checksumData.set(beforeChecksum);\n    checksumData.set(afterChecksum, beforeChecksum.length);\n    const checksum = crc32(checksumData);\n    view.setUint32(24, checksum, true);\n    const tempPath = path + \".tmp\";\n    const handle = await fs.open(tempPath, \"write\");\n    await fs.write(handle, buffer);\n    await fs.sync(handle);\n    await fs.close(handle);\n    await fs.rename(tempPath, path);\n  }\n}\n\n// src/infrastructure/storage/support/write-all-bytes.ts\nclass ShortWriteError extends Error {\n  bytesRequested;\n  bytesWritten;\n  constructor(bytesRequested, bytesWritten) {\n    super(`Short write: only ${bytesWritten}/${bytesRequested} bytes written`);\n    this.bytesRequested = bytesRequested;\n    this.bytesWritten = bytesWritten;\n    this.name = \"ShortWriteError\";\n  }\n}\nasync function writeAllBytes(fs, handle, data, offset) {\n  let written = 0;\n  while (written < data.length) {\n    const remaining = data.subarray(written);\n    const writeOffset = offset !== undefined ? offset + written : undefined;\n    const bytesWritten = await fs.write(handle, remaining, writeOffset);\n    if (bytesWritten === 0) {\n      throw new ShortWriteError(data.length, written);\n    }\n    written += bytesWritten;\n  }\n}\n\n// src/infrastructure/storage/segments/segment-writer.ts\nclass SegmentWriter {\n  fs;\n  serializer;\n  compressor;\n  handle = null;\n  currentOffset = 0;\n  batchIdCounter = 0n;\n  path = \"\";\n  segmentId = 0n;\n  indexEntries = [];\n  constructor(fs, serializer, compressor) {\n    this.fs = fs;\n    this.serializer = serializer;\n    this.compressor = compressor;\n  }\n  async open(path, segmentId, basePosition) {\n    if (this.handle) {\n      throw new Error(\"SegmentWriter already has an open file\");\n    }\n    this.path = path;\n    this.segmentId = segmentId;\n    this.indexEntries = [];\n    this.handle = await this.fs.open(path, \"write\");\n    const header = encodeSegmentHeader({\n      flags: 0,\n      segmentId,\n      basePosition\n    });\n    await writeAllBytes(this.fs, this.handle, header, 0);\n    this.currentOffset = SEGMENT_HEADER_SIZE;\n    await this.fs.sync(this.handle);\n  }\n  async resume(path, offset, lastBatchId) {\n    if (this.handle) {\n      throw new Error(\"SegmentWriter already has an open file\");\n    }\n    this.path = path;\n    this.handle = await this.fs.open(path, \"readwrite\");\n    this.currentOffset = offset;\n    this.batchIdCounter = lastBatchId + 1n;\n  }\n  async appendBatch(events) {\n    if (!this.handle) {\n      throw new Error(\"SegmentWriter is not open\");\n    }\n    if (events.length === 0) {\n      throw new Error(\"Cannot write empty batch\");\n    }\n    const batchId = this.batchIdCounter++;\n    const batchOffset = this.currentOffset;\n    const serialized = this.serializer.encode(events);\n    const compressed = this.compressor.compress(serialized);\n    const record = encodeBatchRecord(batchId, events.length, compressed, serialized.length);\n    await writeAllBytes(this.fs, this.handle, record, batchOffset);\n    this.currentOffset += record.length;\n    for (const event of events) {\n      this.indexEntries.push({\n        streamId: event.streamId,\n        revision: event.revision,\n        globalPosition: event.globalPosition,\n        batchOffset\n      });\n    }\n    return {\n      batchId,\n      offset: batchOffset,\n      length: record.length,\n      eventCount: events.length\n    };\n  }\n  async sync() {\n    if (!this.handle) {\n      throw new Error(\"SegmentWriter is not open\");\n    }\n    await this.fs.sync(this.handle);\n  }\n  async close() {\n    if (!this.handle) {\n      return;\n    }\n    if (this.indexEntries.length > 0) {\n      const idxPath = this.path.replace(\".log\", \".idx\");\n      await SegmentIndexFile.write(this.fs, idxPath, this.segmentId, this.indexEntries);\n    }\n    await this.fs.close(this.handle);\n    this.handle = null;\n    this.path = \"\";\n    this.indexEntries = [];\n  }\n  get offset() {\n    return this.currentOffset;\n  }\n  get isOpen() {\n    return this.handle !== null;\n  }\n  get filePath() {\n    return this.path;\n  }\n  get nextBatchId() {\n    return this.batchIdCounter;\n  }\n}\n\n// src/infrastructure/storage/segments/segment-reader.ts\nclass SegmentReader {\n  fs;\n  serializer;\n  compressor;\n  profiler;\n  constructor(fs, serializer, compressor, profiler) {\n    this.fs = fs;\n    this.serializer = serializer;\n    this.compressor = compressor;\n    this.profiler = profiler;\n  }\n  async readHeader(path) {\n    const data = await this.fs.readFileSlice(path, 0, SEGMENT_HEADER_SIZE);\n    return decodeSegmentHeader(data);\n  }\n  async readBatch(path, offset) {\n    const totalStart = performance.now();\n    const headerStart = performance.now();\n    const headerData = await this.fs.readFileSlice(path, offset, offset + BATCH_HEADER_SIZE);\n    const headerReadMs = performance.now() - headerStart;\n    const header = decodeBatchHeader(headerData, false);\n    const fullBatchSize = BATCH_HEADER_SIZE + header.compressedLength;\n    const payloadStart = performance.now();\n    const batchData = await this.fs.readFileSlice(path, offset, offset + fullBatchSize);\n    const payloadReadMs = performance.now() - payloadStart;\n    decodeBatchHeader(batchData, true);\n    const payload = extractBatchPayload(batchData, header);\n    const decompressStart = performance.now();\n    const decompressed = this.compressor.decompress(payload);\n    const decompressMs = performance.now() - decompressStart;\n    const decodeStart = performance.now();\n    const events = this.serializer.decode(decompressed);\n    const decodeMs = performance.now() - decodeStart;\n    const totalMs = performance.now() - totalStart;\n    this.profiler?.record({\n      headerReadMs,\n      payloadReadMs,\n      decompressMs,\n      decodeMs,\n      totalMs,\n      eventCount: events.length,\n      compressedBytes: header.compressedLength,\n      uncompressedBytes: header.uncompressedLength\n    });\n    return events;\n  }\n  async readBatchWithMetadata(path, offset) {\n    const totalStart = performance.now();\n    const headerStart = performance.now();\n    const headerData = await this.fs.readFileSlice(path, offset, offset + BATCH_HEADER_SIZE);\n    const headerReadMs = performance.now() - headerStart;\n    const header = decodeBatchHeader(headerData, false);\n    const fullBatchSize = BATCH_HEADER_SIZE + header.compressedLength;\n    const payloadStart = performance.now();\n    const batchData = await this.fs.readFileSlice(path, offset, offset + fullBatchSize);\n    const payloadReadMs = performance.now() - payloadStart;\n    decodeBatchHeader(batchData, true);\n    const payload = extractBatchPayload(batchData, header);\n    const decompressStart = performance.now();\n    const decompressed = this.compressor.decompress(payload);\n    const decompressMs = performance.now() - decompressStart;\n    const decodeStart = performance.now();\n    const events = this.serializer.decode(decompressed);\n    const decodeMs = performance.now() - decodeStart;\n    const totalMs = performance.now() - totalStart;\n    this.profiler?.record({\n      headerReadMs,\n      payloadReadMs,\n      decompressMs,\n      decodeMs,\n      totalMs,\n      eventCount: events.length,\n      compressedBytes: header.compressedLength,\n      uncompressedBytes: header.uncompressedLength\n    });\n    return {\n      events,\n      batchId: header.batchId,\n      nextOffset: offset + fullBatchSize\n    };\n  }\n  async* readAllBatches(path, startOffset = SEGMENT_HEADER_SIZE) {\n    const fileStat = await this.fs.stat(path);\n    let offset = Math.max(startOffset, SEGMENT_HEADER_SIZE);\n    while (offset < fileStat.size) {\n      const remaining = fileStat.size - offset;\n      if (remaining < BATCH_HEADER_SIZE) {\n        break;\n      }\n      let headerData;\n      try {\n        headerData = await this.fs.readFileSlice(path, offset, offset + BATCH_HEADER_SIZE);\n      } catch (error) {\n        throw error;\n      }\n      if (!isValidBatchMagic(headerData)) {\n        break;\n      }\n      let header;\n      try {\n        header = decodeBatchHeader(headerData, false);\n      } catch (error) {\n        if (error instanceof InvalidBatchError || error instanceof BatchChecksumError) {\n          break;\n        }\n        throw error;\n      }\n      const fullBatchSize = BATCH_HEADER_SIZE + header.compressedLength;\n      if (offset + fullBatchSize > fileStat.size) {\n        break;\n      }\n      try {\n        const events = await this.readBatch(path, offset);\n        yield events;\n      } catch (error) {\n        if (error instanceof InvalidBatchError || error instanceof BatchChecksumError) {\n          break;\n        }\n        throw error;\n      }\n      offset += fullBatchSize;\n    }\n  }\n  async validateSegment(path) {\n    const errors = [];\n    let batchCount = 0;\n    let eventCount = 0;\n    let lastValidOffset = 0;\n    if (!await this.fs.exists(path)) {\n      return {\n        valid: false,\n        lastValidOffset: 0,\n        batchCount: 0,\n        eventCount: 0,\n        errors: [\"File does not exist\"]\n      };\n    }\n    const fileStat = await this.fs.stat(path);\n    if (fileStat.size < SEGMENT_HEADER_SIZE) {\n      return {\n        valid: false,\n        lastValidOffset: 0,\n        batchCount: 0,\n        eventCount: 0,\n        errors: [`File too small: ${fileStat.size} bytes (need at least ${SEGMENT_HEADER_SIZE})`]\n      };\n    }\n    const headerData = await this.fs.readFileSlice(path, 0, SEGMENT_HEADER_SIZE);\n    if (!isValidSegmentHeader(headerData)) {\n      return {\n        valid: false,\n        lastValidOffset: 0,\n        batchCount: 0,\n        eventCount: 0,\n        errors: [\"Invalid segment header\"]\n      };\n    }\n    lastValidOffset = SEGMENT_HEADER_SIZE;\n    let offset = SEGMENT_HEADER_SIZE;\n    while (offset < fileStat.size) {\n      const remaining = fileStat.size - offset;\n      if (remaining < BATCH_HEADER_SIZE) {\n        errors.push(`Incomplete batch header at offset ${offset}: only ${remaining} bytes remaining`);\n        break;\n      }\n      try {\n        const headerData2 = await this.fs.readFileSlice(path, offset, offset + BATCH_HEADER_SIZE);\n        if (!isValidBatchMagic(headerData2)) {\n          errors.push(`Invalid batch magic at offset ${offset}`);\n          break;\n        }\n        const header = decodeBatchHeader(headerData2, false);\n        const fullBatchSize = BATCH_HEADER_SIZE + header.compressedLength;\n        if (offset + fullBatchSize > fileStat.size) {\n          errors.push(`Incomplete batch at offset ${offset}: need ${fullBatchSize} bytes, have ${remaining}`);\n          break;\n        }\n        const batchData = await this.fs.readFileSlice(path, offset, offset + fullBatchSize);\n        decodeBatchHeader(batchData, true);\n        const payload = extractBatchPayload(batchData, header);\n        const decompressed = this.compressor.decompress(payload);\n        const events = this.serializer.decode(decompressed);\n        batchCount++;\n        eventCount += events.length;\n        offset += fullBatchSize;\n        lastValidOffset = offset;\n      } catch (error) {\n        const message = error instanceof Error ? error.message : String(error);\n        errors.push(`Error at offset ${offset}: ${message}`);\n        break;\n      }\n    }\n    return {\n      valid: errors.length === 0,\n      lastValidOffset,\n      batchCount,\n      eventCount,\n      errors\n    };\n  }\n  async getFileSize(path) {\n    const stat = await this.fs.stat(path);\n    return stat.size;\n  }\n}\n\n// src/infrastructure/storage/segments/segment-index.ts\nclass SegmentIndex {\n  streamIndex = new Map;\n  positionIndex = new Map;\n  indexedSegments = new Set;\n  addBatch(segmentId, batchOffset, events) {\n    this.indexedSegments.add(segmentId);\n    for (const event of events) {\n      let streamEntries = this.streamIndex.get(event.streamId);\n      if (!streamEntries) {\n        streamEntries = [];\n        this.streamIndex.set(event.streamId, streamEntries);\n      }\n      streamEntries.push({\n        segmentId,\n        batchOffset,\n        revision: event.revision,\n        globalPosition: event.globalPosition\n      });\n      this.positionIndex.set(event.globalPosition, {\n        segmentId,\n        batchOffset,\n        streamId: event.streamId\n      });\n    }\n  }\n  findByStream(streamId, options = {}) {\n    const entries = this.streamIndex.get(streamId);\n    if (!entries || entries.length === 0) {\n      return [];\n    }\n    const sorted = [...entries].sort((a, b) => a.revision - b.revision);\n    let filtered = sorted;\n    if (options.fromRevision !== undefined) {\n      filtered = filtered.filter((e) => e.revision >= options.fromRevision);\n    }\n    if (options.toRevision !== undefined) {\n      filtered = filtered.filter((e) => e.revision <= options.toRevision);\n    }\n    if (options.direction === \"backward\") {\n      filtered.reverse();\n    }\n    return filtered;\n  }\n  findByPosition(position) {\n    return this.positionIndex.get(position);\n  }\n  getStreamRevision(streamId) {\n    const entries = this.streamIndex.get(streamId);\n    if (!entries || entries.length === 0) {\n      return -1;\n    }\n    return Math.max(...entries.map((e) => e.revision));\n  }\n  hasStream(streamId) {\n    const entries = this.streamIndex.get(streamId);\n    return entries !== undefined && entries.length > 0;\n  }\n  getStreamIds() {\n    return Array.from(this.streamIndex.keys());\n  }\n  getStreamEventCount(streamId) {\n    return this.streamIndex.get(streamId)?.length ?? 0;\n  }\n  evictSegment(segmentId) {\n    if (!this.indexedSegments.has(segmentId)) {\n      return;\n    }\n    for (const [streamId, entries] of this.streamIndex) {\n      const filtered = entries.filter((e) => e.segmentId !== segmentId);\n      if (filtered.length === 0) {\n        this.streamIndex.delete(streamId);\n      } else {\n        this.streamIndex.set(streamId, filtered);\n      }\n    }\n    for (const [position, entry] of this.positionIndex) {\n      if (entry.segmentId === segmentId) {\n        this.positionIndex.delete(position);\n      }\n    }\n    this.indexedSegments.delete(segmentId);\n  }\n  isSegmentIndexed(segmentId) {\n    return this.indexedSegments.has(segmentId);\n  }\n  async rebuildFromSegment(reader, path, segmentId) {\n    this.evictSegment(segmentId);\n    let batchCount = 0;\n    let eventCount = 0;\n    let offset = 32;\n    const fileSize = await reader.getFileSize(path);\n    while (offset < fileSize) {\n      try {\n        const { events, nextOffset } = await reader.readBatchWithMetadata(path, offset);\n        this.addBatch(segmentId, offset, events);\n        batchCount++;\n        eventCount += events.length;\n        offset = nextOffset;\n      } catch {\n        break;\n      }\n    }\n    return { batchCount, eventCount };\n  }\n  clear() {\n    this.streamIndex.clear();\n    this.positionIndex.clear();\n    this.indexedSegments.clear();\n  }\n  getStats() {\n    let totalEntries = 0;\n    for (const entries of this.streamIndex.values()) {\n      totalEntries += entries.length;\n    }\n    return {\n      streamCount: this.streamIndex.size,\n      totalEntries,\n      segmentCount: this.indexedSegments.size\n    };\n  }\n}\n\n// src/infrastructure/storage/support/stream-map.ts\nclass StreamMap {\n  streams = new Map;\n  getRevision(streamId) {\n    return this.streams.get(streamId)?.latestRevision ?? -1;\n  }\n  getSegments(streamId) {\n    const meta = this.streams.get(streamId);\n    return meta ? Array.from(meta.segments) : [];\n  }\n  hasStream(streamId) {\n    return this.streams.has(streamId);\n  }\n  updateStream(streamId, revision, segmentId) {\n    const existing = this.streams.get(streamId);\n    if (!existing) {\n      this.streams.set(streamId, {\n        latestRevision: revision,\n        segments: new Set([segmentId])\n      });\n      return;\n    }\n    if (revision > existing.latestRevision) {\n      existing.latestRevision = revision;\n    }\n    existing.segments.add(segmentId);\n  }\n  clear() {\n    this.streams.clear();\n  }\n  getStreamCount() {\n    return this.streams.size;\n  }\n  getAllStreamIds() {\n    return Array.from(this.streams.keys()).sort();\n  }\n  getStats() {\n    let totalSegmentRefs = 0;\n    for (const meta of this.streams.values()) {\n      totalSegmentRefs += meta.segments.size;\n    }\n    return {\n      streamCount: this.streams.size,\n      totalSegmentRefs\n    };\n  }\n  evictSegment(segmentId) {\n    const streamsToRemove = [];\n    for (const [streamId, meta] of this.streams) {\n      if (meta.segments.has(segmentId)) {\n        meta.segments.delete(segmentId);\n        if (meta.segments.size === 0) {\n          streamsToRemove.push(streamId);\n        }\n      }\n    }\n    for (const streamId of streamsToRemove) {\n      this.streams.delete(streamId);\n    }\n  }\n  toJSON() {\n    const streams = [];\n    for (const [streamId, meta] of this.streams) {\n      streams.push({\n        streamId,\n        latestRevision: meta.latestRevision,\n        segments: Array.from(meta.segments).map((s) => s.toString())\n      });\n    }\n    return { streams };\n  }\n  static fromJSON(data) {\n    const map = new StreamMap;\n    for (const entry of data.streams) {\n      map.streams.set(entry.streamId, {\n        latestRevision: entry.latestRevision,\n        segments: new Set(entry.segments.map((s) => BigInt(s)))\n      });\n    }\n    return map;\n  }\n}\n\n// src/infrastructure/storage/support/lru-cache.ts\nclass LRUCache {\n  cache = new Map;\n  maxSize;\n  constructor(maxSize) {\n    if (maxSize < 1) {\n      throw new Error(\"LRUCache maxSize must be at least 1\");\n    }\n    this.maxSize = maxSize;\n  }\n  get(key) {\n    if (!this.cache.has(key)) {\n      return;\n    }\n    const value = this.cache.get(key);\n    this.cache.delete(key);\n    this.cache.set(key, value);\n    return value;\n  }\n  set(key, value) {\n    if (this.cache.has(key)) {\n      this.cache.delete(key);\n    }\n    if (this.cache.size >= this.maxSize) {\n      const oldestKey = this.cache.keys().next().value;\n      if (oldestKey !== undefined) {\n        this.cache.delete(oldestKey);\n      }\n    }\n    this.cache.set(key, value);\n  }\n  has(key) {\n    return this.cache.has(key);\n  }\n  delete(key) {\n    return this.cache.delete(key);\n  }\n  clear() {\n    this.cache.clear();\n  }\n  get size() {\n    return this.cache.size;\n  }\n  keys() {\n    return this.cache.keys();\n  }\n  values() {\n    return this.cache.values();\n  }\n  entries() {\n    return this.cache.entries();\n  }\n}\n\n// src/infrastructure/storage/support/manifest.ts\nvar MANIFEST_VERSION = 1;\nvar MANIFEST_FILENAME = \".manifest\";\nvar MANIFEST_TEMP_FILENAME = \".manifest.tmp\";\n\nclass Manifest {\n  fs;\n  segments = new Map;\n  globalPosition = 0;\n  nextSegmentId = 0n;\n  dataDir = \"\";\n  dirty = false;\n  saveCounter = 0;\n  constructor(fs) {\n    this.fs = fs;\n  }\n  async load(dataDir) {\n    this.dataDir = dataDir;\n    const manifestPath = `${dataDir}/${MANIFEST_FILENAME}`;\n    try {\n      if (!await this.fs.exists(manifestPath)) {\n        return false;\n      }\n      const content = await this.fs.readFile(manifestPath);\n      const json = new TextDecoder().decode(content);\n      const data = JSON.parse(json);\n      if (data.version !== MANIFEST_VERSION) {\n        console.warn(`[spitedb] Manifest version mismatch: expected ${MANIFEST_VERSION}, got ${data.version}`);\n        return false;\n      }\n      this.globalPosition = Number(data.globalPosition);\n      this.nextSegmentId = BigInt(data.nextSegmentId);\n      this.segments.clear();\n      for (const seg of data.segments) {\n        const segment = {\n          id: BigInt(seg.id),\n          path: seg.path,\n          basePosition: Number(seg.basePosition)\n        };\n        this.segments.set(segment.id, segment);\n      }\n      this.dirty = false;\n      return true;\n    } catch (error) {\n      console.warn(`[spitedb] Failed to load manifest: ${error}`);\n      return false;\n    }\n  }\n  async save() {\n    if (!this.dataDir) {\n      throw new Error(\"Manifest not initialized. Call load() first.\");\n    }\n    const data = {\n      version: MANIFEST_VERSION,\n      globalPosition: this.globalPosition.toString(),\n      nextSegmentId: this.nextSegmentId.toString(),\n      segments: Array.from(this.segments.values()).map((seg) => ({\n        id: seg.id.toString(),\n        path: seg.path,\n        basePosition: seg.basePosition.toString()\n      }))\n    };\n    const json = JSON.stringify(data, null, 2);\n    const content = new TextEncoder().encode(json);\n    const tempPath = `${this.dataDir}/${MANIFEST_TEMP_FILENAME}.${this.saveCounter++}`;\n    const finalPath = `${this.dataDir}/${MANIFEST_FILENAME}`;\n    const handle = await this.fs.open(tempPath, \"write\");\n    await this.fs.write(handle, content);\n    await this.fs.sync(handle);\n    await this.fs.close(handle);\n    await this.fs.rename(tempPath, finalPath);\n    this.dirty = false;\n  }\n  addSegment(segment) {\n    this.segments.set(segment.id, segment);\n    if (segment.id >= this.nextSegmentId) {\n      this.nextSegmentId = segment.id + 1n;\n    }\n    this.dirty = true;\n  }\n  removeSegment(segmentId) {\n    const removed = this.segments.delete(segmentId);\n    if (removed) {\n      this.dirty = true;\n    }\n    return removed;\n  }\n  getSegment(segmentId) {\n    return this.segments.get(segmentId);\n  }\n  getSegments() {\n    return Array.from(this.segments.values()).sort((a, b) => a.id < b.id ? -1 : a.id > b.id ? 1 : 0);\n  }\n  getSegmentCount() {\n    return this.segments.size;\n  }\n  getGlobalPosition() {\n    return this.globalPosition;\n  }\n  setGlobalPosition(position) {\n    if (position !== this.globalPosition) {\n      this.globalPosition = position;\n      this.dirty = true;\n    }\n  }\n  getNextSegmentId() {\n    return this.nextSegmentId;\n  }\n  setNextSegmentId(id) {\n    if (id !== this.nextSegmentId) {\n      this.nextSegmentId = id;\n      this.dirty = true;\n    }\n  }\n  allocateSegmentId() {\n    const id = this.nextSegmentId;\n    this.nextSegmentId++;\n    this.dirty = true;\n    return id;\n  }\n  isDirty() {\n    return this.dirty;\n  }\n  clear() {\n    this.segments.clear();\n    this.globalPosition = 0;\n    this.nextSegmentId = 0n;\n    this.dirty = true;\n  }\n  initializeFromScan(segments, globalPosition, nextSegmentId) {\n    this.segments.clear();\n    for (const seg of segments) {\n      this.segments.set(seg.id, seg);\n    }\n    this.globalPosition = globalPosition;\n    this.nextSegmentId = nextSegmentId;\n    this.dirty = true;\n  }\n}\n\n// src/infrastructure/storage/segments/segment-manager.ts\nvar DEFAULT_MAX_SEGMENT_SIZE = 128 * 1024 * 1024;\nvar DEFAULT_INDEX_CACHE_SIZE = 10;\n\nclass SegmentManager {\n  fs;\n  serializer;\n  compressor;\n  config;\n  reader;\n  indexCache;\n  indexFileCache;\n  streamMap;\n  manifest;\n  segments = new Map;\n  activeWriter = null;\n  activeSegmentId = 0n;\n  nextSegmentId = 0n;\n  globalPosition = 0;\n  initialized = false;\n  pendingCloses = [];\n  pendingBatch = null;\n  constructor(fs, serializer, compressor, config) {\n    this.fs = fs;\n    this.serializer = serializer;\n    this.compressor = compressor;\n    this.config = {\n      dataDir: config.dataDir,\n      maxSegmentSize: config.maxSegmentSize ?? DEFAULT_MAX_SEGMENT_SIZE,\n      indexCacheSize: config.indexCacheSize ?? DEFAULT_INDEX_CACHE_SIZE\n    };\n    this.reader = new SegmentReader(fs, serializer, compressor, config.readProfiler);\n    this.indexCache = new LRUCache(this.config.indexCacheSize);\n    this.indexFileCache = new LRUCache(this.config.indexCacheSize);\n    this.streamMap = new StreamMap;\n    this.manifest = new Manifest(fs);\n  }\n  async initialize() {\n    if (this.initialized) {\n      throw new Error(\"SegmentManager already initialized\");\n    }\n    if (!await this.fs.exists(this.config.dataDir)) {\n      await this.fs.mkdir(this.config.dataDir, { recursive: true });\n    }\n    const manifestLoaded = await this.manifest.load(this.config.dataDir);\n    if (manifestLoaded) {\n      for (const seg of this.manifest.getSegments()) {\n        const path = `${this.config.dataDir}/${seg.path}`;\n        if (await this.fs.exists(path)) {\n          try {\n            const stat = await this.fs.stat(path);\n            this.segments.set(seg.id, {\n              id: seg.id,\n              path,\n              size: stat.size,\n              basePosition: seg.basePosition,\n              isActive: false\n            });\n          } catch {}\n        }\n      }\n      this.nextSegmentId = this.manifest.getNextSegmentId();\n      this.globalPosition = this.manifest.getGlobalPosition();\n    } else {\n      await this.scanDirectory();\n      this.manifest.initializeFromScan(Array.from(this.segments.values()).map((seg) => ({\n        id: seg.id,\n        path: seg.path.split(\"/\").pop(),\n        basePosition: seg.basePosition\n      })), this.globalPosition, this.nextSegmentId);\n      await this.manifest.save();\n    }\n    await this.rebuildStreamMap();\n    this.initialized = true;\n  }\n  async scanDirectory() {\n    const files = await this.fs.readdir(this.config.dataDir);\n    const segmentFiles = files.filter((f) => f.startsWith(\"segment-\") && f.endsWith(\".log\"));\n    for (const file of segmentFiles) {\n      const path = `${this.config.dataDir}/${file}`;\n      try {\n        const stat = await this.fs.stat(path);\n        const header = await this.reader.readHeader(path);\n        this.segments.set(header.segmentId, {\n          id: header.segmentId,\n          path,\n          size: stat.size,\n          basePosition: header.basePosition,\n          isActive: false\n        });\n        if (header.segmentId >= this.nextSegmentId) {\n          this.nextSegmentId = header.segmentId + 1n;\n        }\n      } catch (error) {\n        console.warn(`Skipping invalid segment file: ${file}`, error);\n      }\n    }\n  }\n  async rebuildStreamMap() {\n    this.streamMap.clear();\n    for (const [segmentId, info] of this.segments) {\n      try {\n        const indexFile = await this.loadOrRebuildIndexFile(segmentId, info.path);\n        for (const streamId of indexFile.getAllStreamIds()) {\n          const maxRevision = indexFile.getStreamMaxRevision(streamId);\n          this.streamMap.updateStream(streamId, maxRevision, segmentId);\n          const entries = indexFile.findByStream(streamId);\n          for (const entry of entries) {\n            if (entry.globalPosition >= this.globalPosition) {\n              this.globalPosition = entry.globalPosition + 1;\n            }\n          }\n        }\n      } catch (error) {\n        console.warn(`Failed to load index for segment ${segmentId}:`, error);\n        try {\n          await this.rebuildStreamMapFromLog(segmentId, info.path);\n        } catch (scanError) {\n          console.warn(`Failed to scan segment ${segmentId} for recovery:`, scanError);\n        }\n      }\n    }\n  }\n  async rebuildStreamMapFromLog(segmentId, logPath) {\n    for await (const batch of this.reader.readAllBatches(logPath)) {\n      for (const event of batch) {\n        this.streamMap.updateStream(event.streamId, event.revision, segmentId);\n        if (event.globalPosition >= this.globalPosition) {\n          this.globalPosition = event.globalPosition + 1;\n        }\n      }\n    }\n  }\n  async loadOrRebuildIndexFile(segmentId, logPath) {\n    const idxPath = logPath.replace(\".log\", \".idx\");\n    if (await this.fs.exists(idxPath)) {\n      try {\n        const indexFile = new SegmentIndexFile;\n        await indexFile.load(this.fs, idxPath);\n        return indexFile;\n      } catch (error) {\n        if (error instanceof IndexCorruptedError) {\n          console.warn(`Index file corrupted for segment ${segmentId}, rebuilding...`);\n        } else {\n          throw error;\n        }\n      }\n    }\n    return await this.rebuildIndexFile(segmentId, logPath);\n  }\n  async rebuildIndexFile(segmentId, logPath) {\n    const idxPath = logPath.replace(\".log\", \".idx\");\n    const entries = [];\n    const fileStat = await this.fs.stat(logPath);\n    let offset = SEGMENT_HEADER_SIZE;\n    while (offset < fileStat.size) {\n      try {\n        const result = await this.reader.readBatchWithMetadata(logPath, offset);\n        const batchOffset = offset;\n        for (const event of result.events) {\n          entries.push({\n            streamId: event.streamId,\n            revision: event.revision,\n            globalPosition: event.globalPosition,\n            batchOffset\n          });\n        }\n        offset = result.nextOffset;\n      } catch {\n        break;\n      }\n    }\n    if (entries.length > 0) {\n      await SegmentIndexFile.write(this.fs, idxPath, segmentId, entries);\n    }\n    const indexFile = new SegmentIndexFile;\n    if (entries.length > 0) {\n      await indexFile.load(this.fs, idxPath);\n    }\n    return indexFile;\n  }\n  async getWriter(basePosition) {\n    this.ensureInitialized();\n    if (!this.activeWriter) {\n      await this.rotateSegment(basePosition);\n    }\n    const activeInfo = this.segments.get(this.activeSegmentId);\n    if (activeInfo && activeInfo.size >= this.config.maxSegmentSize) {\n      await this.rotateSegment(basePosition);\n    }\n    return this.activeWriter;\n  }\n  async rotateSegment(basePosition = this.globalPosition) {\n    this.ensureInitialized();\n    const oldWriter = this.activeWriter;\n    const oldSegmentId = this.activeSegmentId;\n    if (oldWriter) {\n      await oldWriter.sync();\n      const finalSize = oldWriter.offset;\n      const activeInfo = this.segments.get(oldSegmentId);\n      if (activeInfo) {\n        activeInfo.isActive = false;\n        activeInfo.size = finalSize;\n      }\n    }\n    const segmentId = this.nextSegmentId++;\n    const path = this.getSegmentPath(segmentId);\n    this.activeWriter = new SegmentWriter(this.fs, this.serializer, this.compressor);\n    await this.activeWriter.open(path, segmentId, basePosition);\n    this.activeSegmentId = segmentId;\n    this.segments.set(segmentId, {\n      id: segmentId,\n      path,\n      size: SEGMENT_HEADER_SIZE,\n      basePosition,\n      isActive: true\n    });\n    const segmentFileName = path.split(\"/\").pop();\n    this.manifest.addSegment({\n      id: segmentId,\n      path: segmentFileName,\n      basePosition\n    });\n    this.manifest.setNextSegmentId(this.nextSegmentId);\n    this.manifest.save().catch((err) => {\n      console.warn(`Failed to save manifest after rotation:`, err);\n    });\n    if (oldWriter) {\n      const closePromise = oldWriter.close().catch((err) => {\n        console.warn(`Background index write failed for segment ${oldSegmentId}:`, err);\n      });\n      this.pendingCloses.push(closePromise);\n    }\n  }\n  async writeBatch(events) {\n    const firstPosition = events[0].globalPosition;\n    const lastPosition = events[events.length - 1].globalPosition;\n    if (this.pendingBatch && this.pendingBatch.firstPosition === firstPosition && this.pendingBatch.lastPosition === lastPosition && this.pendingBatch.eventCount === events.length) {\n      return {\n        batchId: 0n,\n        offset: 0,\n        length: 0,\n        eventCount: events.length,\n        alreadyWritten: true\n      };\n    }\n    const writer = await this.getWriter(events[0].globalPosition);\n    const result = await writer.appendBatch(events);\n    this.pendingBatch = {\n      firstPosition,\n      lastPosition,\n      eventCount: events.length\n    };\n    const activeInfo = this.segments.get(this.activeSegmentId);\n    if (activeInfo) {\n      activeInfo.size = writer.offset;\n    }\n    for (const event of events) {\n      if (event.globalPosition >= this.globalPosition) {\n        this.globalPosition = event.globalPosition + 1;\n      }\n      this.streamMap.updateStream(event.streamId, event.revision, this.activeSegmentId);\n    }\n    return result;\n  }\n  async sync() {\n    if (this.activeWriter) {\n      await this.activeWriter.sync();\n      this.pendingBatch = null;\n    }\n  }\n  async getIndex(segmentId) {\n    this.ensureInitialized();\n    let index = this.indexCache.get(segmentId);\n    if (index) {\n      return index;\n    }\n    const segmentInfo = this.segments.get(segmentId);\n    if (!segmentInfo) {\n      throw new Error(`Segment ${segmentId} not found`);\n    }\n    index = new SegmentIndex;\n    await index.rebuildFromSegment(this.reader, segmentInfo.path, segmentId);\n    this.indexCache.set(segmentId, index);\n    return index;\n  }\n  async recover() {\n    this.ensureInitialized();\n    const errors = [];\n    let recoveredSegments = 0;\n    let totalEvents = 0;\n    for (const [segmentId, info] of this.segments) {\n      try {\n        const result = await this.reader.validateSegment(info.path);\n        if (!result.valid) {\n          const handle = await this.fs.open(info.path, \"readwrite\");\n          await this.fs.truncate(handle, result.lastValidOffset);\n          await this.fs.close(handle);\n          recoveredSegments++;\n          info.size = result.lastValidOffset;\n          this.indexCache.delete(segmentId);\n          errors.push(`Segment ${segmentId}: truncated from ${info.size} to ${result.lastValidOffset} bytes. ` + `Errors: ${result.errors.join(\", \")}`);\n        }\n        totalEvents += result.eventCount;\n      } catch (error) {\n        const message = error instanceof Error ? error.message : String(error);\n        errors.push(`Segment ${segmentId}: recovery failed - ${message}`);\n      }\n    }\n    return {\n      segmentCount: this.segments.size,\n      recoveredSegments,\n      totalEvents,\n      errors\n    };\n  }\n  async close() {\n    if (this.pendingCloses.length > 0) {\n      await Promise.all(this.pendingCloses);\n      this.pendingCloses = [];\n    }\n    if (this.activeWriter) {\n      await this.activeWriter.close();\n      this.activeWriter = null;\n    }\n    this.manifest.setGlobalPosition(this.globalPosition);\n    try {\n      await this.manifest.save();\n    } catch (err) {\n      console.warn(`Failed to save manifest on close:`, err);\n    }\n    this.indexCache.clear();\n    this.indexFileCache.clear();\n    this.streamMap.clear();\n    this.segments.clear();\n    this.initialized = false;\n  }\n  getReader() {\n    return this.reader;\n  }\n  getSegments() {\n    return Array.from(this.segments.values()).sort((a, b) => a.id < b.id ? -1 : a.id > b.id ? 1 : 0);\n  }\n  getGlobalPosition() {\n    return this.globalPosition;\n  }\n  getNextGlobalPosition() {\n    return this.globalPosition;\n  }\n  allocateGlobalPosition() {\n    return this.globalPosition++;\n  }\n  getActiveSegmentId() {\n    return this.activeSegmentId;\n  }\n  isInitialized() {\n    return this.initialized;\n  }\n  getStreamRevision(streamId) {\n    return this.streamMap.getRevision(streamId);\n  }\n  getStreamSegments(streamId) {\n    return this.streamMap.getSegments(streamId);\n  }\n  hasStream(streamId) {\n    return this.streamMap.hasStream(streamId);\n  }\n  getAllStreamIds() {\n    return this.streamMap.getAllStreamIds();\n  }\n  getStreamMap() {\n    return this.streamMap;\n  }\n  async getSegmentIndexFile(segmentId) {\n    this.ensureInitialized();\n    let indexFile = this.indexFileCache.get(segmentId);\n    if (indexFile) {\n      return indexFile;\n    }\n    const segmentInfo = this.segments.get(segmentId);\n    if (!segmentInfo) {\n      throw new Error(`Segment ${segmentId} not found`);\n    }\n    indexFile = await this.loadOrRebuildIndexFile(segmentId, segmentInfo.path);\n    this.indexFileCache.set(segmentId, indexFile);\n    return indexFile;\n  }\n  getSegment(segmentId) {\n    return this.segments.get(segmentId);\n  }\n  getSegmentPath(segmentId) {\n    const paddedId = segmentId.toString().padStart(8, \"0\");\n    return `${this.config.dataDir}/segment-${paddedId}.log`;\n  }\n  ensureInitialized() {\n    if (!this.initialized) {\n      throw new Error(\"SegmentManager not initialized. Call initialize() first.\");\n    }\n  }\n}\n\n// src/domain/errors/invalid-stream-id.error.ts\nclass InvalidStreamIdError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"InvalidStreamIdError\";\n    Object.setPrototypeOf(this, InvalidStreamIdError.prototype);\n  }\n}\n// src/domain/errors/invalid-position.error.ts\nclass InvalidPositionError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"InvalidPositionError\";\n    Object.setPrototypeOf(this, InvalidPositionError.prototype);\n  }\n}\n// src/domain/errors/concurrency.error.ts\nclass ConcurrencyError extends Error {\n  streamId;\n  expectedRevision;\n  actualRevision;\n  constructor(streamId, expectedRevision, actualRevision) {\n    super(`Concurrency conflict on stream '${streamId}': ` + `expected revision ${expectedRevision}, ` + `but actual revision is ${actualRevision}`);\n    this.streamId = streamId;\n    this.expectedRevision = expectedRevision;\n    this.actualRevision = actualRevision;\n    this.name = \"ConcurrencyError\";\n    Object.setPrototypeOf(this, ConcurrencyError.prototype);\n  }\n}\n// src/domain/errors/store-fatal.error.ts\nclass StoreFatalError extends Error {\n  cause;\n  constructor(message, cause) {\n    super(message);\n    this.cause = cause;\n    this.name = \"StoreFatalError\";\n    Object.setPrototypeOf(this, StoreFatalError.prototype);\n  }\n}\n// src/infrastructure/resource-limits.ts\nimport { dlopen, FFIType, ptr, suffix } from \"bun:ffi\";\nvar RLIMIT_NOFILE = process.platform === \"darwin\" ? 8 : 7;\nvar LOCK_SH = 1;\nvar LOCK_EX = 2;\nvar LOCK_UN = 8;\nvar libc = null;\nfunction getLibcCandidates() {\n  if (process.platform === \"darwin\") {\n    return [\"libc.dylib\"];\n  }\n  if (process.platform === \"linux\") {\n    return [\"libc.so.6\", \"libc.so\"];\n  }\n  return [`libc.${suffix}`];\n}\nfunction getLibc() {\n  if (!libc) {\n    try {\n      for (const candidate of getLibcCandidates()) {\n        try {\n          libc = dlopen(candidate, {\n            getrlimit: {\n              args: [FFIType.i32, FFIType.ptr],\n              returns: FFIType.i32\n            },\n            setrlimit: {\n              args: [FFIType.i32, FFIType.ptr],\n              returns: FFIType.i32\n            },\n            flock: {\n              args: [FFIType.i32, FFIType.i32],\n              returns: FFIType.i32\n            }\n          });\n          break;\n        } catch {}\n      }\n    } catch (e) {\n      return null;\n    }\n  }\n  return libc;\n}\nfunction getResourceLimits() {\n  const lib = getLibc();\n  if (!lib)\n    return null;\n  const getrlimitFn = lib.symbols[\"getrlimit\"];\n  if (!getrlimitFn)\n    return null;\n  const buffer = new BigUint64Array(2);\n  const result = getrlimitFn(RLIMIT_NOFILE, ptr(buffer));\n  if (result !== 0) {\n    return null;\n  }\n  return {\n    soft: buffer[0],\n    hard: buffer[1]\n  };\n}\nfunction setResourceLimits(soft, hard) {\n  const lib = getLibc();\n  if (!lib)\n    return false;\n  const setrlimitFn = lib.symbols[\"setrlimit\"];\n  if (!setrlimitFn)\n    return false;\n  const buffer = new BigUint64Array([soft, hard]);\n  const result = setrlimitFn(RLIMIT_NOFILE, ptr(buffer));\n  return result === 0;\n}\nfunction flock(fd, operation) {\n  const lib = getLibc();\n  if (!lib) {\n    throw new Error(\"FFI not available for flock\");\n  }\n  const flockFn = lib.symbols[\"flock\"];\n  if (!flockFn) {\n    throw new Error(\"flock symbol not available\");\n  }\n  const result = flockFn(fd, operation);\n  return result === 0;\n}\nvar TARGET_FD_LIMIT = 65536n;\nvar MIN_RECOMMENDED_FD_LIMIT = 1024n;\nfunction increaseFileDescriptorLimit() {\n  const limits = getResourceLimits();\n  if (!limits) {\n    return {\n      success: false,\n      previousLimit: 0,\n      newLimit: 0,\n      hardLimit: 0,\n      warning: \"Could not read file descriptor limits (FFI unavailable)\"\n    };\n  }\n  const previousLimit = Number(limits.soft);\n  const hardLimit = Number(limits.hard);\n  const target = limits.hard < TARGET_FD_LIMIT ? limits.hard : TARGET_FD_LIMIT;\n  if (limits.soft >= target) {\n    return {\n      success: true,\n      previousLimit,\n      newLimit: previousLimit,\n      hardLimit\n    };\n  }\n  const success = setResourceLimits(target, limits.hard);\n  if (success) {\n    const newLimit = Number(target);\n    const result = {\n      success: true,\n      previousLimit,\n      newLimit,\n      hardLimit\n    };\n    if (limits.hard < MIN_RECOMMENDED_FD_LIMIT) {\n      result.warning = `File descriptor hard limit is ${hardLimit}. For large deployments, ` + `increase it with: sudo launchctl limit maxfiles 65536 65536 (macOS) ` + `or edit /etc/security/limits.conf (Linux)`;\n    }\n    return result;\n  }\n  return {\n    success: false,\n    previousLimit,\n    newLimit: previousLimit,\n    hardLimit,\n    warning: `Could not increase file descriptor limit from ${previousLimit} to ${Number(target)}. ` + `You may need to increase the hard limit manually.`\n  };\n}\n\n// src/application/event-store/event-store.ts\nvar DEFAULT_AUTO_FLUSH_COUNT = 1000;\nvar DEFAULT_MAX_CACHED_EVENTS = 1e5;\n\nclass EventStore {\n  config;\n  segmentManager = null;\n  pendingEvents = [];\n  streamRevisions = new Map;\n  dataDir = \"\";\n  lockHandle = null;\n  lastFlushedGlobalPosition = -1;\n  writeQueue = Promise.resolve();\n  failed = false;\n  failedError = null;\n  recentlyFlushedEvents = new Map;\n  maxCachedEvents = DEFAULT_MAX_CACHED_EVENTS;\n  constructor(config) {\n    this.config = {\n      ...config,\n      autoFlushCount: config.autoFlushCount ?? DEFAULT_AUTO_FLUSH_COUNT\n    };\n  }\n  async open(dataDir) {\n    if (this.segmentManager) {\n      throw new Error(\"EventStore already open\");\n    }\n    const fdResult = increaseFileDescriptorLimit();\n    if (fdResult.warning) {\n      console.warn(`[spitedb] ${fdResult.warning}`);\n    }\n    this.dataDir = dataDir;\n    if (!await this.config.fs.exists(dataDir)) {\n      await this.config.fs.mkdir(dataDir, { recursive: true });\n    }\n    const lockPath = `${dataDir}/.lock`;\n    const disableFlock = process.env[\"SPITEDB_DISABLE_FLOCK\"] === \"1\" || process.env[\"SPITEDB_DISABLE_FLOCK\"] === \"true\";\n    if (disableFlock) {\n      console.warn(\"[spitedb] File locking disabled (SPITEDB_DISABLE_FLOCK=1).\");\n    } else {\n      try {\n        this.lockHandle = await this.config.fs.open(lockPath, \"write\");\n        await this.config.fs.flock(this.lockHandle, \"exclusive\");\n      } catch (error) {\n        if (this.lockHandle) {\n          try {\n            await this.config.fs.close(this.lockHandle);\n          } catch {}\n          this.lockHandle = null;\n        }\n        throw new Error(`Failed to open EventStore: another process may have it open. ` + `Lock file: ${lockPath}. Original error: ${error instanceof Error ? error.message : error}`);\n      }\n    }\n    try {\n      const segmentConfig = {\n        dataDir,\n        maxSegmentSize: this.config.maxSegmentSize,\n        indexCacheSize: this.config.indexCacheSize\n      };\n      if (this.config.readProfiler) {\n        segmentConfig.readProfiler = this.config.readProfiler;\n      }\n      this.segmentManager = new SegmentManager(this.config.fs, this.config.serializer, this.config.compressor, segmentConfig);\n      await this.segmentManager.initialize();\n      await this.rebuildStreamRevisions();\n      const currentGlobal = this.segmentManager.getGlobalPosition();\n      this.lastFlushedGlobalPosition = currentGlobal > 0 ? currentGlobal - 1 : -1;\n    } catch (error) {\n      if (this.lockHandle) {\n        try {\n          await this.config.fs.close(this.lockHandle);\n        } catch {}\n        this.lockHandle = null;\n      }\n      this.segmentManager = null;\n      throw error;\n    }\n  }\n  async close() {\n    if (!this.segmentManager) {\n      return;\n    }\n    if (!this.failed) {\n      await this.flush();\n    }\n    this.failed = false;\n    this.failedError = null;\n    await this.segmentManager.close();\n    this.segmentManager = null;\n    this.streamRevisions.clear();\n    this.pendingEvents = [];\n    this.recentlyFlushedEvents.clear();\n    this.lastFlushedGlobalPosition = -1;\n    if (this.lockHandle) {\n      await this.config.fs.close(this.lockHandle);\n      this.lockHandle = null;\n    }\n  }\n  async append(streamId, events, options = {}) {\n    return this.enqueueWrite(async () => {\n      this.ensureOpen();\n      if (events.length === 0) {\n        throw new Error(\"Cannot append empty event list\");\n      }\n      const tenantId = options.tenantId ?? \"default\";\n      const currentRevision = this.getStreamRevision(streamId);\n      if (options.expectedRevision !== undefined) {\n        if (options.expectedRevision === -1) {\n          if (currentRevision >= 0) {\n            throw new ConcurrencyError(streamId, options.expectedRevision, currentRevision);\n          }\n        } else if (options.expectedRevision !== currentRevision) {\n          throw new ConcurrencyError(streamId, options.expectedRevision, currentRevision);\n        }\n      }\n      const timestamp = this.config.clock.now();\n      const firstGlobalPosition = this.segmentManager.getNextGlobalPosition();\n      let revision = currentRevision + 1;\n      const storedEvents = [];\n      for (const event of events) {\n        storedEvents.push({\n          streamId,\n          type: event.type,\n          data: event.data,\n          metadata: event.metadata,\n          revision,\n          globalPosition: this.segmentManager.allocateGlobalPosition(),\n          timestamp,\n          tenantId\n        });\n        revision++;\n      }\n      this.pendingEvents.push(...storedEvents);\n      this.streamRevisions.set(streamId, revision - 1);\n      if (this.config.autoFlushCount > 0 && this.pendingEvents.length >= this.config.autoFlushCount) {\n        await this.flushInternal();\n      }\n      return {\n        streamRevision: revision - 1,\n        globalPosition: firstGlobalPosition,\n        eventCount: events.length\n      };\n    });\n  }\n  async appendBatch(operations) {\n    return this.enqueueWrite(async () => {\n      this.ensureOpen();\n      if (operations.length === 0) {\n        return {\n          streams: new Map,\n          globalPosition: this.segmentManager.getNextGlobalPosition(),\n          totalEvents: 0\n        };\n      }\n      for (const op of operations) {\n        if (op.events.length === 0) {\n          throw new Error(`Cannot append empty event list for stream ${op.streamId}`);\n        }\n        if (op.expectedRevision !== undefined) {\n          const current = this.getStreamRevision(op.streamId);\n          if (op.expectedRevision === -1) {\n            if (current >= 0) {\n              throw new ConcurrencyError(op.streamId, -1, current);\n            }\n          } else if (op.expectedRevision !== current) {\n            throw new ConcurrencyError(op.streamId, op.expectedRevision, current);\n          }\n        }\n      }\n      const timestamp = this.config.clock.now();\n      const firstGlobalPosition = this.segmentManager.getNextGlobalPosition();\n      const storedEvents = [];\n      const results = new Map;\n      for (const op of operations) {\n        const tenantId = op.tenantId ?? \"default\";\n        let revision = this.getStreamRevision(op.streamId) + 1;\n        for (const event of op.events) {\n          storedEvents.push({\n            streamId: op.streamId,\n            type: event.type,\n            data: event.data,\n            metadata: event.metadata,\n            revision,\n            globalPosition: this.segmentManager.allocateGlobalPosition(),\n            timestamp,\n            tenantId\n          });\n          revision++;\n        }\n        this.streamRevisions.set(op.streamId, revision - 1);\n        results.set(op.streamId, {\n          streamRevision: revision - 1,\n          eventCount: op.events.length\n        });\n      }\n      this.pendingEvents.push(...storedEvents);\n      if (this.config.autoFlushCount > 0 && this.pendingEvents.length >= this.config.autoFlushCount) {\n        await this.flushInternal();\n      }\n      return {\n        streams: results,\n        globalPosition: firstGlobalPosition,\n        totalEvents: storedEvents.length\n      };\n    });\n  }\n  async flush() {\n    await this.enqueueWrite(async () => {\n      this.ensureOpen();\n      await this.flushInternal();\n    });\n  }\n  enqueueWrite(fn) {\n    const run = this.writeQueue.then(fn);\n    this.writeQueue = run.then(() => {\n      return;\n    }, () => {\n      return;\n    });\n    return run;\n  }\n  async flushInternal() {\n    if (this.pendingEvents.length === 0) {\n      return;\n    }\n    const result = await this.segmentManager.writeBatch(this.pendingEvents);\n    try {\n      await this.segmentManager.sync();\n    } catch (error) {\n      this.failed = true;\n      this.failedError = error instanceof Error ? error : new Error(String(error));\n      throw new StoreFatalError(\"Sync failed after write. Store is now in failed state and must be closed and reopened. \" + \"Events may need to be re-appended after recovery.\", this.failedError);\n    }\n    if (!result.alreadyWritten) {\n      for (const event of this.pendingEvents) {\n        this.recentlyFlushedEvents.set(event.globalPosition, event);\n      }\n      this.trimRecentCache();\n    }\n    this.lastFlushedGlobalPosition = this.pendingEvents[this.pendingEvents.length - 1].globalPosition;\n    this.pendingEvents = [];\n  }\n  trimRecentCache() {\n    if (this.recentlyFlushedEvents.size <= this.maxCachedEvents) {\n      return;\n    }\n    const toRemove = this.recentlyFlushedEvents.size - this.maxCachedEvents;\n    let removed = 0;\n    for (const key of this.recentlyFlushedEvents.keys()) {\n      if (removed >= toRemove)\n        break;\n      this.recentlyFlushedEvents.delete(key);\n      removed++;\n    }\n  }\n  readGlobalCached(fromPosition, maxCount) {\n    if (fromPosition > this.lastFlushedGlobalPosition) {\n      return null;\n    }\n    const events = [];\n    for (let i = 0;i < maxCount; i++) {\n      const pos = fromPosition + i;\n      if (pos > this.lastFlushedGlobalPosition) {\n        break;\n      }\n      const cached = this.recentlyFlushedEvents.get(pos);\n      if (!cached) {\n        return null;\n      }\n      events.push(cached);\n    }\n    return events;\n  }\n  async readStream(streamId, options = {}) {\n    this.ensureOpen();\n    const events = [];\n    const maxCount = options.maxCount ?? Infinity;\n    for await (const event of this.streamEvents(streamId, options)) {\n      events.push(event);\n      if (events.length >= maxCount) {\n        break;\n      }\n    }\n    return events;\n  }\n  async* streamEvents(streamId, options = {}) {\n    this.ensureOpen();\n    const fromRevision = options.fromRevision ?? 0;\n    const toRevision = options.toRevision;\n    const direction = options.direction ?? \"forward\";\n    const maxCount = options.maxCount ?? Infinity;\n    const durableEvents = await this.readDurableStreamEvents(streamId, fromRevision, toRevision);\n    const pendingEvents = this.pendingEvents.filter((event) => {\n      if (event.streamId !== streamId)\n        return false;\n      if (event.revision < fromRevision)\n        return false;\n      if (toRevision !== undefined && event.revision > toRevision)\n        return false;\n      return true;\n    });\n    const combined = [...durableEvents, ...pendingEvents];\n    combined.sort((a, b) => a.revision - b.revision);\n    if (direction === \"backward\") {\n      combined.reverse();\n    }\n    let yielded = 0;\n    for (const event of combined) {\n      if (yielded >= maxCount) {\n        break;\n      }\n      yield event;\n      yielded++;\n    }\n  }\n  async readGlobal(fromPosition = 0, options = {}) {\n    this.ensureOpen();\n    const events = [];\n    const maxCount = options.maxCount ?? Infinity;\n    for await (const event of this.streamGlobal(fromPosition)) {\n      events.push(event);\n      if (events.length >= maxCount) {\n        break;\n      }\n    }\n    return events;\n  }\n  async readGlobalDurable(fromPosition = 0, options = {}) {\n    this.ensureOpen();\n    const events = [];\n    const maxCount = options.maxCount ?? Infinity;\n    for await (const event of this.streamGlobalDurable(fromPosition)) {\n      events.push(event);\n      if (events.length >= maxCount) {\n        break;\n      }\n    }\n    return events;\n  }\n  async* streamGlobalDurable(fromPosition = 0) {\n    this.ensureOpen();\n    const maxDurablePosition = this.lastFlushedGlobalPosition;\n    if (maxDurablePosition < 0 || maxDurablePosition < fromPosition) {\n      return;\n    }\n    let lastPosition = fromPosition > 0 ? fromPosition - 1 : -1;\n    const reader = this.segmentManager.getReader();\n    const segments = this.segmentManager.getSegments();\n    let startIndex = 0;\n    if (segments.length > 0 && fromPosition > segments[0].basePosition) {\n      for (let i = 0;i < segments.length; i += 1) {\n        const segment = segments[i];\n        const next = segments[i + 1];\n        if (fromPosition >= segment.basePosition && (!next || fromPosition < next.basePosition)) {\n          startIndex = i;\n          break;\n        }\n      }\n    }\n    for (let i = startIndex;i < segments.length; i += 1) {\n      const segment = segments[i];\n      if (segment.basePosition > maxDurablePosition) {\n        break;\n      }\n      let startOffset = SEGMENT_HEADER_SIZE;\n      if (i === startIndex && fromPosition > segment.basePosition) {\n        try {\n          const indexFile = await this.segmentManager.getSegmentIndexFile(segment.id);\n          const batchOffset = indexFile.findBatchOffsetForGlobalPosition(fromPosition);\n          if (batchOffset !== null) {\n            startOffset = batchOffset;\n          }\n        } catch {}\n      }\n      for await (const batch of reader.readAllBatches(segment.path, startOffset)) {\n        for (const event of batch) {\n          if (event.globalPosition < fromPosition) {\n            continue;\n          }\n          if (event.globalPosition > maxDurablePosition) {\n            return;\n          }\n          if (event.globalPosition <= lastPosition) {\n            return;\n          }\n          yield event;\n          lastPosition = event.globalPosition;\n        }\n      }\n    }\n  }\n  async* streamGlobal(fromPosition = 0) {\n    this.ensureOpen();\n    const maxDurablePosition = this.lastFlushedGlobalPosition;\n    let lastPosition = fromPosition > 0 ? fromPosition - 1 : -1;\n    const pendingSnapshot = this.pendingEvents.slice();\n    if (maxDurablePosition >= 0 && maxDurablePosition >= fromPosition) {\n      const reader = this.segmentManager.getReader();\n      const segments = this.segmentManager.getSegments();\n      let startIndex = 0;\n      if (segments.length > 0 && fromPosition > segments[0].basePosition) {\n        for (let i = 0;i < segments.length; i += 1) {\n          const segment = segments[i];\n          const next = segments[i + 1];\n          if (fromPosition >= segment.basePosition && (!next || fromPosition < next.basePosition)) {\n            startIndex = i;\n            break;\n          }\n        }\n      }\n      readDurable:\n        for (let i = startIndex;i < segments.length; i += 1) {\n          const segment = segments[i];\n          if (segment.basePosition > maxDurablePosition) {\n            break;\n          }\n          let startOffset = SEGMENT_HEADER_SIZE;\n          if (i === startIndex && fromPosition > segment.basePosition) {\n            try {\n              const indexFile = await this.segmentManager.getSegmentIndexFile(segment.id);\n              const batchOffset = indexFile.findBatchOffsetForGlobalPosition(fromPosition);\n              if (batchOffset !== null) {\n                startOffset = batchOffset;\n              }\n            } catch {}\n          }\n          for await (const batch of reader.readAllBatches(segment.path, startOffset)) {\n            for (const event of batch) {\n              if (event.globalPosition < fromPosition) {\n                continue;\n              }\n              if (event.globalPosition > maxDurablePosition) {\n                break readDurable;\n              }\n              if (event.globalPosition <= lastPosition) {\n                return;\n              }\n              yield event;\n              lastPosition = event.globalPosition;\n            }\n          }\n        }\n    }\n    if (pendingSnapshot.length > 0) {\n      for (const event of pendingSnapshot) {\n        if (event.globalPosition <= maxDurablePosition) {\n          continue;\n        }\n        if (event.globalPosition >= fromPosition) {\n          if (event.globalPosition <= lastPosition) {\n            return;\n          }\n          yield event;\n          lastPosition = event.globalPosition;\n        }\n      }\n    }\n  }\n  getStreamRevision(streamId) {\n    const cached = this.streamRevisions.get(streamId);\n    if (cached !== undefined) {\n      return cached;\n    }\n    for (let i = this.pendingEvents.length - 1;i >= 0; i--) {\n      if (this.pendingEvents[i].streamId === streamId) {\n        return this.pendingEvents[i].revision;\n      }\n    }\n    return -1;\n  }\n  hasStream(streamId) {\n    return this.getStreamRevision(streamId) >= 0;\n  }\n  async getStreamIds() {\n    this.ensureOpen();\n    const streamIds = new Set(this.segmentManager.getAllStreamIds());\n    for (const event of this.pendingEvents) {\n      streamIds.add(event.streamId);\n    }\n    return Array.from(streamIds).sort();\n  }\n  getGlobalPosition() {\n    this.ensureOpen();\n    return this.segmentManager.getGlobalPosition();\n  }\n  getDurableGlobalPosition() {\n    this.ensureOpen();\n    return this.lastFlushedGlobalPosition;\n  }\n  isOpen() {\n    return this.segmentManager !== null;\n  }\n  async rebuildStreamRevisions() {\n    for (const segment of this.segmentManager.getSegments()) {\n      const index = await this.segmentManager.getIndex(segment.id);\n      for (const streamId of index.getStreamIds()) {\n        const currentMax = this.streamRevisions.get(streamId) ?? -1;\n        const segmentMax = index.getStreamRevision(streamId);\n        if (segmentMax > currentMax) {\n          this.streamRevisions.set(streamId, segmentMax);\n        }\n      }\n    }\n  }\n  async readDurableStreamEvents(streamId, fromRevision = 0, toRevision) {\n    const maxDurablePosition = this.lastFlushedGlobalPosition;\n    if (maxDurablePosition < 0) {\n      return [];\n    }\n    const segmentIds = this.segmentManager.getStreamSegments(streamId);\n    if (segmentIds.length === 0) {\n      return [];\n    }\n    const events = [];\n    const batchCache = new Map;\n    const reader = this.segmentManager.getReader();\n    for (const segmentId of segmentIds) {\n      const segment = this.segmentManager.getSegment(segmentId);\n      if (!segment) {\n        continue;\n      }\n      if (segment.basePosition > maxDurablePosition) {\n        continue;\n      }\n      if (segment.isActive) {\n        for await (const batch of reader.readAllBatches(segment.path)) {\n          for (const event of batch) {\n            if (event.streamId !== streamId) {\n              continue;\n            }\n            if (event.globalPosition > maxDurablePosition) {\n              continue;\n            }\n            if (event.revision < fromRevision) {\n              continue;\n            }\n            if (toRevision !== undefined && event.revision > toRevision) {\n              continue;\n            }\n            events.push(event);\n          }\n        }\n        continue;\n      }\n      const indexFile = await this.segmentManager.getSegmentIndexFile(segmentId);\n      const entries = indexFile.findByStream(streamId, fromRevision, toRevision);\n      for (const entry of entries) {\n        if (entry.globalPosition > maxDurablePosition) {\n          continue;\n        }\n        const cacheKey = `${segmentId}:${entry.batchOffset}`;\n        let batch = batchCache.get(cacheKey);\n        if (!batch) {\n          try {\n            batch = await reader.readBatch(segment.path, entry.batchOffset);\n          } catch {\n            continue;\n          }\n          batchCache.set(cacheKey, batch);\n        }\n        const event = batch.find((e) => e.streamId === streamId && e.revision === entry.revision);\n        if (event) {\n          events.push(event);\n        }\n      }\n    }\n    return events;\n  }\n  ensureOpen() {\n    if (!this.segmentManager) {\n      throw new Error(\"EventStore not open. Call open() first.\");\n    }\n    if (this.failed) {\n      throw new StoreFatalError(\"EventStore is in failed state due to previous sync error. Close and reopen to recover.\", this.failedError ?? undefined);\n    }\n  }\n  isFailed() {\n    return this.failed;\n  }\n  getFailedError() {\n    return this.failedError;\n  }\n}\n// src/errors/spitedb-error.ts\nclass SpiteDBError extends Error {\n  constructor(message) {\n    super(`[SpiteDB] ${message}`);\n    this.name = \"SpiteDBError\";\n    Object.setPrototypeOf(this, SpiteDBError.prototype);\n  }\n}\n\nclass SpiteDBNotOpenError extends SpiteDBError {\n  constructor() {\n    super(\"Database is not open. Call SpiteDB.open() first.\");\n    this.name = \"SpiteDBNotOpenError\";\n    Object.setPrototypeOf(this, SpiteDBNotOpenError.prototype);\n  }\n}\n\nclass ProjectionsNotStartedError extends SpiteDBError {\n  constructor() {\n    super(\"Projections not started. Call startProjections() first.\");\n    this.name = \"ProjectionsNotStartedError\";\n    Object.setPrototypeOf(this, ProjectionsNotStartedError.prototype);\n  }\n}\n\nclass ProjectionBackpressureError extends SpiteDBError {\n  projectionName;\n  lag;\n  maxLag;\n  constructor(projectionName, lag, maxLag) {\n    super(`Append blocked: projection '${projectionName}' is lagging by ${lag} (max ${maxLag}).`);\n    this.projectionName = projectionName;\n    this.lag = lag;\n    this.maxLag = maxLag;\n    this.name = \"ProjectionBackpressureError\";\n    Object.setPrototypeOf(this, ProjectionBackpressureError.prototype);\n  }\n}\n\nclass ProjectionBackpressureTimeoutError extends ProjectionBackpressureError {\n  waitedMs;\n  maxWaitMs;\n  constructor(projectionName, lag, maxLag, waitedMs, maxWaitMs) {\n    super(projectionName, lag, maxLag);\n    this.waitedMs = waitedMs;\n    this.maxWaitMs = maxWaitMs;\n    this.name = \"ProjectionBackpressureTimeoutError\";\n    Object.setPrototypeOf(this, ProjectionBackpressureTimeoutError.prototype);\n  }\n}\n// src/errors/projection-errors.ts\nclass ProjectionError extends Error {\n  projectionName;\n  constructor(message, projectionName) {\n    super(`[Projection: ${projectionName}] ${message}`);\n    this.projectionName = projectionName;\n    this.name = \"ProjectionError\";\n    Object.setPrototypeOf(this, ProjectionError.prototype);\n  }\n}\n\nclass ProjectionBuildError extends ProjectionError {\n  eventType;\n  globalPosition;\n  cause;\n  constructor(projectionName, eventType, globalPosition, cause) {\n    super(`Failed to process event '${eventType}' at position ${globalPosition}: ${cause.message}`, projectionName);\n    this.eventType = eventType;\n    this.globalPosition = globalPosition;\n    this.cause = cause;\n    this.name = \"ProjectionBuildError\";\n    Object.setPrototypeOf(this, ProjectionBuildError.prototype);\n  }\n}\n\nclass ProjectionNotFoundError extends ProjectionError {\n  constructor(projectionName) {\n    super(`Projection not found`, projectionName);\n    this.name = \"ProjectionNotFoundError\";\n    Object.setPrototypeOf(this, ProjectionNotFoundError.prototype);\n  }\n}\n\nclass ProjectionAlreadyRegisteredError extends ProjectionError {\n  constructor(projectionName) {\n    super(`Projection is already registered`, projectionName);\n    this.name = \"ProjectionAlreadyRegisteredError\";\n    Object.setPrototypeOf(this, ProjectionAlreadyRegisteredError.prototype);\n  }\n}\n\nclass ProjectionDisabledError extends ProjectionError {\n  constructor(projectionName) {\n    super(`Projection is disabled`, projectionName);\n    this.name = \"ProjectionDisabledError\";\n    Object.setPrototypeOf(this, ProjectionDisabledError.prototype);\n  }\n}\n\nclass ProjectionCoordinatorError extends Error {\n  constructor(message) {\n    super(message);\n    this.name = \"ProjectionCoordinatorError\";\n    Object.setPrototypeOf(this, ProjectionCoordinatorError.prototype);\n  }\n}\n\nclass ProjectionCatchUpTimeoutError extends ProjectionCoordinatorError {\n  projectionName;\n  currentPosition;\n  targetPosition;\n  timeoutMs;\n  constructor(projectionName, currentPosition, targetPosition, timeoutMs) {\n    super(`Projection '${projectionName}' failed to catch up: ` + `at position ${currentPosition}, target ${targetPosition}, timeout ${timeoutMs}ms`);\n    this.projectionName = projectionName;\n    this.currentPosition = currentPosition;\n    this.targetPosition = targetPosition;\n    this.timeoutMs = timeoutMs;\n    this.name = \"ProjectionCatchUpTimeoutError\";\n    Object.setPrototypeOf(this, ProjectionCatchUpTimeoutError.prototype);\n  }\n}\n// src/errors/checkpoint-errors.ts\nclass CheckpointWriteError extends Error {\n  projectionName;\n  path;\n  cause;\n  constructor(projectionName, path, cause) {\n    super(`Failed to write checkpoint for '${projectionName}' to ${path}: ${cause.message}`);\n    this.projectionName = projectionName;\n    this.path = path;\n    this.cause = cause;\n    this.name = \"CheckpointWriteError\";\n    Object.setPrototypeOf(this, CheckpointWriteError.prototype);\n  }\n}\n\nclass CheckpointLoadError extends Error {\n  projectionName;\n  path;\n  cause;\n  constructor(projectionName, path, cause) {\n    super(`Failed to load checkpoint for '${projectionName}' from ${path}: ${cause.message}`);\n    this.projectionName = projectionName;\n    this.path = path;\n    this.cause = cause;\n    this.name = \"CheckpointLoadError\";\n    Object.setPrototypeOf(this, CheckpointLoadError.prototype);\n  }\n}\n\nclass CheckpointCorruptionError extends Error {\n  projectionName;\n  path;\n  reason;\n  constructor(projectionName, path, reason) {\n    super(`Corrupted checkpoint for '${projectionName}' at ${path}: ${reason}`);\n    this.projectionName = projectionName;\n    this.path = path;\n    this.reason = reason;\n    this.name = \"CheckpointCorruptionError\";\n    Object.setPrototypeOf(this, CheckpointCorruptionError.prototype);\n  }\n}\n\nclass CheckpointVersionError extends Error {\n  projectionName;\n  path;\n  version;\n  supportedVersions;\n  constructor(projectionName, path, version, supportedVersions) {\n    super(`Unsupported checkpoint version ${version} for '${projectionName}' at ${path}. ` + `Supported versions: ${supportedVersions.join(\", \")}`);\n    this.projectionName = projectionName;\n    this.path = path;\n    this.version = version;\n    this.supportedVersions = supportedVersions;\n    this.name = \"CheckpointVersionError\";\n    Object.setPrototypeOf(this, CheckpointVersionError.prototype);\n  }\n}\n// src/application/projections/checkpoint-manager.ts\nvar CHECKPOINT_MAGIC = 1128811344;\nvar CHECKPOINT_VERSION = 1;\nvar SUPPORTED_VERSIONS = [1];\nvar CHECKPOINT_HEADER_SIZE = 32;\n\nclass CheckpointManager {\n  fs;\n  serializer;\n  clock;\n  dataDir;\n  jitterMs;\n  scheduledTimes = new Map;\n  constructor(config) {\n    this.fs = config.fs;\n    this.serializer = config.serializer;\n    this.clock = config.clock;\n    this.dataDir = config.dataDir;\n    this.jitterMs = config.jitterMs ?? 1000;\n  }\n  async initialize() {\n    if (!await this.fs.exists(this.dataDir)) {\n      await this.fs.mkdir(this.dataDir, { recursive: true });\n    }\n  }\n  scheduleCheckpoint(projectionName, baseIntervalMs) {\n    const now = this.clock.now();\n    const lastScheduled = this.scheduledTimes.get(projectionName) ?? now;\n    const jitter = (Math.random() * 2 - 1) * this.jitterMs;\n    const nextTime = Math.max(now, lastScheduled) + baseIntervalMs + jitter;\n    this.scheduledTimes.set(projectionName, nextTime);\n    return nextTime;\n  }\n  shouldCheckpoint(projectionName) {\n    const scheduled = this.scheduledTimes.get(projectionName);\n    if (scheduled === undefined)\n      return false;\n    return this.clock.now() >= scheduled;\n  }\n  async writeCheckpoint(checkpoint) {\n    const path = this.getCheckpointPath(checkpoint.projectionName);\n    const tempPath = path + \".tmp\";\n    try {\n      if (!Number.isSafeInteger(checkpoint.position) || checkpoint.position < 0) {\n        throw new Error(\"Checkpoint position must be a non-negative safe integer\");\n      }\n      if (!Number.isSafeInteger(checkpoint.timestamp) || checkpoint.timestamp < 0) {\n        throw new Error(\"Checkpoint timestamp must be a non-negative safe integer\");\n      }\n      const stateData = this.serializer.encode(checkpoint.state);\n      const totalSize = CHECKPOINT_HEADER_SIZE + stateData.length;\n      const buffer = new Uint8Array(totalSize);\n      const view = new DataView(buffer.buffer);\n      view.setUint32(0, CHECKPOINT_MAGIC, true);\n      view.setUint32(4, CHECKPOINT_VERSION, true);\n      view.setBigUint64(8, BigInt(checkpoint.position), true);\n      view.setBigUint64(16, BigInt(checkpoint.timestamp), true);\n      view.setUint32(24, stateData.length, true);\n      view.setUint32(28, 0, true);\n      buffer.set(stateData, CHECKPOINT_HEADER_SIZE);\n      const beforeChecksum = buffer.subarray(0, 28);\n      const afterChecksum = buffer.subarray(32);\n      const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n      checksumData.set(beforeChecksum);\n      checksumData.set(afterChecksum, beforeChecksum.length);\n      const checksum = crc32(checksumData);\n      view.setUint32(28, checksum, true);\n      const handle = await this.fs.open(tempPath, \"write\");\n      try {\n        await this.fs.write(handle, buffer);\n        await this.fs.sync(handle);\n      } finally {\n        await this.fs.close(handle);\n      }\n      await this.fs.rename(tempPath, path);\n    } catch (error) {\n      try {\n        if (await this.fs.exists(tempPath)) {\n          await this.fs.unlink(tempPath);\n        }\n      } catch {}\n      throw new CheckpointWriteError(checkpoint.projectionName, path, error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n  async loadCheckpoint(projectionName) {\n    const path = this.getCheckpointPath(projectionName);\n    if (!await this.fs.exists(path)) {\n      return null;\n    }\n    try {\n      const data = await this.fs.readFile(path);\n      if (data.length < CHECKPOINT_HEADER_SIZE) {\n        throw new CheckpointCorruptionError(projectionName, path, `File too small: ${data.length} bytes, expected at least ${CHECKPOINT_HEADER_SIZE}`);\n      }\n      const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n      const header = {\n        magic: view.getUint32(0, true),\n        version: view.getUint32(4, true),\n        position: Number(view.getBigUint64(8, true)),\n        timestamp: Number(view.getBigUint64(16, true)),\n        stateLength: view.getUint32(24, true),\n        checksum: view.getUint32(28, true)\n      };\n      if (header.magic !== CHECKPOINT_MAGIC) {\n        throw new CheckpointCorruptionError(projectionName, path, `Invalid magic: expected 0x${CHECKPOINT_MAGIC.toString(16)}, got 0x${header.magic.toString(16)}`);\n      }\n      if (!SUPPORTED_VERSIONS.includes(header.version)) {\n        throw new CheckpointVersionError(projectionName, path, header.version, SUPPORTED_VERSIONS);\n      }\n      const expectedSize = CHECKPOINT_HEADER_SIZE + header.stateLength;\n      if (data.length < expectedSize) {\n        throw new CheckpointCorruptionError(projectionName, path, `File too small for state: ${data.length} bytes, expected ${expectedSize}`);\n      }\n      const beforeChecksum = data.subarray(0, 28);\n      const afterChecksum = data.subarray(32);\n      const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n      checksumData.set(beforeChecksum);\n      checksumData.set(afterChecksum, beforeChecksum.length);\n      const calculatedChecksum = crc32(checksumData);\n      if (calculatedChecksum !== header.checksum) {\n        throw new CheckpointCorruptionError(projectionName, path, `Checksum mismatch: expected 0x${header.checksum.toString(16)}, calculated 0x${calculatedChecksum.toString(16)}`);\n      }\n      const stateData = data.subarray(CHECKPOINT_HEADER_SIZE, expectedSize);\n      const state = this.serializer.decode(stateData);\n      return {\n        projectionName,\n        position: header.position,\n        state,\n        timestamp: Number(header.timestamp)\n      };\n    } catch (error) {\n      if (error instanceof CheckpointCorruptionError || error instanceof CheckpointVersionError) {\n        throw error;\n      }\n      throw new CheckpointLoadError(projectionName, path, error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n  async deleteCheckpoint(projectionName) {\n    const path = this.getCheckpointPath(projectionName);\n    if (!await this.fs.exists(path)) {\n      return false;\n    }\n    await this.fs.unlink(path);\n    this.scheduledTimes.delete(projectionName);\n    return true;\n  }\n  async listCheckpoints() {\n    if (!await this.fs.exists(this.dataDir)) {\n      return [];\n    }\n    const files = await this.fs.readdir(this.dataDir);\n    return files.filter((f) => f.endsWith(\".ckpt\")).map((f) => f.slice(0, -5));\n  }\n  getCheckpointPath(projectionName) {\n    const safeName = projectionName.replace(/[^a-zA-Z0-9_-]/g, \"_\");\n    return `${this.dataDir}/${safeName}.ckpt`;\n  }\n}\n// src/application/projections/projection-runner.ts\nclass ProjectionRunner {\n  eventStore;\n  projection;\n  store;\n  metadata;\n  clock;\n  pollingIntervalMs;\n  checkpointIntervalMs;\n  jitterMs;\n  eventFilter;\n  batchSize;\n  useIncrementalStore;\n  useSharedReader;\n  subscribedEventTypes;\n  currentPosition = -1;\n  pollingTimer = null;\n  running = false;\n  eventsProcessed = 0;\n  errorsCount = 0;\n  lastCheckpointPosition = -1;\n  lastCheckpointTime = 0;\n  nextCheckpointTime = 0;\n  stateDirty = false;\n  constructor(config) {\n    this.eventStore = config.eventStore;\n    this.projection = config.projection;\n    this.store = config.store;\n    this.metadata = config.metadata;\n    this.clock = config.clock;\n    this.pollingIntervalMs = config.pollingIntervalMs ?? 100;\n    this.checkpointIntervalMs = config.checkpointIntervalMs ?? 5000;\n    this.jitterMs = config.jitterMs ?? 1000;\n    this.eventFilter = config.eventFilter;\n    this.batchSize = config.batchSize ?? 100;\n    this.useSharedReader = config.useSharedReader ?? false;\n    this.useIncrementalStore = typeof this.projection.applyToStore === \"function\" && typeof this.store.setByKey === \"function\";\n    this.subscribedEventTypes = new Set(config.metadata.subscribedEvents);\n  }\n  async start() {\n    if (this.running) {\n      return;\n    }\n    const checkpointPosition = await this.store.load();\n    if (checkpointPosition !== null) {\n      this.currentPosition = checkpointPosition;\n      this.lastCheckpointPosition = checkpointPosition;\n      this.lastCheckpointTime = this.clock.now();\n      this.projection.setState(this.store.get());\n      this.stateDirty = false;\n    }\n    this.scheduleNextCheckpoint();\n    this.running = true;\n    if (!this.useSharedReader) {\n      this.poll();\n    }\n  }\n  async stop() {\n    if (!this.running) {\n      return;\n    }\n    this.running = false;\n    if (this.pollingTimer) {\n      this.pollingTimer.cancel();\n      this.pollingTimer = null;\n    }\n    await this.checkpoint();\n  }\n  getStatus() {\n    return {\n      name: this.metadata.name,\n      kind: this.metadata.kind,\n      running: this.running,\n      currentPosition: this.currentPosition,\n      eventsProcessed: this.eventsProcessed,\n      errorsCount: this.errorsCount,\n      lastCheckpointPosition: this.lastCheckpointPosition,\n      lastCheckpointTime: this.lastCheckpointTime,\n      nextCheckpointTime: this.nextCheckpointTime\n    };\n  }\n  getCurrentPosition() {\n    return this.currentPosition;\n  }\n  processBatch(events) {\n    if (!this.running || events.length === 0) {\n      return;\n    }\n    this.applyEvents(events);\n  }\n  async checkpointIfNeeded() {\n    await this.maybeCheckpoint();\n  }\n  async forceCheckpoint() {\n    await this.checkpoint();\n  }\n  async waitForCatchUp(targetPosition, timeoutMs = 30000) {\n    const startTime = this.clock.now();\n    const realStartTime = Date.now();\n    const isSimulatedClock = typeof this.clock.tick === \"function\";\n    while (this.currentPosition < targetPosition) {\n      const elapsed = this.clock.now() - startTime;\n      const realElapsed = Date.now() - realStartTime;\n      if (elapsed > timeoutMs || realElapsed > timeoutMs) {\n        return false;\n      }\n      if (this.running) {\n        const catchUpBudget = Math.min(1000, timeoutMs);\n        await this.processEvents(catchUpBudget);\n      }\n      const remaining = timeoutMs - (this.clock.now() - startTime);\n      if (remaining <= 0) {\n        return false;\n      }\n      if (isSimulatedClock) {\n        await Promise.resolve();\n      } else {\n        await this.clock.sleep(Math.min(10, remaining));\n      }\n    }\n    return true;\n  }\n  poll() {\n    if (!this.running) {\n      return;\n    }\n    this.pollingTimer = this.clock.setTimeout(async () => {\n      try {\n        await this.processEvents();\n        await this.maybeCheckpoint();\n      } catch (error) {\n        console.error(`[Projection: ${this.metadata.name}] Poll error:`, error);\n        this.errorsCount++;\n      }\n      this.poll();\n    }, this.pollingIntervalMs);\n  }\n  async processEvents(maxDurationMs) {\n    const durationBudget = maxDurationMs ?? this.pollingIntervalMs;\n    const startTime = this.clock.now();\n    const realStartTime = Date.now();\n    while (this.currentPosition < this.eventStore.getDurableGlobalPosition()) {\n      const events = await this.eventStore.readGlobalDurable(this.currentPosition, {\n        maxCount: this.batchSize\n      });\n      if (events.length === 0) {\n        break;\n      }\n      this.applyEvents(events);\n      if (!this.running) {\n        break;\n      }\n      const elapsed = this.clock.now() - startTime;\n      const realElapsed = Date.now() - realStartTime;\n      if (elapsed >= durationBudget || realElapsed >= durationBudget) {\n        break;\n      }\n      if (events.length < this.batchSize) {\n        break;\n      }\n    }\n    this.commitStateIfNeeded();\n  }\n  applyEvents(events) {\n    let updated = false;\n    for (const event of events) {\n      if (event.globalPosition <= this.currentPosition) {\n        continue;\n      }\n      const subscribesAll = this.subscribedEventTypes.has(\"*\");\n      if (!subscribesAll && !this.subscribedEventTypes.has(event.type)) {\n        this.currentPosition = event.globalPosition;\n        continue;\n      }\n      if (this.eventFilter && !this.eventFilter(event)) {\n        this.currentPosition = event.globalPosition;\n        continue;\n      }\n      try {\n        this.projection.build(event);\n        if (this.useIncrementalStore) {\n          this.projection.applyToStore?.(event, this.store);\n        }\n        this.eventsProcessed++;\n        this.currentPosition = event.globalPosition;\n        updated = true;\n      } catch (error) {\n        this.errorsCount++;\n        throw new ProjectionBuildError(this.metadata.name, event.type, event.globalPosition, error instanceof Error ? error : new Error(String(error)));\n      }\n    }\n    if (updated && !this.useIncrementalStore) {\n      this.stateDirty = true;\n    }\n  }\n  commitStateIfNeeded() {\n    if (!this.useIncrementalStore && this.stateDirty) {\n      this.store.set(this.projection.getState());\n      this.stateDirty = false;\n    }\n  }\n  async maybeCheckpoint() {\n    const now = this.clock.now();\n    if (now >= this.nextCheckpointTime) {\n      await this.checkpoint();\n    }\n  }\n  async checkpoint() {\n    if (this.currentPosition === this.lastCheckpointPosition) {\n      this.scheduleNextCheckpoint();\n      return;\n    }\n    try {\n      this.commitStateIfNeeded();\n      await this.store.persist(this.currentPosition);\n      this.lastCheckpointPosition = this.currentPosition;\n      this.lastCheckpointTime = this.clock.now();\n    } catch (error) {\n      console.error(`[Projection: ${this.metadata.name}] Checkpoint error:`, error);\n      this.errorsCount++;\n    }\n    this.scheduleNextCheckpoint();\n  }\n  scheduleNextCheckpoint() {\n    const now = this.clock.now();\n    const jitter = (Math.random() * 2 - 1) * this.jitterMs;\n    this.nextCheckpointTime = now + this.checkpointIntervalMs + jitter;\n  }\n}\n// src/infrastructure/projections/default-registry.ts\nclass DefaultProjectionRegistry {\n  registrations = new Map;\n  defaultOptions = {\n    checkpointIntervalMs: 5000,\n    memoryThresholdBytes: 50 * 1024 * 1024,\n    enabled: true\n  };\n  register(registration, options = {}) {\n    const { name } = registration.metadata;\n    if (this.registrations.has(name)) {\n      throw new Error(`Projection '${name}' is already registered`);\n    }\n    const resolved = {\n      registration,\n      options: {\n        checkpointIntervalMs: options.checkpointIntervalMs ?? registration.metadata.checkpointIntervalMs ?? this.defaultOptions.checkpointIntervalMs,\n        memoryThresholdBytes: options.memoryThresholdBytes ?? this.defaultOptions.memoryThresholdBytes,\n        enabled: options.enabled ?? this.defaultOptions.enabled,\n        eventFilter: options.eventFilter\n      }\n    };\n    this.registrations.set(name, resolved);\n  }\n  getAll() {\n    return Array.from(this.registrations.values());\n  }\n  get(name) {\n    return this.registrations.get(name);\n  }\n  has(name) {\n    return this.registrations.has(name);\n  }\n  count() {\n    return this.registrations.size;\n  }\n  clear() {\n    this.registrations.clear();\n  }\n}\n\n// src/infrastructure/projections/stores/aggregator-store.ts\nvar AGGREGATOR_MAGIC = 1095189328;\nvar AGGREGATOR_VERSION = 1;\nvar SUPPORTED_VERSIONS2 = [1];\nvar HEADER_SIZE = 32;\n\nclass AggregatorStore {\n  fs;\n  serializer;\n  clock;\n  dataDir;\n  projectionName;\n  state;\n  initialState;\n  constructor(config, initialState) {\n    this.fs = config.fs;\n    this.serializer = config.serializer;\n    this.clock = config.clock;\n    this.dataDir = config.dataDir;\n    this.projectionName = config.projectionName;\n    this.initialState = typeof initialState === \"object\" && initialState !== null ? JSON.parse(JSON.stringify(initialState)) : initialState;\n    this.state = this.cloneState(this.initialState);\n  }\n  async initialize() {\n    if (!await this.fs.exists(this.dataDir)) {\n      await this.fs.mkdir(this.dataDir, { recursive: true });\n    }\n  }\n  get() {\n    return this.state;\n  }\n  set(state) {\n    this.state = state;\n  }\n  async persist(position) {\n    const path = this.getCheckpointPath();\n    const tempPath = path + \".tmp\";\n    try {\n      const stateData = this.serializer.encode(this.state);\n      const totalSize = HEADER_SIZE + stateData.length;\n      const buffer = new Uint8Array(totalSize);\n      const view = new DataView(buffer.buffer);\n      const timestamp = BigInt(this.clock.now());\n      view.setUint32(0, AGGREGATOR_MAGIC, true);\n      view.setUint32(4, AGGREGATOR_VERSION, true);\n      view.setBigUint64(8, BigInt(position), true);\n      view.setBigUint64(16, timestamp, true);\n      view.setUint32(24, stateData.length, true);\n      view.setUint32(28, 0, true);\n      buffer.set(stateData, HEADER_SIZE);\n      const beforeChecksum = buffer.subarray(0, 28);\n      const afterChecksum = buffer.subarray(32);\n      const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n      checksumData.set(beforeChecksum);\n      checksumData.set(afterChecksum, beforeChecksum.length);\n      const checksum = crc32(checksumData);\n      view.setUint32(28, checksum, true);\n      const handle = await this.fs.open(tempPath, \"write\");\n      try {\n        await this.fs.write(handle, buffer);\n        await this.fs.sync(handle);\n      } finally {\n        await this.fs.close(handle);\n      }\n      await this.fs.rename(tempPath, path);\n    } catch (error) {\n      try {\n        if (await this.fs.exists(tempPath)) {\n          await this.fs.unlink(tempPath);\n        }\n      } catch {}\n      throw new CheckpointWriteError(this.projectionName, path, error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n  async load() {\n    const path = this.getCheckpointPath();\n    if (!await this.fs.exists(path)) {\n      return null;\n    }\n    try {\n      const data = await this.fs.readFile(path);\n      if (data.length < HEADER_SIZE) {\n        throw new CheckpointCorruptionError(this.projectionName, path, `File too small: ${data.length} bytes`);\n      }\n      const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n      const magic = view.getUint32(0, true);\n      const version = view.getUint32(4, true);\n      const position = Number(view.getBigUint64(8, true));\n      const stateLength = view.getUint32(24, true);\n      const storedChecksum = view.getUint32(28, true);\n      if (magic !== AGGREGATOR_MAGIC) {\n        throw new CheckpointCorruptionError(this.projectionName, path, `Invalid magic: 0x${magic.toString(16)}`);\n      }\n      if (!SUPPORTED_VERSIONS2.includes(version)) {\n        throw new CheckpointVersionError(this.projectionName, path, version, SUPPORTED_VERSIONS2);\n      }\n      const expectedSize = HEADER_SIZE + stateLength;\n      if (data.length < expectedSize) {\n        throw new CheckpointCorruptionError(this.projectionName, path, `File too small for state: ${data.length} bytes, expected ${expectedSize}`);\n      }\n      const beforeChecksum = data.subarray(0, 28);\n      const afterChecksum = data.subarray(32);\n      const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n      checksumData.set(beforeChecksum);\n      checksumData.set(afterChecksum, beforeChecksum.length);\n      const calculatedChecksum = crc32(checksumData);\n      if (calculatedChecksum !== storedChecksum) {\n        throw new CheckpointCorruptionError(this.projectionName, path, `Checksum mismatch: expected 0x${storedChecksum.toString(16)}, got 0x${calculatedChecksum.toString(16)}`);\n      }\n      const stateData = data.subarray(HEADER_SIZE, expectedSize);\n      this.state = this.serializer.decode(stateData);\n      return position;\n    } catch (error) {\n      if (error instanceof CheckpointCorruptionError || error instanceof CheckpointVersionError) {\n        throw error;\n      }\n      throw new CheckpointLoadError(this.projectionName, path, error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n  async close() {}\n  getMemoryUsage() {\n    try {\n      const encoded = this.serializer.encode(this.state);\n      return encoded.length;\n    } catch {\n      return 0;\n    }\n  }\n  reset() {\n    this.state = this.cloneState(this.initialState);\n  }\n  getCheckpointPath() {\n    const safeName = this.projectionName.replace(/[^a-zA-Z0-9_-]/g, \"_\");\n    return `${this.dataDir}/${safeName}.ckpt`;\n  }\n  cloneState(state) {\n    if (typeof state === \"object\" && state !== null) {\n      return JSON.parse(JSON.stringify(state));\n    }\n    return state;\n  }\n}\n\n// src/infrastructure/projections/stores/index-structures.ts\nclass EqualityIndex {\n  fieldName;\n  index = new Map;\n  reverse = new Map;\n  constructor(fieldName) {\n    this.fieldName = fieldName;\n  }\n  add(primaryKey, fieldValue) {\n    let keys = this.index.get(fieldValue);\n    if (!keys) {\n      keys = new Set;\n      this.index.set(fieldValue, keys);\n    }\n    keys.add(primaryKey);\n    this.reverse.set(primaryKey, fieldValue);\n  }\n  remove(primaryKey, fieldValue) {\n    const keys = this.index.get(fieldValue);\n    if (keys) {\n      keys.delete(primaryKey);\n      if (keys.size === 0) {\n        this.index.delete(fieldValue);\n      }\n    }\n    this.reverse.delete(primaryKey);\n  }\n  update(primaryKey, oldValue, newValue) {\n    if (oldValue === newValue) {\n      return;\n    }\n    this.remove(primaryKey, oldValue);\n    this.add(primaryKey, newValue);\n  }\n  updateFromKey(primaryKey, newValue) {\n    const oldValue = this.reverse.get(primaryKey);\n    if (oldValue !== undefined) {\n      this.remove(primaryKey, oldValue);\n    }\n    this.add(primaryKey, newValue);\n  }\n  find(value) {\n    return this.index.get(value) ?? new Set;\n  }\n  has(value) {\n    const keys = this.index.get(value);\n    return keys !== undefined && keys.size > 0;\n  }\n  getValues() {\n    return Array.from(this.index.keys());\n  }\n  countFor(value) {\n    return this.index.get(value)?.size ?? 0;\n  }\n  get size() {\n    return this.reverse.size;\n  }\n  clear() {\n    this.index.clear();\n    this.reverse.clear();\n  }\n  getMemoryUsage() {\n    let bytes = 0;\n    bytes += this.index.size * 50;\n    bytes += this.reverse.size * 50;\n    for (const key of this.reverse.keys()) {\n      bytes += key.length * 2;\n    }\n    for (const keys of this.index.values()) {\n      bytes += keys.size * 30;\n    }\n    return bytes;\n  }\n  toJSON() {\n    const entries = [];\n    for (const [value, keys] of this.index) {\n      entries.push([value, Array.from(keys)]);\n    }\n    return { entries };\n  }\n  static fromJSON(fieldName, data) {\n    const index = new EqualityIndex(fieldName);\n    for (const [value, keys] of data.entries) {\n      for (const key of keys) {\n        index.add(key, value);\n      }\n    }\n    return index;\n  }\n}\n\nclass SortedIndex {\n  entries = [];\n  keyToIndex = new Map;\n  dirty = false;\n  set(key, value) {\n    const existingIndex = this.keyToIndex.get(key);\n    if (existingIndex !== undefined) {\n      this.entries[existingIndex][1] = value;\n    } else {\n      this.entries.push([key, value]);\n      this.keyToIndex.set(key, this.entries.length - 1);\n      this.dirty = true;\n    }\n  }\n  get(key) {\n    this.ensureSorted();\n    const index = this.binarySearchExact(key);\n    return index >= 0 ? this.entries[index][1] : undefined;\n  }\n  has(key) {\n    return this.keyToIndex.has(key);\n  }\n  delete(key) {\n    const index = this.keyToIndex.get(key);\n    if (index === undefined) {\n      return false;\n    }\n    this.entries.splice(index, 1);\n    this.rebuildKeyToIndex();\n    return true;\n  }\n  queryRange(start, end) {\n    this.ensureSorted();\n    const result = new Map;\n    let startIndex = 0;\n    if (start !== undefined) {\n      startIndex = this.binarySearchLeft(start);\n    }\n    for (let i = startIndex;i < this.entries.length; i++) {\n      const entry = this.entries[i];\n      const key = entry[0];\n      if (end !== undefined && key > end) {\n        break;\n      }\n      result.set(key, entry[1]);\n    }\n    return result;\n  }\n  getAll() {\n    this.ensureSorted();\n    return new Map(this.entries);\n  }\n  get size() {\n    return this.entries.length;\n  }\n  clear() {\n    this.entries = [];\n    this.keyToIndex.clear();\n    this.dirty = false;\n  }\n  getMemoryUsage() {\n    let bytes = 0;\n    bytes += this.entries.length * 30;\n    for (const [key] of this.entries) {\n      bytes += key.length * 2;\n    }\n    bytes += this.keyToIndex.size * 50;\n    return bytes;\n  }\n  toJSON() {\n    this.ensureSorted();\n    return { entries: [...this.entries] };\n  }\n  static fromJSON(data) {\n    const index = new SortedIndex;\n    for (const [key, value] of data.entries) {\n      index.set(key, value);\n    }\n    return index;\n  }\n  ensureSorted() {\n    if (!this.dirty) {\n      return;\n    }\n    this.entries.sort((a, b) => a[0].localeCompare(b[0]));\n    this.rebuildKeyToIndex();\n    this.dirty = false;\n  }\n  rebuildKeyToIndex() {\n    this.keyToIndex.clear();\n    for (let i = 0;i < this.entries.length; i++) {\n      this.keyToIndex.set(this.entries[i][0], i);\n    }\n  }\n  binarySearchExact(key) {\n    let left = 0;\n    let right = this.entries.length - 1;\n    while (left <= right) {\n      const mid = Math.floor((left + right) / 2);\n      const cmp = this.entries[mid][0].localeCompare(key);\n      if (cmp === 0) {\n        return mid;\n      } else if (cmp < 0) {\n        left = mid + 1;\n      } else {\n        right = mid - 1;\n      }\n    }\n    return -1;\n  }\n  binarySearchLeft(key) {\n    let left = 0;\n    let right = this.entries.length;\n    while (left < right) {\n      const mid = Math.floor((left + right) / 2);\n      if (this.entries[mid][0].localeCompare(key) < 0) {\n        left = mid + 1;\n      } else {\n        right = mid;\n      }\n    }\n    return left;\n  }\n}\n\nclass IndexCollection {\n  equalityIndexes = new Map;\n  sortedIndexes = new Map;\n  addEqualityIndex(fieldName) {\n    const index = new EqualityIndex(fieldName);\n    this.equalityIndexes.set(fieldName, index);\n    return index;\n  }\n  addSortedIndex(fieldName) {\n    const index = new SortedIndex;\n    this.sortedIndexes.set(fieldName, index);\n    return index;\n  }\n  getEqualityIndex(fieldName) {\n    return this.equalityIndexes.get(fieldName);\n  }\n  getSortedIndex(fieldName) {\n    return this.sortedIndexes.get(fieldName);\n  }\n  indexRow(primaryKey, row) {\n    for (const [fieldName, index] of this.equalityIndexes) {\n      const value = row[fieldName];\n      if (value !== undefined) {\n        index.updateFromKey(primaryKey, value);\n      }\n    }\n    for (const [fieldName, index] of this.sortedIndexes) {\n      const value = row[fieldName];\n      if (value !== undefined) {\n        index.set(primaryKey, value);\n      }\n    }\n  }\n  removeRow(primaryKey, row) {\n    for (const [fieldName, index] of this.equalityIndexes) {\n      const value = row[fieldName];\n      if (value !== undefined) {\n        index.remove(primaryKey, value);\n      }\n    }\n    for (const [, index] of this.sortedIndexes) {\n      index.delete(primaryKey);\n    }\n  }\n  clear() {\n    for (const index of this.equalityIndexes.values()) {\n      index.clear();\n    }\n    for (const index of this.sortedIndexes.values()) {\n      index.clear();\n    }\n  }\n  getMemoryUsage() {\n    let bytes = 0;\n    for (const index of this.equalityIndexes.values()) {\n      bytes += index.getMemoryUsage();\n    }\n    for (const index of this.sortedIndexes.values()) {\n      bytes += index.getMemoryUsage();\n    }\n    return bytes;\n  }\n}\n\n// src/infrastructure/projections/stores/denormalized-view-store.ts\nvar DENORMALIZED_MAGIC = 1145979728;\nvar DENORMALIZED_VERSION = 1;\nvar SUPPORTED_VERSIONS3 = [1];\nvar DEFAULT_MEMORY_THRESHOLD = 50 * 1024 * 1024;\n\nclass DenormalizedViewStore {\n  fs;\n  serializer;\n  clock;\n  dataDir;\n  projectionName;\n  memoryThreshold;\n  rows = new Map;\n  indexes;\n  indexFields;\n  rangeFields;\n  constructor(config) {\n    this.fs = config.fs;\n    this.serializer = config.serializer;\n    this.clock = config.clock;\n    this.dataDir = config.dataDir;\n    this.projectionName = config.projectionName;\n    this.memoryThreshold = config.memoryThreshold ?? DEFAULT_MEMORY_THRESHOLD;\n    this.indexFields = config.indexFields ?? [];\n    this.rangeFields = config.rangeFields ?? [];\n    this.indexes = new IndexCollection;\n    for (const field of this.indexFields) {\n      this.indexes.addEqualityIndex(field);\n    }\n    for (const field of this.rangeFields) {\n      this.indexes.addSortedIndex(field);\n    }\n  }\n  async initialize() {\n    if (!await this.fs.exists(this.dataDir)) {\n      await this.fs.mkdir(this.dataDir, { recursive: true });\n    }\n  }\n  get() {\n    return new Map(this.rows);\n  }\n  set(state) {\n    this.rows.clear();\n    this.indexes.clear();\n    for (const [key, row] of state) {\n      this.rows.set(key, row);\n      this.indexes.indexRow(key, row);\n    }\n  }\n  getByKey(key) {\n    return this.rows.get(key);\n  }\n  setByKey(key, value) {\n    const existingRow = this.rows.get(key);\n    if (existingRow) {\n      this.indexes.removeRow(key, existingRow);\n    }\n    this.rows.set(key, value);\n    this.indexes.indexRow(key, value);\n  }\n  deleteByKey(key) {\n    const row = this.rows.get(key);\n    if (!row) {\n      return false;\n    }\n    this.indexes.removeRow(key, row);\n    this.rows.delete(key);\n    return true;\n  }\n  query(filter) {\n    if (Object.keys(filter).length === 0) {\n      return Array.from(this.rows.values());\n    }\n    let candidateKeys = null;\n    for (const [field, value] of Object.entries(filter)) {\n      const index = this.indexes.getEqualityIndex(field);\n      if (index) {\n        const matchingKeys = index.find(value);\n        if (candidateKeys === null) {\n          candidateKeys = new Set(matchingKeys);\n        } else {\n          const intersection = new Set;\n          for (const k of candidateKeys) {\n            if (matchingKeys.has(k)) {\n              intersection.add(k);\n            }\n          }\n          candidateKeys = intersection;\n        }\n      }\n    }\n    if (candidateKeys === null) {\n      candidateKeys = new Set(this.rows.keys());\n    }\n    const results = [];\n    for (const key of candidateKeys) {\n      const row = this.rows.get(key);\n      if (row && this.matchesFilter(row, filter)) {\n        results.push(row);\n      }\n    }\n    return results;\n  }\n  queryRange(range) {\n    const result = new Map;\n    if (this.rangeFields.length > 0) {\n      const rangeField = this.rangeFields[0];\n      const sortedIndex = this.indexes.getSortedIndex(rangeField);\n      if (sortedIndex) {\n        const keyMatches = sortedIndex.queryRange(range.start, range.end);\n        for (const [key] of keyMatches) {\n          const row = this.rows.get(key);\n          if (row) {\n            result.set(key, row);\n          }\n        }\n        return result;\n      }\n    }\n    for (const [key, row] of this.rows) {\n      if (this.keyInRange(key, range)) {\n        result.set(key, row);\n      }\n    }\n    return result;\n  }\n  get size() {\n    return this.rows.size;\n  }\n  async persist(position) {\n    const path = this.getCheckpointPath();\n    const tempPath = path + \".tmp\";\n    try {\n      const stateObject = {\n        rows: Array.from(this.rows.entries()),\n        indexFields: this.indexFields,\n        rangeFields: this.rangeFields\n      };\n      const stateData = this.serializer.encode(stateObject);\n      const headerSize = 32;\n      const totalSize = headerSize + stateData.length;\n      const buffer = new Uint8Array(totalSize);\n      const view = new DataView(buffer.buffer);\n      const timestamp = BigInt(this.clock.now());\n      view.setUint32(0, DENORMALIZED_MAGIC, true);\n      view.setUint32(4, DENORMALIZED_VERSION, true);\n      view.setBigUint64(8, BigInt(position), true);\n      view.setBigUint64(16, timestamp, true);\n      view.setUint32(24, stateData.length, true);\n      view.setUint32(28, 0, true);\n      buffer.set(stateData, headerSize);\n      const beforeChecksum = buffer.subarray(0, 28);\n      const afterChecksum = buffer.subarray(32);\n      const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n      checksumData.set(beforeChecksum);\n      checksumData.set(afterChecksum, beforeChecksum.length);\n      const checksum = crc32(checksumData);\n      view.setUint32(28, checksum, true);\n      const handle = await this.fs.open(tempPath, \"write\");\n      try {\n        await this.fs.write(handle, buffer);\n        await this.fs.sync(handle);\n      } finally {\n        await this.fs.close(handle);\n      }\n      await this.fs.rename(tempPath, path);\n    } catch (error) {\n      try {\n        if (await this.fs.exists(tempPath)) {\n          await this.fs.unlink(tempPath);\n        }\n      } catch {}\n      throw new CheckpointWriteError(this.projectionName, path, error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n  async load() {\n    const path = this.getCheckpointPath();\n    if (!await this.fs.exists(path)) {\n      return null;\n    }\n    try {\n      const data = await this.fs.readFile(path);\n      const headerSize = 32;\n      if (data.length < headerSize) {\n        throw new CheckpointCorruptionError(this.projectionName, path, \"File too small\");\n      }\n      const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n      const magic = view.getUint32(0, true);\n      const version = view.getUint32(4, true);\n      const position = Number(view.getBigUint64(8, true));\n      const stateLength = view.getUint32(24, true);\n      const storedChecksum = view.getUint32(28, true);\n      if (magic !== DENORMALIZED_MAGIC) {\n        throw new CheckpointCorruptionError(this.projectionName, path, `Invalid magic: 0x${magic.toString(16)}`);\n      }\n      if (!SUPPORTED_VERSIONS3.includes(version)) {\n        throw new CheckpointVersionError(this.projectionName, path, version, SUPPORTED_VERSIONS3);\n      }\n      const expectedSize = headerSize + stateLength;\n      if (data.length < expectedSize) {\n        throw new CheckpointCorruptionError(this.projectionName, path, \"File too small for state\");\n      }\n      const beforeChecksum = data.subarray(0, 28);\n      const afterChecksum = data.subarray(32);\n      const checksumData = new Uint8Array(beforeChecksum.length + afterChecksum.length);\n      checksumData.set(beforeChecksum);\n      checksumData.set(afterChecksum, beforeChecksum.length);\n      const calculatedChecksum = crc32(checksumData);\n      if (calculatedChecksum !== storedChecksum) {\n        throw new CheckpointCorruptionError(this.projectionName, path, \"Checksum mismatch\");\n      }\n      const stateData = data.subarray(headerSize, expectedSize);\n      const stateObject = this.serializer.decode(stateData);\n      this.rows.clear();\n      this.indexes.clear();\n      for (const [key, row] of stateObject.rows) {\n        this.rows.set(key, row);\n        this.indexes.indexRow(key, row);\n      }\n      return position;\n    } catch (error) {\n      if (error instanceof CheckpointCorruptionError || error instanceof CheckpointVersionError) {\n        throw error;\n      }\n      throw new CheckpointLoadError(this.projectionName, path, error instanceof Error ? error : new Error(String(error)));\n    }\n  }\n  async close() {}\n  getMemoryUsage() {\n    let bytes = 0;\n    try {\n      const rowSample = this.rows.size > 0 ? this.rows.values().next().value : undefined;\n      if (rowSample) {\n        const sampleSize = this.serializer.encode(rowSample).length;\n        bytes += this.rows.size * (sampleSize + 30);\n      }\n    } catch {\n      bytes += this.rows.size * 200;\n    }\n    bytes += this.indexes.getMemoryUsage();\n    return bytes;\n  }\n  clear() {\n    this.rows.clear();\n    this.indexes.clear();\n  }\n  isMemoryThresholdExceeded() {\n    return this.getMemoryUsage() > this.memoryThreshold;\n  }\n  getCheckpointPath() {\n    const safeName = this.projectionName.replace(/[^a-zA-Z0-9_-]/g, \"_\");\n    return `${this.dataDir}/${safeName}.ckpt`;\n  }\n  matchesFilter(row, filter) {\n    for (const [field, value] of Object.entries(filter)) {\n      if (row[field] !== value) {\n        return false;\n      }\n    }\n    return true;\n  }\n  keyInRange(key, range) {\n    if (range.start !== undefined && key < range.start) {\n      return false;\n    }\n    if (range.end !== undefined && key > range.end) {\n      return false;\n    }\n    return true;\n  }\n}\n\n// src/application/projections/projection-coordinator.ts\nclass ProjectionCoordinator {\n  config;\n  registry;\n  runners = new Map;\n  stores = new Map;\n  projectionInstances = new Map;\n  running = false;\n  lockHandle = null;\n  sharedPollingTimer = null;\n  sharedCursor = null;\n  sharedPollingPaused = false;\n  constructor(config) {\n    this.config = {\n      ...config,\n      pollingIntervalMs: config.pollingIntervalMs ?? 100,\n      defaultCheckpointIntervalMs: config.defaultCheckpointIntervalMs ?? 5000,\n      checkpointJitterMs: config.checkpointJitterMs ?? 1000,\n      defaultBatchSize: config.defaultBatchSize ?? 100,\n      sharedReader: config.sharedReader ?? true\n    };\n    this.registry = new DefaultProjectionRegistry;\n  }\n  getRegistry() {\n    return this.registry;\n  }\n  async start() {\n    if (this.running) {\n      throw new ProjectionCoordinatorError(\"Coordinator already running\");\n    }\n    if (!this.config.eventStore.isOpen()) {\n      throw new ProjectionCoordinatorError(\"EventStore is not open\");\n    }\n    if (!await this.config.fs.exists(this.config.dataDir)) {\n      await this.config.fs.mkdir(this.config.dataDir, { recursive: true });\n    }\n    const lockPath = `${this.config.dataDir}/.lock`;\n    try {\n      this.lockHandle = await this.config.fs.open(lockPath, \"write\");\n      await this.config.fs.flock(this.lockHandle, \"exclusive\");\n    } catch (error) {\n      if (this.lockHandle) {\n        try {\n          await this.config.fs.close(this.lockHandle);\n        } catch {}\n        this.lockHandle = null;\n      }\n      throw new ProjectionCoordinatorError(`Failed to acquire lock on ${lockPath}: ${error instanceof Error ? error.message : String(error)}`);\n    }\n    try {\n      const checkpointDir = `${this.config.dataDir}/checkpoints`;\n      if (!await this.config.fs.exists(checkpointDir)) {\n        await this.config.fs.mkdir(checkpointDir, { recursive: true });\n      }\n      const startPromises = [];\n      for (const resolved of this.registry.getAll()) {\n        if (!resolved.options.enabled) {\n          continue;\n        }\n        const { registration, options } = resolved;\n        const { metadata } = registration;\n        const projection = registration.factory();\n        const store = this.createStore(metadata.name, metadata.kind, projection, resolved);\n        await store.initialize();\n        const runner = new ProjectionRunner({\n          eventStore: this.config.eventStore,\n          projection,\n          store,\n          metadata,\n          clock: this.config.clock,\n          pollingIntervalMs: this.config.pollingIntervalMs,\n          checkpointIntervalMs: options.checkpointIntervalMs,\n          jitterMs: this.config.checkpointJitterMs,\n          eventFilter: options.eventFilter,\n          batchSize: this.config.defaultBatchSize,\n          useSharedReader: this.config.sharedReader\n        });\n        this.runners.set(metadata.name, runner);\n        this.stores.set(metadata.name, store);\n        if (registration.getInstance) {\n          this.projectionInstances.set(metadata.name, registration.getInstance());\n        } else {\n          this.projectionInstances.set(metadata.name, projection);\n        }\n        startPromises.push(runner.start());\n      }\n      await Promise.all(startPromises);\n      this.running = true;\n      if (this.config.sharedReader) {\n        this.startSharedReader();\n      }\n    } catch (error) {\n      await this.cleanupAfterStartFailure();\n      throw error;\n    }\n  }\n  async stop() {\n    if (!this.running) {\n      return;\n    }\n    if (this.sharedPollingTimer) {\n      this.sharedPollingTimer.cancel();\n      this.sharedPollingTimer = null;\n    }\n    const stopPromises = Array.from(this.runners.values()).map((runner) => runner.stop());\n    await Promise.all(stopPromises);\n    const closePromises = Array.from(this.stores.values()).map((store) => store.close());\n    await Promise.all(closePromises);\n    this.runners.clear();\n    this.stores.clear();\n    this.projectionInstances.clear();\n    if (this.lockHandle) {\n      try {\n        await this.config.fs.close(this.lockHandle);\n      } catch {}\n      this.lockHandle = null;\n    }\n    this.running = false;\n  }\n  async cleanupAfterStartFailure() {\n    if (this.sharedPollingTimer) {\n      this.sharedPollingTimer.cancel();\n      this.sharedPollingTimer = null;\n    }\n    const stopPromises = Array.from(this.runners.values()).map((runner) => runner.stop().catch(() => {\n      return;\n    }));\n    await Promise.all(stopPromises);\n    const closePromises = Array.from(this.stores.values()).map((store) => store.close().catch(() => {\n      return;\n    }));\n    await Promise.all(closePromises);\n    this.runners.clear();\n    this.stores.clear();\n    this.projectionInstances.clear();\n    if (this.lockHandle) {\n      try {\n        await this.config.fs.close(this.lockHandle);\n      } catch {}\n      this.lockHandle = null;\n    }\n    this.running = false;\n  }\n  startSharedReader() {\n    this.sharedCursor = null;\n    this.sharedPollingPaused = false;\n    this.scheduleSharedPoll();\n  }\n  scheduleSharedPoll(immediate = false) {\n    if (!this.running) {\n      return;\n    }\n    const delay = immediate ? 0 : this.config.pollingIntervalMs;\n    this.sharedPollingTimer = this.config.clock.setTimeout(async () => {\n      let hadEvents = false;\n      try {\n        if (!this.sharedPollingPaused) {\n          hadEvents = await this.pollShared();\n        }\n      } catch (error) {\n        console.error(\"[ProjectionCoordinator] Shared poll error:\", error);\n      } finally {\n        this.scheduleSharedPoll(hadEvents);\n      }\n    }, delay);\n  }\n  async pollShared() {\n    if (!this.running) {\n      return false;\n    }\n    const startTime = this.config.clock.now();\n    const realStart = Date.now();\n    const durationBudget = this.config.pollingIntervalMs;\n    let processedAny = false;\n    while (this.running) {\n      const events = await this.readSharedBatch();\n      if (events.length === 0) {\n        break;\n      }\n      processedAny = true;\n      this.dispatchBatch(events);\n      const elapsed = this.config.clock.now() - startTime;\n      const realElapsed = Date.now() - realStart;\n      if (elapsed >= durationBudget || realElapsed >= durationBudget) {\n        await this.tickCheckpoints();\n        return true;\n      }\n    }\n    await this.tickCheckpoints();\n    return false;\n  }\n  async tickCheckpoints() {\n    const checkpointPromises = Array.from(this.runners.values()).map((runner) => runner.checkpointIfNeeded());\n    await Promise.all(checkpointPromises);\n  }\n  dispatchBatch(events) {\n    for (const runner of this.runners.values()) {\n      try {\n        runner.processBatch(events);\n      } catch (error) {\n        console.error(`[ProjectionCoordinator] Runner '${runner.getStatus().name}' error:`, error);\n      }\n    }\n  }\n  async readSharedBatch() {\n    if (this.sharedCursor === null) {\n      this.sharedCursor = this.getSharedStartCursor();\n    }\n    const cached = this.config.eventStore.readGlobalCached(this.sharedCursor, this.config.defaultBatchSize);\n    if (cached !== null && cached.length > 0) {\n      const last = cached[cached.length - 1];\n      this.sharedCursor = last.globalPosition + 1;\n      return cached;\n    }\n    const events = await this.config.eventStore.readGlobal(this.sharedCursor, {\n      maxCount: this.config.defaultBatchSize\n    });\n    if (events.length > 0) {\n      const last = events[events.length - 1];\n      this.sharedCursor = last.globalPosition + 1;\n    }\n    return events;\n  }\n  getSharedStartCursor() {\n    let minPosition = null;\n    for (const runner of this.runners.values()) {\n      const position = runner.getCurrentPosition();\n      if (minPosition === null || position < minPosition) {\n        minPosition = position;\n      }\n    }\n    if (minPosition === null || minPosition < 0) {\n      return 0;\n    }\n    return minPosition + 1;\n  }\n  getProjection(name) {\n    return this.projectionInstances.get(name);\n  }\n  requireProjection(name) {\n    const resolved = this.registry.get(name);\n    if (!resolved) {\n      throw new ProjectionNotFoundError(name);\n    }\n    if (!resolved.options.enabled) {\n      throw new ProjectionDisabledError(name);\n    }\n    const projection = this.projectionInstances.get(name);\n    if (!projection) {\n      throw new ProjectionNotFoundError(name);\n    }\n    return projection;\n  }\n  async waitForCatchUp(timeoutMs = 30000) {\n    const globalPosition = this.config.eventStore.getGlobalPosition();\n    const targetPosition = globalPosition > 0 ? globalPosition - 1 : -1;\n    if (this.config.sharedReader) {\n      const success = await this.drainSharedUntil(targetPosition, timeoutMs);\n      if (!success) {\n        const firstRunner = this.runners.values().next().value;\n        const current = firstRunner ? firstRunner.getCurrentPosition() : -1;\n        const name = firstRunner ? firstRunner.getStatus().name : \"unknown\";\n        throw new ProjectionCatchUpTimeoutError(name, current, targetPosition, timeoutMs);\n      }\n      return;\n    }\n    const catchUpPromises = Array.from(this.runners.entries()).map(async ([name, runner]) => {\n      const success = await runner.waitForCatchUp(targetPosition, timeoutMs);\n      if (!success) {\n        throw new ProjectionCatchUpTimeoutError(name, runner.getCurrentPosition(), targetPosition, timeoutMs);\n      }\n    });\n    await Promise.all(catchUpPromises);\n  }\n  async drainSharedUntil(targetPosition, timeoutMs) {\n    const startTime = this.config.clock.now();\n    const realStart = Date.now();\n    this.sharedPollingPaused = true;\n    try {\n      while (this.getMinRunnerPosition() < targetPosition) {\n        const events = await this.readSharedBatch();\n        if (events.length > 0) {\n          this.dispatchBatch(events);\n          await this.tickCheckpoints();\n        } else {\n          await this.config.clock.sleep(10);\n        }\n        const elapsed = this.config.clock.now() - startTime;\n        const realElapsed = Date.now() - realStart;\n        if (elapsed > timeoutMs || realElapsed > timeoutMs) {\n          return false;\n        }\n      }\n    } finally {\n      this.sharedPollingPaused = false;\n    }\n    return true;\n  }\n  getMinRunnerPosition() {\n    let minPosition = null;\n    for (const runner of this.runners.values()) {\n      const position = runner.getCurrentPosition();\n      if (minPosition === null || position < minPosition) {\n        minPosition = position;\n      }\n    }\n    return minPosition ?? -1;\n  }\n  async forceCheckpoint() {\n    const checkpointPromises = Array.from(this.runners.values()).map((runner) => runner.forceCheckpoint());\n    await Promise.all(checkpointPromises);\n  }\n  getStatus() {\n    const projections = Array.from(this.runners.values()).map((runner) => runner.getStatus());\n    const enabledCount = this.registry.getAll().filter((r) => r.options.enabled).length;\n    return {\n      running: this.running,\n      registeredCount: this.registry.count(),\n      enabledCount,\n      projections,\n      globalPosition: this.config.eventStore.isOpen() ? this.config.eventStore.getGlobalPosition() : 0\n    };\n  }\n  getProjectionStatus(name) {\n    return this.runners.get(name)?.getStatus();\n  }\n  isRunning() {\n    return this.running;\n  }\n  createStore(name, kind, projection, resolved) {\n    const storeConfig = {\n      fs: this.config.fs,\n      serializer: this.config.serializer,\n      clock: this.config.clock,\n      dataDir: `${this.config.dataDir}/checkpoints`,\n      projectionName: name\n    };\n    switch (kind) {\n      case \"aggregator\":\n        projection.reset();\n        const initialState = projection.getState();\n        return new AggregatorStore(storeConfig, initialState);\n      case \"denormalized_view\":\n        const { accessPatterns } = resolved.registration.metadata;\n        const indexFields = [];\n        const rangeFields = [];\n        for (const pattern of accessPatterns) {\n          if (pattern.isRange) {\n            rangeFields.push(...pattern.indexFields);\n          } else {\n            indexFields.push(...pattern.indexFields);\n          }\n        }\n        return new DenormalizedViewStore({\n          ...storeConfig,\n          indexFields: [...new Set(indexFields)],\n          rangeFields: [...new Set(rangeFields)],\n          memoryThreshold: resolved.options.memoryThresholdBytes\n        });\n      default:\n        throw new ProjectionCoordinatorError(`Unknown projection kind: ${kind}`);\n    }\n  }\n}\n// src/infrastructure/filesystem/bun-filesystem.ts\nimport {\n  statSync,\n  existsSync,\n  renameSync,\n  unlinkSync,\n  mkdirSync,\n  readdirSync,\n  rmdirSync\n} from \"fs\";\nimport { open as openAsync } from \"fs/promises\";\nvar handles = new Map;\n\nclass BunFileSystem {\n  async open(path, mode) {\n    const flags = this.modeToFlags(mode);\n    const nodeHandle = await openAsync(path, flags);\n    const fd = nodeHandle.fd;\n    handles.set(fd, nodeHandle);\n    return { fd };\n  }\n  async close(handle) {\n    const nodeHandle = handles.get(handle.fd);\n    if (nodeHandle) {\n      await nodeHandle.close();\n      handles.delete(handle.fd);\n    }\n  }\n  async read(handle, offset, length) {\n    const nodeHandle = handles.get(handle.fd);\n    if (!nodeHandle) {\n      throw new Error(`Invalid file handle: fd=${handle.fd}`);\n    }\n    const buffer = new Uint8Array(length);\n    const result = await nodeHandle.read(buffer, 0, length, offset);\n    return buffer.subarray(0, result.bytesRead);\n  }\n  async write(handle, data, offset) {\n    const nodeHandle = handles.get(handle.fd);\n    if (!nodeHandle) {\n      throw new Error(`Invalid file handle: fd=${handle.fd}`);\n    }\n    const result = await nodeHandle.write(data, 0, data.length, offset ?? null);\n    return result.bytesWritten;\n  }\n  async sync(handle) {\n    const nodeHandle = handles.get(handle.fd);\n    if (!nodeHandle) {\n      throw new Error(`Invalid file handle: fd=${handle.fd}`);\n    }\n    await nodeHandle.sync();\n  }\n  async readFile(path) {\n    return Bun.file(path).bytes();\n  }\n  async readFileSlice(path, start, end) {\n    const file = Bun.file(path);\n    const blob = file.slice(start, end);\n    const arrayBuffer = await blob.arrayBuffer();\n    return new Uint8Array(arrayBuffer);\n  }\n  async stat(path) {\n    const stats = statSync(path);\n    return {\n      size: stats.size,\n      isFile: stats.isFile(),\n      isDirectory: stats.isDirectory(),\n      mtime: stats.mtime\n    };\n  }\n  async exists(path) {\n    return existsSync(path);\n  }\n  async truncate(handle, length) {\n    const nodeHandle = handles.get(handle.fd);\n    if (!nodeHandle) {\n      throw new Error(`Invalid file handle: fd=${handle.fd}`);\n    }\n    await nodeHandle.truncate(length);\n  }\n  async rename(from, to) {\n    renameSync(from, to);\n  }\n  async unlink(path) {\n    unlinkSync(path);\n  }\n  async mkdir(path, options) {\n    mkdirSync(path, { recursive: options?.recursive ?? false });\n  }\n  async readdir(path) {\n    return readdirSync(path);\n  }\n  async rmdir(path, options) {\n    rmdirSync(path, { recursive: options?.recursive ?? false });\n  }\n  async mmap(path) {\n    try {\n      return Bun.mmap(path);\n    } catch {\n      return await Bun.file(path).bytes();\n    }\n  }\n  async flock(handle, mode) {\n    const operation = mode === \"exclusive\" ? LOCK_EX : LOCK_SH;\n    const success = flock(handle.fd, operation);\n    if (!success) {\n      throw new Error(`Failed to acquire ${mode} lock on file descriptor ${handle.fd}. ` + `Another process may be holding an incompatible lock.`);\n    }\n  }\n  async funlock(handle) {\n    const success = flock(handle.fd, LOCK_UN);\n    if (!success) {\n      throw new Error(`Failed to release lock on file descriptor ${handle.fd}`);\n    }\n  }\n  modeToFlags(mode) {\n    switch (mode) {\n      case \"read\":\n        return \"r\";\n      case \"write\":\n        return \"w\";\n      case \"append\":\n        return \"a\";\n      case \"readwrite\":\n        return \"r+\";\n    }\n  }\n}\n\n// src/infrastructure/time/bun-clock.ts\nclass BunTimer {\n  handle;\n  constructor(handle) {\n    this.handle = handle;\n  }\n  cancel() {\n    clearTimeout(this.handle);\n  }\n}\n\nclass BunClock {\n  now() {\n    return Date.now();\n  }\n  async sleep(ms) {\n    return Bun.sleep(ms);\n  }\n  setTimeout(callback, ms) {\n    return new BunTimer(setTimeout(callback, ms));\n  }\n  setInterval(callback, ms) {\n    return new BunTimer(setInterval(callback, ms));\n  }\n}\n\n// ../../node_modules/.bun/msgpackr@1.11.8/node_modules/msgpackr/unpack.js\nvar decoder;\ntry {\n  decoder = new TextDecoder;\n} catch (error) {}\nvar src;\nvar srcEnd;\nvar position = 0;\nvar EMPTY_ARRAY = [];\nvar strings = EMPTY_ARRAY;\nvar stringPosition = 0;\nvar currentUnpackr = {};\nvar currentStructures;\nvar srcString;\nvar srcStringStart = 0;\nvar srcStringEnd = 0;\nvar bundledStrings;\nvar referenceMap;\nvar currentExtensions = [];\nvar dataView;\nvar defaultOptions = {\n  useRecords: false,\n  mapsAsObjects: true\n};\n\nclass C1Type {\n}\nvar C1 = new C1Type;\nC1.name = \"MessagePack 0xC1\";\nvar sequentialMode = false;\nvar inlineObjectReadThreshold = 2;\nvar readStruct;\nvar onLoadedStructures;\nvar onSaveState;\ntry {\n  new Function(\"\");\n} catch (error) {\n  inlineObjectReadThreshold = Infinity;\n}\n\nclass Unpackr {\n  constructor(options) {\n    if (options) {\n      if (options.useRecords === false && options.mapsAsObjects === undefined)\n        options.mapsAsObjects = true;\n      if (options.sequential && options.trusted !== false) {\n        options.trusted = true;\n        if (!options.structures && options.useRecords != false) {\n          options.structures = [];\n          if (!options.maxSharedStructures)\n            options.maxSharedStructures = 0;\n        }\n      }\n      if (options.structures)\n        options.structures.sharedLength = options.structures.length;\n      else if (options.getStructures) {\n        (options.structures = []).uninitialized = true;\n        options.structures.sharedLength = 0;\n      }\n      if (options.int64AsNumber) {\n        options.int64AsType = \"number\";\n      }\n    }\n    Object.assign(this, options);\n  }\n  unpack(source, options) {\n    if (src) {\n      return saveState(() => {\n        clearSource();\n        return this ? this.unpack(source, options) : Unpackr.prototype.unpack.call(defaultOptions, source, options);\n      });\n    }\n    if (!source.buffer && source.constructor === ArrayBuffer)\n      source = typeof Buffer !== \"undefined\" ? Buffer.from(source) : new Uint8Array(source);\n    if (typeof options === \"object\") {\n      srcEnd = options.end || source.length;\n      position = options.start || 0;\n    } else {\n      position = 0;\n      srcEnd = options > -1 ? options : source.length;\n    }\n    stringPosition = 0;\n    srcStringEnd = 0;\n    srcString = null;\n    strings = EMPTY_ARRAY;\n    bundledStrings = null;\n    src = source;\n    try {\n      dataView = source.dataView || (source.dataView = new DataView(source.buffer, source.byteOffset, source.byteLength));\n    } catch (error) {\n      src = null;\n      if (source instanceof Uint8Array)\n        throw error;\n      throw new Error(\"Source must be a Uint8Array or Buffer but was a \" + (source && typeof source == \"object\" ? source.constructor.name : typeof source));\n    }\n    if (this instanceof Unpackr) {\n      currentUnpackr = this;\n      if (this.structures) {\n        currentStructures = this.structures;\n        return checkedRead(options);\n      } else if (!currentStructures || currentStructures.length > 0) {\n        currentStructures = [];\n      }\n    } else {\n      currentUnpackr = defaultOptions;\n      if (!currentStructures || currentStructures.length > 0)\n        currentStructures = [];\n    }\n    return checkedRead(options);\n  }\n  unpackMultiple(source, forEach) {\n    let values, lastPosition = 0;\n    try {\n      sequentialMode = true;\n      let size = source.length;\n      let value = this ? this.unpack(source, size) : defaultUnpackr.unpack(source, size);\n      if (forEach) {\n        if (forEach(value, lastPosition, position) === false)\n          return;\n        while (position < size) {\n          lastPosition = position;\n          if (forEach(checkedRead(), lastPosition, position) === false) {\n            return;\n          }\n        }\n      } else {\n        values = [value];\n        while (position < size) {\n          lastPosition = position;\n          values.push(checkedRead());\n        }\n        return values;\n      }\n    } catch (error) {\n      error.lastPosition = lastPosition;\n      error.values = values;\n      throw error;\n    } finally {\n      sequentialMode = false;\n      clearSource();\n    }\n  }\n  _mergeStructures(loadedStructures, existingStructures) {\n    if (onLoadedStructures)\n      loadedStructures = onLoadedStructures.call(this, loadedStructures);\n    loadedStructures = loadedStructures || [];\n    if (Object.isFrozen(loadedStructures))\n      loadedStructures = loadedStructures.map((structure) => structure.slice(0));\n    for (let i = 0, l = loadedStructures.length;i < l; i++) {\n      let structure = loadedStructures[i];\n      if (structure) {\n        structure.isShared = true;\n        if (i >= 32)\n          structure.highByte = i - 32 >> 5;\n      }\n    }\n    loadedStructures.sharedLength = loadedStructures.length;\n    for (let id in existingStructures || []) {\n      if (id >= 0) {\n        let structure = loadedStructures[id];\n        let existing = existingStructures[id];\n        if (existing) {\n          if (structure)\n            (loadedStructures.restoreStructures || (loadedStructures.restoreStructures = []))[id] = structure;\n          loadedStructures[id] = existing;\n        }\n      }\n    }\n    return this.structures = loadedStructures;\n  }\n  decode(source, options) {\n    return this.unpack(source, options);\n  }\n}\nfunction checkedRead(options) {\n  try {\n    if (!currentUnpackr.trusted && !sequentialMode) {\n      let sharedLength = currentStructures.sharedLength || 0;\n      if (sharedLength < currentStructures.length)\n        currentStructures.length = sharedLength;\n    }\n    let result;\n    if (currentUnpackr.randomAccessStructure && src[position] < 64 && src[position] >= 32 && readStruct) {\n      result = readStruct(src, position, srcEnd, currentUnpackr);\n      src = null;\n      if (!(options && options.lazy) && result)\n        result = result.toJSON();\n      position = srcEnd;\n    } else\n      result = read();\n    if (bundledStrings) {\n      position = bundledStrings.postBundlePosition;\n      bundledStrings = null;\n    }\n    if (sequentialMode)\n      currentStructures.restoreStructures = null;\n    if (position == srcEnd) {\n      if (currentStructures && currentStructures.restoreStructures)\n        restoreStructures();\n      currentStructures = null;\n      src = null;\n      if (referenceMap)\n        referenceMap = null;\n    } else if (position > srcEnd) {\n      throw new Error(\"Unexpected end of MessagePack data\");\n    } else if (!sequentialMode) {\n      let jsonView;\n      try {\n        jsonView = JSON.stringify(result, (_, value) => typeof value === \"bigint\" ? `${value}n` : value).slice(0, 100);\n      } catch (error) {\n        jsonView = \"(JSON view not available \" + error + \")\";\n      }\n      throw new Error(\"Data read, but end of buffer not reached \" + jsonView);\n    }\n    return result;\n  } catch (error) {\n    if (currentStructures && currentStructures.restoreStructures)\n      restoreStructures();\n    clearSource();\n    if (error instanceof RangeError || error.message.startsWith(\"Unexpected end of buffer\") || position > srcEnd) {\n      error.incomplete = true;\n    }\n    throw error;\n  }\n}\nfunction restoreStructures() {\n  for (let id in currentStructures.restoreStructures) {\n    currentStructures[id] = currentStructures.restoreStructures[id];\n  }\n  currentStructures.restoreStructures = null;\n}\nfunction read() {\n  let token = src[position++];\n  if (token < 160) {\n    if (token < 128) {\n      if (token < 64)\n        return token;\n      else {\n        let structure = currentStructures[token & 63] || currentUnpackr.getStructures && loadStructures()[token & 63];\n        if (structure) {\n          if (!structure.read) {\n            structure.read = createStructureReader(structure, token & 63);\n          }\n          return structure.read();\n        } else\n          return token;\n      }\n    } else if (token < 144) {\n      token -= 128;\n      if (currentUnpackr.mapsAsObjects) {\n        let object = {};\n        for (let i = 0;i < token; i++) {\n          let key = readKey();\n          if (key === \"__proto__\")\n            key = \"__proto_\";\n          object[key] = read();\n        }\n        return object;\n      } else {\n        let map = new Map;\n        for (let i = 0;i < token; i++) {\n          map.set(read(), read());\n        }\n        return map;\n      }\n    } else {\n      token -= 144;\n      let array = new Array(token);\n      for (let i = 0;i < token; i++) {\n        array[i] = read();\n      }\n      if (currentUnpackr.freezeData)\n        return Object.freeze(array);\n      return array;\n    }\n  } else if (token < 192) {\n    let length = token - 160;\n    if (srcStringEnd >= position) {\n      return srcString.slice(position - srcStringStart, (position += length) - srcStringStart);\n    }\n    if (srcStringEnd == 0 && srcEnd < 140) {\n      let string = length < 16 ? shortStringInJS(length) : longStringInJS(length);\n      if (string != null)\n        return string;\n    }\n    return readFixedString(length);\n  } else {\n    let value;\n    switch (token) {\n      case 192:\n        return null;\n      case 193:\n        if (bundledStrings) {\n          value = read();\n          if (value > 0)\n            return bundledStrings[1].slice(bundledStrings.position1, bundledStrings.position1 += value);\n          else\n            return bundledStrings[0].slice(bundledStrings.position0, bundledStrings.position0 -= value);\n        }\n        return C1;\n      case 194:\n        return false;\n      case 195:\n        return true;\n      case 196:\n        value = src[position++];\n        if (value === undefined)\n          throw new Error(\"Unexpected end of buffer\");\n        return readBin(value);\n      case 197:\n        value = dataView.getUint16(position);\n        position += 2;\n        return readBin(value);\n      case 198:\n        value = dataView.getUint32(position);\n        position += 4;\n        return readBin(value);\n      case 199:\n        return readExt(src[position++]);\n      case 200:\n        value = dataView.getUint16(position);\n        position += 2;\n        return readExt(value);\n      case 201:\n        value = dataView.getUint32(position);\n        position += 4;\n        return readExt(value);\n      case 202:\n        value = dataView.getFloat32(position);\n        if (currentUnpackr.useFloat32 > 2) {\n          let multiplier = mult10[(src[position] & 127) << 1 | src[position + 1] >> 7];\n          position += 4;\n          return (multiplier * value + (value > 0 ? 0.5 : -0.5) >> 0) / multiplier;\n        }\n        position += 4;\n        return value;\n      case 203:\n        value = dataView.getFloat64(position);\n        position += 8;\n        return value;\n      case 204:\n        return src[position++];\n      case 205:\n        value = dataView.getUint16(position);\n        position += 2;\n        return value;\n      case 206:\n        value = dataView.getUint32(position);\n        position += 4;\n        return value;\n      case 207:\n        if (currentUnpackr.int64AsType === \"number\") {\n          value = dataView.getUint32(position) * 4294967296;\n          value += dataView.getUint32(position + 4);\n        } else if (currentUnpackr.int64AsType === \"string\") {\n          value = dataView.getBigUint64(position).toString();\n        } else if (currentUnpackr.int64AsType === \"auto\") {\n          value = dataView.getBigUint64(position);\n          if (value <= BigInt(2) << BigInt(52))\n            value = Number(value);\n        } else\n          value = dataView.getBigUint64(position);\n        position += 8;\n        return value;\n      case 208:\n        return dataView.getInt8(position++);\n      case 209:\n        value = dataView.getInt16(position);\n        position += 2;\n        return value;\n      case 210:\n        value = dataView.getInt32(position);\n        position += 4;\n        return value;\n      case 211:\n        if (currentUnpackr.int64AsType === \"number\") {\n          value = dataView.getInt32(position) * 4294967296;\n          value += dataView.getUint32(position + 4);\n        } else if (currentUnpackr.int64AsType === \"string\") {\n          value = dataView.getBigInt64(position).toString();\n        } else if (currentUnpackr.int64AsType === \"auto\") {\n          value = dataView.getBigInt64(position);\n          if (value >= BigInt(-2) << BigInt(52) && value <= BigInt(2) << BigInt(52))\n            value = Number(value);\n        } else\n          value = dataView.getBigInt64(position);\n        position += 8;\n        return value;\n      case 212:\n        value = src[position++];\n        if (value == 114) {\n          return recordDefinition(src[position++] & 63);\n        } else {\n          let extension = currentExtensions[value];\n          if (extension) {\n            if (extension.read) {\n              position++;\n              return extension.read(read());\n            } else if (extension.noBuffer) {\n              position++;\n              return extension();\n            } else\n              return extension(src.subarray(position, ++position));\n          } else\n            throw new Error(\"Unknown extension \" + value);\n        }\n      case 213:\n        value = src[position];\n        if (value == 114) {\n          position++;\n          return recordDefinition(src[position++] & 63, src[position++]);\n        } else\n          return readExt(2);\n      case 214:\n        return readExt(4);\n      case 215:\n        return readExt(8);\n      case 216:\n        return readExt(16);\n      case 217:\n        value = src[position++];\n        if (srcStringEnd >= position) {\n          return srcString.slice(position - srcStringStart, (position += value) - srcStringStart);\n        }\n        return readString8(value);\n      case 218:\n        value = dataView.getUint16(position);\n        position += 2;\n        if (srcStringEnd >= position) {\n          return srcString.slice(position - srcStringStart, (position += value) - srcStringStart);\n        }\n        return readString16(value);\n      case 219:\n        value = dataView.getUint32(position);\n        position += 4;\n        if (srcStringEnd >= position) {\n          return srcString.slice(position - srcStringStart, (position += value) - srcStringStart);\n        }\n        return readString32(value);\n      case 220:\n        value = dataView.getUint16(position);\n        position += 2;\n        return readArray(value);\n      case 221:\n        value = dataView.getUint32(position);\n        position += 4;\n        return readArray(value);\n      case 222:\n        value = dataView.getUint16(position);\n        position += 2;\n        return readMap(value);\n      case 223:\n        value = dataView.getUint32(position);\n        position += 4;\n        return readMap(value);\n      default:\n        if (token >= 224)\n          return token - 256;\n        if (token === undefined) {\n          let error = new Error(\"Unexpected end of MessagePack data\");\n          error.incomplete = true;\n          throw error;\n        }\n        throw new Error(\"Unknown MessagePack token \" + token);\n    }\n  }\n}\nvar validName = /^[a-zA-Z_$][a-zA-Z\\d_$]*$/;\nfunction createStructureReader(structure, firstId) {\n  function readObject() {\n    if (readObject.count++ > inlineObjectReadThreshold) {\n      let readObject2 = structure.read = new Function(\"r\", \"return function(){return \" + (currentUnpackr.freezeData ? \"Object.freeze\" : \"\") + \"({\" + structure.map((key) => key === \"__proto__\" ? \"__proto_:r()\" : validName.test(key) ? key + \":r()\" : \"[\" + JSON.stringify(key) + \"]:r()\").join(\",\") + \"})}\")(read);\n      if (structure.highByte === 0)\n        structure.read = createSecondByteReader(firstId, structure.read);\n      return readObject2();\n    }\n    let object = {};\n    for (let i = 0, l = structure.length;i < l; i++) {\n      let key = structure[i];\n      if (key === \"__proto__\")\n        key = \"__proto_\";\n      object[key] = read();\n    }\n    if (currentUnpackr.freezeData)\n      return Object.freeze(object);\n    return object;\n  }\n  readObject.count = 0;\n  if (structure.highByte === 0) {\n    return createSecondByteReader(firstId, readObject);\n  }\n  return readObject;\n}\nvar createSecondByteReader = (firstId, read0) => {\n  return function() {\n    let highByte = src[position++];\n    if (highByte === 0)\n      return read0();\n    let id = firstId < 32 ? -(firstId + (highByte << 5)) : firstId + (highByte << 5);\n    let structure = currentStructures[id] || loadStructures()[id];\n    if (!structure) {\n      throw new Error(\"Record id is not defined for \" + id);\n    }\n    if (!structure.read)\n      structure.read = createStructureReader(structure, firstId);\n    return structure.read();\n  };\n};\nfunction loadStructures() {\n  let loadedStructures = saveState(() => {\n    src = null;\n    return currentUnpackr.getStructures();\n  });\n  return currentStructures = currentUnpackr._mergeStructures(loadedStructures, currentStructures);\n}\nvar readFixedString = readStringJS;\nvar readString8 = readStringJS;\nvar readString16 = readStringJS;\nvar readString32 = readStringJS;\nvar isNativeAccelerationEnabled = false;\nfunction setExtractor(extractStrings) {\n  isNativeAccelerationEnabled = true;\n  readFixedString = readString(1);\n  readString8 = readString(2);\n  readString16 = readString(3);\n  readString32 = readString(5);\n  function readString(headerLength) {\n    return function readString(length) {\n      let string = strings[stringPosition++];\n      if (string == null) {\n        if (bundledStrings)\n          return readStringJS(length);\n        let byteOffset = src.byteOffset;\n        let extraction = extractStrings(position - headerLength + byteOffset, srcEnd + byteOffset, src.buffer);\n        if (typeof extraction == \"string\") {\n          string = extraction;\n          strings = EMPTY_ARRAY;\n        } else {\n          strings = extraction;\n          stringPosition = 1;\n          srcStringEnd = 1;\n          string = strings[0];\n          if (string === undefined)\n            throw new Error(\"Unexpected end of buffer\");\n        }\n      }\n      let srcStringLength = string.length;\n      if (srcStringLength <= length) {\n        position += length;\n        return string;\n      }\n      srcString = string;\n      srcStringStart = position;\n      srcStringEnd = position + srcStringLength;\n      position += length;\n      return string.slice(0, length);\n    };\n  }\n}\nfunction readStringJS(length) {\n  let result;\n  if (length < 16) {\n    if (result = shortStringInJS(length))\n      return result;\n  }\n  if (length > 64 && decoder)\n    return decoder.decode(src.subarray(position, position += length));\n  const end = position + length;\n  const units = [];\n  result = \"\";\n  while (position < end) {\n    const byte1 = src[position++];\n    if ((byte1 & 128) === 0) {\n      units.push(byte1);\n    } else if ((byte1 & 224) === 192) {\n      const byte2 = src[position++] & 63;\n      units.push((byte1 & 31) << 6 | byte2);\n    } else if ((byte1 & 240) === 224) {\n      const byte2 = src[position++] & 63;\n      const byte3 = src[position++] & 63;\n      units.push((byte1 & 31) << 12 | byte2 << 6 | byte3);\n    } else if ((byte1 & 248) === 240) {\n      const byte2 = src[position++] & 63;\n      const byte3 = src[position++] & 63;\n      const byte4 = src[position++] & 63;\n      let unit = (byte1 & 7) << 18 | byte2 << 12 | byte3 << 6 | byte4;\n      if (unit > 65535) {\n        unit -= 65536;\n        units.push(unit >>> 10 & 1023 | 55296);\n        unit = 56320 | unit & 1023;\n      }\n      units.push(unit);\n    } else {\n      units.push(byte1);\n    }\n    if (units.length >= 4096) {\n      result += fromCharCode.apply(String, units);\n      units.length = 0;\n    }\n  }\n  if (units.length > 0) {\n    result += fromCharCode.apply(String, units);\n  }\n  return result;\n}\nfunction readString(source, start, length) {\n  let existingSrc = src;\n  src = source;\n  position = start;\n  try {\n    return readStringJS(length);\n  } finally {\n    src = existingSrc;\n  }\n}\nfunction readArray(length) {\n  let array = new Array(length);\n  for (let i = 0;i < length; i++) {\n    array[i] = read();\n  }\n  if (currentUnpackr.freezeData)\n    return Object.freeze(array);\n  return array;\n}\nfunction readMap(length) {\n  if (currentUnpackr.mapsAsObjects) {\n    let object = {};\n    for (let i = 0;i < length; i++) {\n      let key = readKey();\n      if (key === \"__proto__\")\n        key = \"__proto_\";\n      object[key] = read();\n    }\n    return object;\n  } else {\n    let map = new Map;\n    for (let i = 0;i < length; i++) {\n      map.set(read(), read());\n    }\n    return map;\n  }\n}\nvar fromCharCode = String.fromCharCode;\nfunction longStringInJS(length) {\n  let start = position;\n  let bytes = new Array(length);\n  for (let i = 0;i < length; i++) {\n    const byte = src[position++];\n    if ((byte & 128) > 0) {\n      position = start;\n      return;\n    }\n    bytes[i] = byte;\n  }\n  return fromCharCode.apply(String, bytes);\n}\nfunction shortStringInJS(length) {\n  if (length < 4) {\n    if (length < 2) {\n      if (length === 0)\n        return \"\";\n      else {\n        let a = src[position++];\n        if ((a & 128) > 1) {\n          position -= 1;\n          return;\n        }\n        return fromCharCode(a);\n      }\n    } else {\n      let a = src[position++];\n      let b = src[position++];\n      if ((a & 128) > 0 || (b & 128) > 0) {\n        position -= 2;\n        return;\n      }\n      if (length < 3)\n        return fromCharCode(a, b);\n      let c = src[position++];\n      if ((c & 128) > 0) {\n        position -= 3;\n        return;\n      }\n      return fromCharCode(a, b, c);\n    }\n  } else {\n    let a = src[position++];\n    let b = src[position++];\n    let c = src[position++];\n    let d = src[position++];\n    if ((a & 128) > 0 || (b & 128) > 0 || (c & 128) > 0 || (d & 128) > 0) {\n      position -= 4;\n      return;\n    }\n    if (length < 6) {\n      if (length === 4)\n        return fromCharCode(a, b, c, d);\n      else {\n        let e = src[position++];\n        if ((e & 128) > 0) {\n          position -= 5;\n          return;\n        }\n        return fromCharCode(a, b, c, d, e);\n      }\n    } else if (length < 8) {\n      let e = src[position++];\n      let f = src[position++];\n      if ((e & 128) > 0 || (f & 128) > 0) {\n        position -= 6;\n        return;\n      }\n      if (length < 7)\n        return fromCharCode(a, b, c, d, e, f);\n      let g = src[position++];\n      if ((g & 128) > 0) {\n        position -= 7;\n        return;\n      }\n      return fromCharCode(a, b, c, d, e, f, g);\n    } else {\n      let e = src[position++];\n      let f = src[position++];\n      let g = src[position++];\n      let h = src[position++];\n      if ((e & 128) > 0 || (f & 128) > 0 || (g & 128) > 0 || (h & 128) > 0) {\n        position -= 8;\n        return;\n      }\n      if (length < 10) {\n        if (length === 8)\n          return fromCharCode(a, b, c, d, e, f, g, h);\n        else {\n          let i = src[position++];\n          if ((i & 128) > 0) {\n            position -= 9;\n            return;\n          }\n          return fromCharCode(a, b, c, d, e, f, g, h, i);\n        }\n      } else if (length < 12) {\n        let i = src[position++];\n        let j = src[position++];\n        if ((i & 128) > 0 || (j & 128) > 0) {\n          position -= 10;\n          return;\n        }\n        if (length < 11)\n          return fromCharCode(a, b, c, d, e, f, g, h, i, j);\n        let k = src[position++];\n        if ((k & 128) > 0) {\n          position -= 11;\n          return;\n        }\n        return fromCharCode(a, b, c, d, e, f, g, h, i, j, k);\n      } else {\n        let i = src[position++];\n        let j = src[position++];\n        let k = src[position++];\n        let l = src[position++];\n        if ((i & 128) > 0 || (j & 128) > 0 || (k & 128) > 0 || (l & 128) > 0) {\n          position -= 12;\n          return;\n        }\n        if (length < 14) {\n          if (length === 12)\n            return fromCharCode(a, b, c, d, e, f, g, h, i, j, k, l);\n          else {\n            let m = src[position++];\n            if ((m & 128) > 0) {\n              position -= 13;\n              return;\n            }\n            return fromCharCode(a, b, c, d, e, f, g, h, i, j, k, l, m);\n          }\n        } else {\n          let m = src[position++];\n          let n = src[position++];\n          if ((m & 128) > 0 || (n & 128) > 0) {\n            position -= 14;\n            return;\n          }\n          if (length < 15)\n            return fromCharCode(a, b, c, d, e, f, g, h, i, j, k, l, m, n);\n          let o = src[position++];\n          if ((o & 128) > 0) {\n            position -= 15;\n            return;\n          }\n          return fromCharCode(a, b, c, d, e, f, g, h, i, j, k, l, m, n, o);\n        }\n      }\n    }\n  }\n}\nfunction readOnlyJSString() {\n  let token = src[position++];\n  let length;\n  if (token < 192) {\n    length = token - 160;\n  } else {\n    switch (token) {\n      case 217:\n        length = src[position++];\n        break;\n      case 218:\n        length = dataView.getUint16(position);\n        position += 2;\n        break;\n      case 219:\n        length = dataView.getUint32(position);\n        position += 4;\n        break;\n      default:\n        throw new Error(\"Expected string\");\n    }\n  }\n  return readStringJS(length);\n}\nfunction readBin(length) {\n  return currentUnpackr.copyBuffers ? Uint8Array.prototype.slice.call(src, position, position += length) : src.subarray(position, position += length);\n}\nfunction readExt(length) {\n  let type = src[position++];\n  if (currentExtensions[type]) {\n    let end;\n    return currentExtensions[type](src.subarray(position, end = position += length), (readPosition) => {\n      position = readPosition;\n      try {\n        return read();\n      } finally {\n        position = end;\n      }\n    });\n  } else\n    throw new Error(\"Unknown extension type \" + type);\n}\nvar keyCache = new Array(4096);\nfunction readKey() {\n  let length = src[position++];\n  if (length >= 160 && length < 192) {\n    length = length - 160;\n    if (srcStringEnd >= position)\n      return srcString.slice(position - srcStringStart, (position += length) - srcStringStart);\n    else if (!(srcStringEnd == 0 && srcEnd < 180))\n      return readFixedString(length);\n  } else {\n    position--;\n    return asSafeString(read());\n  }\n  let key = (length << 5 ^ (length > 1 ? dataView.getUint16(position) : length > 0 ? src[position] : 0)) & 4095;\n  let entry = keyCache[key];\n  let checkPosition = position;\n  let end = position + length - 3;\n  let chunk;\n  let i = 0;\n  if (entry && entry.bytes == length) {\n    while (checkPosition < end) {\n      chunk = dataView.getUint32(checkPosition);\n      if (chunk != entry[i++]) {\n        checkPosition = 1879048192;\n        break;\n      }\n      checkPosition += 4;\n    }\n    end += 3;\n    while (checkPosition < end) {\n      chunk = src[checkPosition++];\n      if (chunk != entry[i++]) {\n        checkPosition = 1879048192;\n        break;\n      }\n    }\n    if (checkPosition === end) {\n      position = checkPosition;\n      return entry.string;\n    }\n    end -= 3;\n    checkPosition = position;\n  }\n  entry = [];\n  keyCache[key] = entry;\n  entry.bytes = length;\n  while (checkPosition < end) {\n    chunk = dataView.getUint32(checkPosition);\n    entry.push(chunk);\n    checkPosition += 4;\n  }\n  end += 3;\n  while (checkPosition < end) {\n    chunk = src[checkPosition++];\n    entry.push(chunk);\n  }\n  let string = length < 16 ? shortStringInJS(length) : longStringInJS(length);\n  if (string != null)\n    return entry.string = string;\n  return entry.string = readFixedString(length);\n}\nfunction asSafeString(property) {\n  if (typeof property === \"string\")\n    return property;\n  if (typeof property === \"number\" || typeof property === \"boolean\" || typeof property === \"bigint\")\n    return property.toString();\n  if (property == null)\n    return property + \"\";\n  if (currentUnpackr.allowArraysInMapKeys && Array.isArray(property) && property.flat().every((item) => [\"string\", \"number\", \"boolean\", \"bigint\"].includes(typeof item))) {\n    return property.flat().toString();\n  }\n  throw new Error(`Invalid property type for record: ${typeof property}`);\n}\nvar recordDefinition = (id, highByte) => {\n  let structure = read().map(asSafeString);\n  let firstByte = id;\n  if (highByte !== undefined) {\n    id = id < 32 ? -((highByte << 5) + id) : (highByte << 5) + id;\n    structure.highByte = highByte;\n  }\n  let existingStructure = currentStructures[id];\n  if (existingStructure && (existingStructure.isShared || sequentialMode)) {\n    (currentStructures.restoreStructures || (currentStructures.restoreStructures = []))[id] = existingStructure;\n  }\n  currentStructures[id] = structure;\n  structure.read = createStructureReader(structure, firstByte);\n  return structure.read();\n};\ncurrentExtensions[0] = () => {};\ncurrentExtensions[0].noBuffer = true;\ncurrentExtensions[66] = (data) => {\n  let headLength = data.byteLength % 8 || 8;\n  let head = BigInt(data[0] & 128 ? data[0] - 256 : data[0]);\n  for (let i = 1;i < headLength; i++) {\n    head <<= BigInt(8);\n    head += BigInt(data[i]);\n  }\n  if (data.byteLength !== headLength) {\n    let view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n    let decode = (start, end) => {\n      let length = end - start;\n      if (length <= 40) {\n        let out = view.getBigUint64(start);\n        for (let i = start + 8;i < end; i += 8) {\n          out <<= BigInt(64n);\n          out |= view.getBigUint64(i);\n        }\n        return out;\n      }\n      let middle = start + (length >> 4 << 3);\n      let left = decode(start, middle);\n      let right = decode(middle, end);\n      return left << BigInt((end - middle) * 8) | right;\n    };\n    head = head << BigInt((view.byteLength - headLength) * 8) | decode(headLength, view.byteLength);\n  }\n  return head;\n};\nvar errors = {\n  Error,\n  EvalError,\n  RangeError,\n  ReferenceError,\n  SyntaxError,\n  TypeError,\n  URIError,\n  AggregateError: typeof AggregateError === \"function\" ? AggregateError : null\n};\ncurrentExtensions[101] = () => {\n  let data = read();\n  if (!errors[data[0]]) {\n    let error = Error(data[1], { cause: data[2] });\n    error.name = data[0];\n    return error;\n  }\n  return errors[data[0]](data[1], { cause: data[2] });\n};\ncurrentExtensions[105] = (data) => {\n  if (currentUnpackr.structuredClone === false)\n    throw new Error(\"Structured clone extension is disabled\");\n  let id = dataView.getUint32(position - 4);\n  if (!referenceMap)\n    referenceMap = new Map;\n  let token = src[position];\n  let target;\n  if (token >= 144 && token < 160 || token == 220 || token == 221)\n    target = [];\n  else if (token >= 128 && token < 144 || token == 222 || token == 223)\n    target = new Map;\n  else if ((token >= 199 && token <= 201 || token >= 212 && token <= 216) && src[position + 1] === 115)\n    target = new Set;\n  else\n    target = {};\n  let refEntry = { target };\n  referenceMap.set(id, refEntry);\n  let targetProperties = read();\n  if (!refEntry.used) {\n    return refEntry.target = targetProperties;\n  } else {\n    Object.assign(target, targetProperties);\n  }\n  if (target instanceof Map)\n    for (let [k, v] of targetProperties.entries())\n      target.set(k, v);\n  if (target instanceof Set)\n    for (let i of Array.from(targetProperties))\n      target.add(i);\n  return target;\n};\ncurrentExtensions[112] = (data) => {\n  if (currentUnpackr.structuredClone === false)\n    throw new Error(\"Structured clone extension is disabled\");\n  let id = dataView.getUint32(position - 4);\n  let refEntry = referenceMap.get(id);\n  refEntry.used = true;\n  return refEntry.target;\n};\ncurrentExtensions[115] = () => new Set(read());\nvar typedArrays = [\"Int8\", \"Uint8\", \"Uint8Clamped\", \"Int16\", \"Uint16\", \"Int32\", \"Uint32\", \"Float32\", \"Float64\", \"BigInt64\", \"BigUint64\"].map((type) => type + \"Array\");\nvar glbl = typeof globalThis === \"object\" ? globalThis : window;\ncurrentExtensions[116] = (data) => {\n  let typeCode = data[0];\n  let buffer = Uint8Array.prototype.slice.call(data, 1).buffer;\n  let typedArrayName = typedArrays[typeCode];\n  if (!typedArrayName) {\n    if (typeCode === 16)\n      return buffer;\n    if (typeCode === 17)\n      return new DataView(buffer);\n    throw new Error(\"Could not find typed array for code \" + typeCode);\n  }\n  return new glbl[typedArrayName](buffer);\n};\ncurrentExtensions[120] = () => {\n  let data = read();\n  return new RegExp(data[0], data[1]);\n};\nvar TEMP_BUNDLE = [];\ncurrentExtensions[98] = (data) => {\n  let dataSize = (data[0] << 24) + (data[1] << 16) + (data[2] << 8) + data[3];\n  let dataPosition = position;\n  position += dataSize - data.length;\n  bundledStrings = TEMP_BUNDLE;\n  bundledStrings = [readOnlyJSString(), readOnlyJSString()];\n  bundledStrings.position0 = 0;\n  bundledStrings.position1 = 0;\n  bundledStrings.postBundlePosition = position;\n  position = dataPosition;\n  return read();\n};\ncurrentExtensions[255] = (data) => {\n  if (data.length == 4)\n    return new Date((data[0] * 16777216 + (data[1] << 16) + (data[2] << 8) + data[3]) * 1000);\n  else if (data.length == 8)\n    return new Date(((data[0] << 22) + (data[1] << 14) + (data[2] << 6) + (data[3] >> 2)) / 1e6 + ((data[3] & 3) * 4294967296 + data[4] * 16777216 + (data[5] << 16) + (data[6] << 8) + data[7]) * 1000);\n  else if (data.length == 12)\n    return new Date(((data[0] << 24) + (data[1] << 16) + (data[2] << 8) + data[3]) / 1e6 + ((data[4] & 128 ? -281474976710656 : 0) + data[6] * 1099511627776 + data[7] * 4294967296 + data[8] * 16777216 + (data[9] << 16) + (data[10] << 8) + data[11]) * 1000);\n  else\n    return new Date(\"invalid\");\n};\nfunction saveState(callback) {\n  if (onSaveState)\n    onSaveState();\n  let savedSrcEnd = srcEnd;\n  let savedPosition = position;\n  let savedStringPosition = stringPosition;\n  let savedSrcStringStart = srcStringStart;\n  let savedSrcStringEnd = srcStringEnd;\n  let savedSrcString = srcString;\n  let savedStrings = strings;\n  let savedReferenceMap = referenceMap;\n  let savedBundledStrings = bundledStrings;\n  let savedSrc = new Uint8Array(src.slice(0, srcEnd));\n  let savedStructures = currentStructures;\n  let savedStructuresContents = currentStructures.slice(0, currentStructures.length);\n  let savedPackr = currentUnpackr;\n  let savedSequentialMode = sequentialMode;\n  let value = callback();\n  srcEnd = savedSrcEnd;\n  position = savedPosition;\n  stringPosition = savedStringPosition;\n  srcStringStart = savedSrcStringStart;\n  srcStringEnd = savedSrcStringEnd;\n  srcString = savedSrcString;\n  strings = savedStrings;\n  referenceMap = savedReferenceMap;\n  bundledStrings = savedBundledStrings;\n  src = savedSrc;\n  sequentialMode = savedSequentialMode;\n  currentStructures = savedStructures;\n  currentStructures.splice(0, currentStructures.length, ...savedStructuresContents);\n  currentUnpackr = savedPackr;\n  dataView = new DataView(src.buffer, src.byteOffset, src.byteLength);\n  return value;\n}\nfunction clearSource() {\n  src = null;\n  referenceMap = null;\n  currentStructures = null;\n}\nvar mult10 = new Array(147);\nfor (let i = 0;i < 256; i++) {\n  mult10[i] = +(\"1e\" + Math.floor(45.15 - i * 0.30103));\n}\nvar defaultUnpackr = new Unpackr({ useRecords: false });\nvar unpack = defaultUnpackr.unpack;\nvar unpackMultiple = defaultUnpackr.unpackMultiple;\nvar decode = defaultUnpackr.unpack;\nvar f32Array = new Float32Array(1);\nvar u8Array = new Uint8Array(f32Array.buffer, 0, 4);\nfunction setReadStruct(updatedReadStruct, loadedStructs, saveState2) {\n  readStruct = updatedReadStruct;\n  onLoadedStructures = loadedStructs;\n  onSaveState = saveState2;\n}\n// ../../node_modules/.bun/msgpackr@1.11.8/node_modules/msgpackr/pack.js\nvar textEncoder;\ntry {\n  textEncoder = new TextEncoder;\n} catch (error) {}\nvar extensions;\nvar extensionClasses;\nvar hasNodeBuffer = typeof Buffer !== \"undefined\";\nvar ByteArrayAllocate = hasNodeBuffer ? function(length) {\n  return Buffer.allocUnsafeSlow(length);\n} : Uint8Array;\nvar ByteArray = hasNodeBuffer ? Buffer : Uint8Array;\nvar MAX_BUFFER_SIZE = hasNodeBuffer ? 4294967296 : 2144337920;\nvar target;\nvar keysTarget;\nvar targetView;\nvar position2 = 0;\nvar safeEnd;\nvar bundledStrings2 = null;\nvar writeStructSlots;\nvar MAX_BUNDLE_SIZE = 21760;\nvar hasNonLatin = /[\\u0080-\\uFFFF]/;\nvar RECORD_SYMBOL = Symbol(\"record-id\");\n\nclass Packr extends Unpackr {\n  constructor(options) {\n    super(options);\n    this.offset = 0;\n    let typeBuffer;\n    let start;\n    let hasSharedUpdate;\n    let structures;\n    let referenceMap2;\n    let encodeUtf8 = ByteArray.prototype.utf8Write ? function(string, position3) {\n      return target.utf8Write(string, position3, target.byteLength - position3);\n    } : textEncoder && textEncoder.encodeInto ? function(string, position3) {\n      return textEncoder.encodeInto(string, target.subarray(position3)).written;\n    } : false;\n    let packr = this;\n    if (!options)\n      options = {};\n    let isSequential = options && options.sequential;\n    let hasSharedStructures = options.structures || options.saveStructures;\n    let maxSharedStructures = options.maxSharedStructures;\n    if (maxSharedStructures == null)\n      maxSharedStructures = hasSharedStructures ? 32 : 0;\n    if (maxSharedStructures > 8160)\n      throw new Error(\"Maximum maxSharedStructure is 8160\");\n    if (options.structuredClone && options.moreTypes == undefined) {\n      this.moreTypes = true;\n    }\n    let maxOwnStructures = options.maxOwnStructures;\n    if (maxOwnStructures == null)\n      maxOwnStructures = hasSharedStructures ? 32 : 64;\n    if (!this.structures && options.useRecords != false)\n      this.structures = [];\n    let useTwoByteRecords = maxSharedStructures > 32 || maxOwnStructures + maxSharedStructures > 64;\n    let sharedLimitId = maxSharedStructures + 64;\n    let maxStructureId = maxSharedStructures + maxOwnStructures + 64;\n    if (maxStructureId > 8256) {\n      throw new Error(\"Maximum maxSharedStructure + maxOwnStructure is 8192\");\n    }\n    let recordIdsToRemove = [];\n    let transitionsCount = 0;\n    let serializationsSinceTransitionRebuild = 0;\n    this.pack = this.encode = function(value, encodeOptions) {\n      if (!target) {\n        target = new ByteArrayAllocate(8192);\n        targetView = target.dataView || (target.dataView = new DataView(target.buffer, 0, 8192));\n        position2 = 0;\n      }\n      safeEnd = target.length - 10;\n      if (safeEnd - position2 < 2048) {\n        target = new ByteArrayAllocate(target.length);\n        targetView = target.dataView || (target.dataView = new DataView(target.buffer, 0, target.length));\n        safeEnd = target.length - 10;\n        position2 = 0;\n      } else\n        position2 = position2 + 7 & 2147483640;\n      start = position2;\n      if (encodeOptions & RESERVE_START_SPACE)\n        position2 += encodeOptions & 255;\n      referenceMap2 = packr.structuredClone ? new Map : null;\n      if (packr.bundleStrings && typeof value !== \"string\") {\n        bundledStrings2 = [];\n        bundledStrings2.size = Infinity;\n      } else\n        bundledStrings2 = null;\n      structures = packr.structures;\n      if (structures) {\n        if (structures.uninitialized)\n          structures = packr._mergeStructures(packr.getStructures());\n        let sharedLength = structures.sharedLength || 0;\n        if (sharedLength > maxSharedStructures) {\n          throw new Error(\"Shared structures is larger than maximum shared structures, try increasing maxSharedStructures to \" + structures.sharedLength);\n        }\n        if (!structures.transitions) {\n          structures.transitions = Object.create(null);\n          for (let i = 0;i < sharedLength; i++) {\n            let keys = structures[i];\n            if (!keys)\n              continue;\n            let nextTransition, transition = structures.transitions;\n            for (let j = 0, l = keys.length;j < l; j++) {\n              let key = keys[j];\n              nextTransition = transition[key];\n              if (!nextTransition) {\n                nextTransition = transition[key] = Object.create(null);\n              }\n              transition = nextTransition;\n            }\n            transition[RECORD_SYMBOL] = i + 64;\n          }\n          this.lastNamedStructuresLength = sharedLength;\n        }\n        if (!isSequential) {\n          structures.nextId = sharedLength + 64;\n        }\n      }\n      if (hasSharedUpdate)\n        hasSharedUpdate = false;\n      let encodingError;\n      try {\n        if (packr.randomAccessStructure && value && typeof value === \"object\") {\n          if (value.constructor === Object)\n            writeStruct(value);\n          else if (value.constructor !== Map && !Array.isArray(value) && !extensionClasses.some((extClass) => value instanceof extClass)) {\n            writeStruct(value.toJSON ? value.toJSON() : value);\n          } else\n            pack(value);\n        } else\n          pack(value);\n        let lastBundle = bundledStrings2;\n        if (bundledStrings2)\n          writeBundles(start, pack, 0);\n        if (referenceMap2 && referenceMap2.idsToInsert) {\n          let idsToInsert = referenceMap2.idsToInsert.sort((a, b) => a.offset > b.offset ? 1 : -1);\n          let i = idsToInsert.length;\n          let incrementPosition = -1;\n          while (lastBundle && i > 0) {\n            let insertionPoint = idsToInsert[--i].offset + start;\n            if (insertionPoint < lastBundle.stringsPosition + start && incrementPosition === -1)\n              incrementPosition = 0;\n            if (insertionPoint > lastBundle.position + start) {\n              if (incrementPosition >= 0)\n                incrementPosition += 6;\n            } else {\n              if (incrementPosition >= 0) {\n                targetView.setUint32(lastBundle.position + start, targetView.getUint32(lastBundle.position + start) + incrementPosition);\n                incrementPosition = -1;\n              }\n              lastBundle = lastBundle.previous;\n              i++;\n            }\n          }\n          if (incrementPosition >= 0 && lastBundle) {\n            targetView.setUint32(lastBundle.position + start, targetView.getUint32(lastBundle.position + start) + incrementPosition);\n          }\n          position2 += idsToInsert.length * 6;\n          if (position2 > safeEnd)\n            makeRoom(position2);\n          packr.offset = position2;\n          let serialized = insertIds(target.subarray(start, position2), idsToInsert);\n          referenceMap2 = null;\n          return serialized;\n        }\n        packr.offset = position2;\n        if (encodeOptions & REUSE_BUFFER_MODE) {\n          target.start = start;\n          target.end = position2;\n          return target;\n        }\n        return target.subarray(start, position2);\n      } catch (error) {\n        encodingError = error;\n        throw error;\n      } finally {\n        if (structures) {\n          resetStructures();\n          if (hasSharedUpdate && packr.saveStructures) {\n            let sharedLength = structures.sharedLength || 0;\n            let returnBuffer = target.subarray(start, position2);\n            let newSharedData = prepareStructures(structures, packr);\n            if (!encodingError) {\n              if (packr.saveStructures(newSharedData, newSharedData.isCompatible) === false) {\n                return packr.pack(value, encodeOptions);\n              }\n              packr.lastNamedStructuresLength = sharedLength;\n              if (target.length > 1073741824)\n                target = null;\n              return returnBuffer;\n            }\n          }\n        }\n        if (target.length > 1073741824)\n          target = null;\n        if (encodeOptions & RESET_BUFFER_MODE)\n          position2 = start;\n      }\n    };\n    const resetStructures = () => {\n      if (serializationsSinceTransitionRebuild < 10)\n        serializationsSinceTransitionRebuild++;\n      let sharedLength = structures.sharedLength || 0;\n      if (structures.length > sharedLength && !isSequential)\n        structures.length = sharedLength;\n      if (transitionsCount > 1e4) {\n        structures.transitions = null;\n        serializationsSinceTransitionRebuild = 0;\n        transitionsCount = 0;\n        if (recordIdsToRemove.length > 0)\n          recordIdsToRemove = [];\n      } else if (recordIdsToRemove.length > 0 && !isSequential) {\n        for (let i = 0, l = recordIdsToRemove.length;i < l; i++) {\n          recordIdsToRemove[i][RECORD_SYMBOL] = 0;\n        }\n        recordIdsToRemove = [];\n      }\n    };\n    const packArray = (value) => {\n      var length = value.length;\n      if (length < 16) {\n        target[position2++] = 144 | length;\n      } else if (length < 65536) {\n        target[position2++] = 220;\n        target[position2++] = length >> 8;\n        target[position2++] = length & 255;\n      } else {\n        target[position2++] = 221;\n        targetView.setUint32(position2, length);\n        position2 += 4;\n      }\n      for (let i = 0;i < length; i++) {\n        pack(value[i]);\n      }\n    };\n    const pack = (value) => {\n      if (position2 > safeEnd)\n        target = makeRoom(position2);\n      var type = typeof value;\n      var length;\n      if (type === \"string\") {\n        let strLength = value.length;\n        if (bundledStrings2 && strLength >= 4 && strLength < 4096) {\n          if ((bundledStrings2.size += strLength) > MAX_BUNDLE_SIZE) {\n            let extStart;\n            let maxBytes2 = (bundledStrings2[0] ? bundledStrings2[0].length * 3 + bundledStrings2[1].length : 0) + 10;\n            if (position2 + maxBytes2 > safeEnd)\n              target = makeRoom(position2 + maxBytes2);\n            let lastBundle;\n            if (bundledStrings2.position) {\n              lastBundle = bundledStrings2;\n              target[position2] = 200;\n              position2 += 3;\n              target[position2++] = 98;\n              extStart = position2 - start;\n              position2 += 4;\n              writeBundles(start, pack, 0);\n              targetView.setUint16(extStart + start - 3, position2 - start - extStart);\n            } else {\n              target[position2++] = 214;\n              target[position2++] = 98;\n              extStart = position2 - start;\n              position2 += 4;\n            }\n            bundledStrings2 = [\"\", \"\"];\n            bundledStrings2.previous = lastBundle;\n            bundledStrings2.size = 0;\n            bundledStrings2.position = extStart;\n          }\n          let twoByte = hasNonLatin.test(value);\n          bundledStrings2[twoByte ? 0 : 1] += value;\n          target[position2++] = 193;\n          pack(twoByte ? -strLength : strLength);\n          return;\n        }\n        let headerSize;\n        if (strLength < 32) {\n          headerSize = 1;\n        } else if (strLength < 256) {\n          headerSize = 2;\n        } else if (strLength < 65536) {\n          headerSize = 3;\n        } else {\n          headerSize = 5;\n        }\n        let maxBytes = strLength * 3;\n        if (position2 + maxBytes > safeEnd)\n          target = makeRoom(position2 + maxBytes);\n        if (strLength < 64 || !encodeUtf8) {\n          let i, c1, c2, strPosition = position2 + headerSize;\n          for (i = 0;i < strLength; i++) {\n            c1 = value.charCodeAt(i);\n            if (c1 < 128) {\n              target[strPosition++] = c1;\n            } else if (c1 < 2048) {\n              target[strPosition++] = c1 >> 6 | 192;\n              target[strPosition++] = c1 & 63 | 128;\n            } else if ((c1 & 64512) === 55296 && ((c2 = value.charCodeAt(i + 1)) & 64512) === 56320) {\n              c1 = 65536 + ((c1 & 1023) << 10) + (c2 & 1023);\n              i++;\n              target[strPosition++] = c1 >> 18 | 240;\n              target[strPosition++] = c1 >> 12 & 63 | 128;\n              target[strPosition++] = c1 >> 6 & 63 | 128;\n              target[strPosition++] = c1 & 63 | 128;\n            } else {\n              target[strPosition++] = c1 >> 12 | 224;\n              target[strPosition++] = c1 >> 6 & 63 | 128;\n              target[strPosition++] = c1 & 63 | 128;\n            }\n          }\n          length = strPosition - position2 - headerSize;\n        } else {\n          length = encodeUtf8(value, position2 + headerSize);\n        }\n        if (length < 32) {\n          target[position2++] = 160 | length;\n        } else if (length < 256) {\n          if (headerSize < 2) {\n            target.copyWithin(position2 + 2, position2 + 1, position2 + 1 + length);\n          }\n          target[position2++] = 217;\n          target[position2++] = length;\n        } else if (length < 65536) {\n          if (headerSize < 3) {\n            target.copyWithin(position2 + 3, position2 + 2, position2 + 2 + length);\n          }\n          target[position2++] = 218;\n          target[position2++] = length >> 8;\n          target[position2++] = length & 255;\n        } else {\n          if (headerSize < 5) {\n            target.copyWithin(position2 + 5, position2 + 3, position2 + 3 + length);\n          }\n          target[position2++] = 219;\n          targetView.setUint32(position2, length);\n          position2 += 4;\n        }\n        position2 += length;\n      } else if (type === \"number\") {\n        if (value >>> 0 === value) {\n          if (value < 32 || value < 128 && this.useRecords === false || value < 64 && !this.randomAccessStructure) {\n            target[position2++] = value;\n          } else if (value < 256) {\n            target[position2++] = 204;\n            target[position2++] = value;\n          } else if (value < 65536) {\n            target[position2++] = 205;\n            target[position2++] = value >> 8;\n            target[position2++] = value & 255;\n          } else {\n            target[position2++] = 206;\n            targetView.setUint32(position2, value);\n            position2 += 4;\n          }\n        } else if (value >> 0 === value) {\n          if (value >= -32) {\n            target[position2++] = 256 + value;\n          } else if (value >= -128) {\n            target[position2++] = 208;\n            target[position2++] = value + 256;\n          } else if (value >= -32768) {\n            target[position2++] = 209;\n            targetView.setInt16(position2, value);\n            position2 += 2;\n          } else {\n            target[position2++] = 210;\n            targetView.setInt32(position2, value);\n            position2 += 4;\n          }\n        } else {\n          let useFloat32;\n          if ((useFloat32 = this.useFloat32) > 0 && value < 4294967296 && value >= -2147483648) {\n            target[position2++] = 202;\n            targetView.setFloat32(position2, value);\n            let xShifted;\n            if (useFloat32 < 4 || (xShifted = value * mult10[(target[position2] & 127) << 1 | target[position2 + 1] >> 7]) >> 0 === xShifted) {\n              position2 += 4;\n              return;\n            } else\n              position2--;\n          }\n          target[position2++] = 203;\n          targetView.setFloat64(position2, value);\n          position2 += 8;\n        }\n      } else if (type === \"object\" || type === \"function\") {\n        if (!value)\n          target[position2++] = 192;\n        else {\n          if (referenceMap2) {\n            let referee = referenceMap2.get(value);\n            if (referee) {\n              if (!referee.id) {\n                let idsToInsert = referenceMap2.idsToInsert || (referenceMap2.idsToInsert = []);\n                referee.id = idsToInsert.push(referee);\n              }\n              target[position2++] = 214;\n              target[position2++] = 112;\n              targetView.setUint32(position2, referee.id);\n              position2 += 4;\n              return;\n            } else\n              referenceMap2.set(value, { offset: position2 - start });\n          }\n          let constructor = value.constructor;\n          if (constructor === Object) {\n            writeObject(value);\n          } else if (constructor === Array) {\n            packArray(value);\n          } else if (constructor === Map) {\n            if (this.mapAsEmptyObject)\n              target[position2++] = 128;\n            else {\n              length = value.size;\n              if (length < 16) {\n                target[position2++] = 128 | length;\n              } else if (length < 65536) {\n                target[position2++] = 222;\n                target[position2++] = length >> 8;\n                target[position2++] = length & 255;\n              } else {\n                target[position2++] = 223;\n                targetView.setUint32(position2, length);\n                position2 += 4;\n              }\n              for (let [key, entryValue] of value) {\n                pack(key);\n                pack(entryValue);\n              }\n            }\n          } else {\n            for (let i = 0, l = extensions.length;i < l; i++) {\n              let extensionClass = extensionClasses[i];\n              if (value instanceof extensionClass) {\n                let extension = extensions[i];\n                if (extension.write) {\n                  if (extension.type) {\n                    target[position2++] = 212;\n                    target[position2++] = extension.type;\n                    target[position2++] = 0;\n                  }\n                  let writeResult = extension.write.call(this, value);\n                  if (writeResult === value) {\n                    if (Array.isArray(value)) {\n                      packArray(value);\n                    } else {\n                      writeObject(value);\n                    }\n                  } else {\n                    pack(writeResult);\n                  }\n                  return;\n                }\n                let currentTarget = target;\n                let currentTargetView = targetView;\n                let currentPosition = position2;\n                target = null;\n                let result;\n                try {\n                  result = extension.pack.call(this, value, (size) => {\n                    target = currentTarget;\n                    currentTarget = null;\n                    position2 += size;\n                    if (position2 > safeEnd)\n                      makeRoom(position2);\n                    return {\n                      target,\n                      targetView,\n                      position: position2 - size\n                    };\n                  }, pack);\n                } finally {\n                  if (currentTarget) {\n                    target = currentTarget;\n                    targetView = currentTargetView;\n                    position2 = currentPosition;\n                    safeEnd = target.length - 10;\n                  }\n                }\n                if (result) {\n                  if (result.length + position2 > safeEnd)\n                    makeRoom(result.length + position2);\n                  position2 = writeExtensionData(result, target, position2, extension.type);\n                }\n                return;\n              }\n            }\n            if (Array.isArray(value)) {\n              packArray(value);\n            } else {\n              if (value.toJSON) {\n                const json = value.toJSON();\n                if (json !== value)\n                  return pack(json);\n              }\n              if (type === \"function\")\n                return pack(this.writeFunction && this.writeFunction(value));\n              writeObject(value);\n            }\n          }\n        }\n      } else if (type === \"boolean\") {\n        target[position2++] = value ? 195 : 194;\n      } else if (type === \"bigint\") {\n        if (value < 9223372036854776000 && value >= -9223372036854776000) {\n          target[position2++] = 211;\n          targetView.setBigInt64(position2, value);\n        } else if (value < 18446744073709552000 && value > 0) {\n          target[position2++] = 207;\n          targetView.setBigUint64(position2, value);\n        } else {\n          if (this.largeBigIntToFloat) {\n            target[position2++] = 203;\n            targetView.setFloat64(position2, Number(value));\n          } else if (this.largeBigIntToString) {\n            return pack(value.toString());\n          } else if (this.useBigIntExtension || this.moreTypes) {\n            let empty = value < 0 ? BigInt(-1) : BigInt(0);\n            let array;\n            if (value >> BigInt(65536) === empty) {\n              let mask = BigInt(18446744073709552000) - BigInt(1);\n              let chunks = [];\n              while (true) {\n                chunks.push(value & mask);\n                if (value >> BigInt(63) === empty)\n                  break;\n                value >>= BigInt(64);\n              }\n              array = new Uint8Array(new BigUint64Array(chunks).buffer);\n              array.reverse();\n            } else {\n              let invert = value < 0;\n              let string = (invert ? ~value : value).toString(16);\n              if (string.length % 2) {\n                string = \"0\" + string;\n              } else if (parseInt(string.charAt(0), 16) >= 8) {\n                string = \"00\" + string;\n              }\n              if (hasNodeBuffer) {\n                array = Buffer.from(string, \"hex\");\n              } else {\n                array = new Uint8Array(string.length / 2);\n                for (let i = 0;i < array.length; i++) {\n                  array[i] = parseInt(string.slice(i * 2, i * 2 + 2), 16);\n                }\n              }\n              if (invert) {\n                for (let i = 0;i < array.length; i++)\n                  array[i] = ~array[i];\n              }\n            }\n            if (array.length + position2 > safeEnd)\n              makeRoom(array.length + position2);\n            position2 = writeExtensionData(array, target, position2, 66);\n            return;\n          } else {\n            throw new RangeError(value + \" was too large to fit in MessagePack 64-bit integer format, use\" + \" useBigIntExtension, or set largeBigIntToFloat to convert to float-64, or set\" + \" largeBigIntToString to convert to string\");\n          }\n        }\n        position2 += 8;\n      } else if (type === \"undefined\") {\n        if (this.encodeUndefinedAsNil)\n          target[position2++] = 192;\n        else {\n          target[position2++] = 212;\n          target[position2++] = 0;\n          target[position2++] = 0;\n        }\n      } else {\n        throw new Error(\"Unknown type: \" + type);\n      }\n    };\n    const writePlainObject = this.variableMapSize || this.coercibleKeyAsNumber || this.skipValues ? (object) => {\n      let keys;\n      if (this.skipValues) {\n        keys = [];\n        for (let key2 in object) {\n          if ((typeof object.hasOwnProperty !== \"function\" || object.hasOwnProperty(key2)) && !this.skipValues.includes(object[key2]))\n            keys.push(key2);\n        }\n      } else {\n        keys = Object.keys(object);\n      }\n      let length = keys.length;\n      if (length < 16) {\n        target[position2++] = 128 | length;\n      } else if (length < 65536) {\n        target[position2++] = 222;\n        target[position2++] = length >> 8;\n        target[position2++] = length & 255;\n      } else {\n        target[position2++] = 223;\n        targetView.setUint32(position2, length);\n        position2 += 4;\n      }\n      let key;\n      if (this.coercibleKeyAsNumber) {\n        for (let i = 0;i < length; i++) {\n          key = keys[i];\n          let num = Number(key);\n          pack(isNaN(num) ? key : num);\n          pack(object[key]);\n        }\n      } else {\n        for (let i = 0;i < length; i++) {\n          pack(key = keys[i]);\n          pack(object[key]);\n        }\n      }\n    } : (object) => {\n      target[position2++] = 222;\n      let objectOffset = position2 - start;\n      position2 += 2;\n      let size = 0;\n      for (let key in object) {\n        if (typeof object.hasOwnProperty !== \"function\" || object.hasOwnProperty(key)) {\n          pack(key);\n          pack(object[key]);\n          size++;\n        }\n      }\n      if (size > 65535) {\n        throw new Error(\"Object is too large to serialize with fast 16-bit map size,\" + ' use the \"variableMapSize\" option to serialize this object');\n      }\n      target[objectOffset++ + start] = size >> 8;\n      target[objectOffset + start] = size & 255;\n    };\n    const writeRecord = this.useRecords === false ? writePlainObject : options.progressiveRecords && !useTwoByteRecords ? (object) => {\n      let nextTransition, transition = structures.transitions || (structures.transitions = Object.create(null));\n      let objectOffset = position2++ - start;\n      let wroteKeys;\n      for (let key in object) {\n        if (typeof object.hasOwnProperty !== \"function\" || object.hasOwnProperty(key)) {\n          nextTransition = transition[key];\n          if (nextTransition)\n            transition = nextTransition;\n          else {\n            let keys = Object.keys(object);\n            let lastTransition = transition;\n            transition = structures.transitions;\n            let newTransitions = 0;\n            for (let i = 0, l = keys.length;i < l; i++) {\n              let key2 = keys[i];\n              nextTransition = transition[key2];\n              if (!nextTransition) {\n                nextTransition = transition[key2] = Object.create(null);\n                newTransitions++;\n              }\n              transition = nextTransition;\n            }\n            if (objectOffset + start + 1 == position2) {\n              position2--;\n              newRecord(transition, keys, newTransitions);\n            } else\n              insertNewRecord(transition, keys, objectOffset, newTransitions);\n            wroteKeys = true;\n            transition = lastTransition[key];\n          }\n          pack(object[key]);\n        }\n      }\n      if (!wroteKeys) {\n        let recordId = transition[RECORD_SYMBOL];\n        if (recordId)\n          target[objectOffset + start] = recordId;\n        else\n          insertNewRecord(transition, Object.keys(object), objectOffset, 0);\n      }\n    } : (object) => {\n      let nextTransition, transition = structures.transitions || (structures.transitions = Object.create(null));\n      let newTransitions = 0;\n      for (let key in object)\n        if (typeof object.hasOwnProperty !== \"function\" || object.hasOwnProperty(key)) {\n          nextTransition = transition[key];\n          if (!nextTransition) {\n            nextTransition = transition[key] = Object.create(null);\n            newTransitions++;\n          }\n          transition = nextTransition;\n        }\n      let recordId = transition[RECORD_SYMBOL];\n      if (recordId) {\n        if (recordId >= 96 && useTwoByteRecords) {\n          target[position2++] = ((recordId -= 96) & 31) + 96;\n          target[position2++] = recordId >> 5;\n        } else\n          target[position2++] = recordId;\n      } else {\n        newRecord(transition, transition.__keys__ || Object.keys(object), newTransitions);\n      }\n      for (let key in object)\n        if (typeof object.hasOwnProperty !== \"function\" || object.hasOwnProperty(key)) {\n          pack(object[key]);\n        }\n    };\n    const checkUseRecords = typeof this.useRecords == \"function\" && this.useRecords;\n    const writeObject = checkUseRecords ? (object) => {\n      checkUseRecords(object) ? writeRecord(object) : writePlainObject(object);\n    } : writeRecord;\n    const makeRoom = (end) => {\n      let newSize;\n      if (end > 16777216) {\n        if (end - start > MAX_BUFFER_SIZE)\n          throw new Error(\"Packed buffer would be larger than maximum buffer size\");\n        newSize = Math.min(MAX_BUFFER_SIZE, Math.round(Math.max((end - start) * (end > 67108864 ? 1.25 : 2), 4194304) / 4096) * 4096);\n      } else\n        newSize = (Math.max(end - start << 2, target.length - 1) >> 12) + 1 << 12;\n      let newBuffer = new ByteArrayAllocate(newSize);\n      targetView = newBuffer.dataView || (newBuffer.dataView = new DataView(newBuffer.buffer, 0, newSize));\n      end = Math.min(end, target.length);\n      if (target.copy)\n        target.copy(newBuffer, 0, start, end);\n      else\n        newBuffer.set(target.slice(start, end));\n      position2 -= start;\n      start = 0;\n      safeEnd = newBuffer.length - 10;\n      return target = newBuffer;\n    };\n    const newRecord = (transition, keys, newTransitions) => {\n      let recordId = structures.nextId;\n      if (!recordId)\n        recordId = 64;\n      if (recordId < sharedLimitId && this.shouldShareStructure && !this.shouldShareStructure(keys)) {\n        recordId = structures.nextOwnId;\n        if (!(recordId < maxStructureId))\n          recordId = sharedLimitId;\n        structures.nextOwnId = recordId + 1;\n      } else {\n        if (recordId >= maxStructureId)\n          recordId = sharedLimitId;\n        structures.nextId = recordId + 1;\n      }\n      let highByte = keys.highByte = recordId >= 96 && useTwoByteRecords ? recordId - 96 >> 5 : -1;\n      transition[RECORD_SYMBOL] = recordId;\n      transition.__keys__ = keys;\n      structures[recordId - 64] = keys;\n      if (recordId < sharedLimitId) {\n        keys.isShared = true;\n        structures.sharedLength = recordId - 63;\n        hasSharedUpdate = true;\n        if (highByte >= 0) {\n          target[position2++] = (recordId & 31) + 96;\n          target[position2++] = highByte;\n        } else {\n          target[position2++] = recordId;\n        }\n      } else {\n        if (highByte >= 0) {\n          target[position2++] = 213;\n          target[position2++] = 114;\n          target[position2++] = (recordId & 31) + 96;\n          target[position2++] = highByte;\n        } else {\n          target[position2++] = 212;\n          target[position2++] = 114;\n          target[position2++] = recordId;\n        }\n        if (newTransitions)\n          transitionsCount += serializationsSinceTransitionRebuild * newTransitions;\n        if (recordIdsToRemove.length >= maxOwnStructures)\n          recordIdsToRemove.shift()[RECORD_SYMBOL] = 0;\n        recordIdsToRemove.push(transition);\n        pack(keys);\n      }\n    };\n    const insertNewRecord = (transition, keys, insertionOffset, newTransitions) => {\n      let mainTarget = target;\n      let mainPosition = position2;\n      let mainSafeEnd = safeEnd;\n      let mainStart = start;\n      target = keysTarget;\n      position2 = 0;\n      start = 0;\n      if (!target)\n        keysTarget = target = new ByteArrayAllocate(8192);\n      safeEnd = target.length - 10;\n      newRecord(transition, keys, newTransitions);\n      keysTarget = target;\n      let keysPosition = position2;\n      target = mainTarget;\n      position2 = mainPosition;\n      safeEnd = mainSafeEnd;\n      start = mainStart;\n      if (keysPosition > 1) {\n        let newEnd = position2 + keysPosition - 1;\n        if (newEnd > safeEnd)\n          makeRoom(newEnd);\n        let insertionPosition = insertionOffset + start;\n        target.copyWithin(insertionPosition + keysPosition, insertionPosition + 1, position2);\n        target.set(keysTarget.slice(0, keysPosition), insertionPosition);\n        position2 = newEnd;\n      } else {\n        target[insertionOffset + start] = keysTarget[0];\n      }\n    };\n    const writeStruct = (object) => {\n      let newPosition = writeStructSlots(object, target, start, position2, structures, makeRoom, (value, newPosition2, notifySharedUpdate) => {\n        if (notifySharedUpdate)\n          return hasSharedUpdate = true;\n        position2 = newPosition2;\n        let startTarget = target;\n        pack(value);\n        resetStructures();\n        if (startTarget !== target) {\n          return { position: position2, targetView, target };\n        }\n        return position2;\n      }, this);\n      if (newPosition === 0)\n        return writeObject(object);\n      position2 = newPosition;\n    };\n  }\n  useBuffer(buffer) {\n    target = buffer;\n    target.dataView || (target.dataView = new DataView(target.buffer, target.byteOffset, target.byteLength));\n    targetView = target.dataView;\n    position2 = 0;\n  }\n  set position(value) {\n    position2 = value;\n  }\n  get position() {\n    return position2;\n  }\n  clearSharedData() {\n    if (this.structures)\n      this.structures = [];\n    if (this.typedStructs)\n      this.typedStructs = [];\n  }\n}\nextensionClasses = [Date, Set, Error, RegExp, ArrayBuffer, Object.getPrototypeOf(Uint8Array.prototype).constructor, DataView, C1Type];\nextensions = [{\n  pack(date, allocateForWrite, pack) {\n    let seconds = date.getTime() / 1000;\n    if ((this.useTimestamp32 || date.getMilliseconds() === 0) && seconds >= 0 && seconds < 4294967296) {\n      let { target: target2, targetView: targetView2, position: position3 } = allocateForWrite(6);\n      target2[position3++] = 214;\n      target2[position3++] = 255;\n      targetView2.setUint32(position3, seconds);\n    } else if (seconds > 0 && seconds < 4294967296) {\n      let { target: target2, targetView: targetView2, position: position3 } = allocateForWrite(10);\n      target2[position3++] = 215;\n      target2[position3++] = 255;\n      targetView2.setUint32(position3, date.getMilliseconds() * 4000000 + (seconds / 1000 / 4294967296 >> 0));\n      targetView2.setUint32(position3 + 4, seconds);\n    } else if (isNaN(seconds)) {\n      if (this.onInvalidDate) {\n        allocateForWrite(0);\n        return pack(this.onInvalidDate());\n      }\n      let { target: target2, targetView: targetView2, position: position3 } = allocateForWrite(3);\n      target2[position3++] = 212;\n      target2[position3++] = 255;\n      target2[position3++] = 255;\n    } else {\n      let { target: target2, targetView: targetView2, position: position3 } = allocateForWrite(15);\n      target2[position3++] = 199;\n      target2[position3++] = 12;\n      target2[position3++] = 255;\n      targetView2.setUint32(position3, date.getMilliseconds() * 1e6);\n      targetView2.setBigInt64(position3 + 4, BigInt(Math.floor(seconds)));\n    }\n  }\n}, {\n  pack(set, allocateForWrite, pack) {\n    if (this.setAsEmptyObject) {\n      allocateForWrite(0);\n      return pack({});\n    }\n    let array = Array.from(set);\n    let { target: target2, position: position3 } = allocateForWrite(this.moreTypes ? 3 : 0);\n    if (this.moreTypes) {\n      target2[position3++] = 212;\n      target2[position3++] = 115;\n      target2[position3++] = 0;\n    }\n    pack(array);\n  }\n}, {\n  pack(error, allocateForWrite, pack) {\n    let { target: target2, position: position3 } = allocateForWrite(this.moreTypes ? 3 : 0);\n    if (this.moreTypes) {\n      target2[position3++] = 212;\n      target2[position3++] = 101;\n      target2[position3++] = 0;\n    }\n    pack([error.name, error.message, error.cause]);\n  }\n}, {\n  pack(regex, allocateForWrite, pack) {\n    let { target: target2, position: position3 } = allocateForWrite(this.moreTypes ? 3 : 0);\n    if (this.moreTypes) {\n      target2[position3++] = 212;\n      target2[position3++] = 120;\n      target2[position3++] = 0;\n    }\n    pack([regex.source, regex.flags]);\n  }\n}, {\n  pack(arrayBuffer, allocateForWrite) {\n    if (this.moreTypes)\n      writeExtBuffer(arrayBuffer, 16, allocateForWrite);\n    else\n      writeBuffer(hasNodeBuffer ? Buffer.from(arrayBuffer) : new Uint8Array(arrayBuffer), allocateForWrite);\n  }\n}, {\n  pack(typedArray, allocateForWrite) {\n    let constructor = typedArray.constructor;\n    if (constructor !== ByteArray && this.moreTypes)\n      writeExtBuffer(typedArray, typedArrays.indexOf(constructor.name), allocateForWrite);\n    else\n      writeBuffer(typedArray, allocateForWrite);\n  }\n}, {\n  pack(arrayBuffer, allocateForWrite) {\n    if (this.moreTypes)\n      writeExtBuffer(arrayBuffer, 17, allocateForWrite);\n    else\n      writeBuffer(hasNodeBuffer ? Buffer.from(arrayBuffer) : new Uint8Array(arrayBuffer), allocateForWrite);\n  }\n}, {\n  pack(c1, allocateForWrite) {\n    let { target: target2, position: position3 } = allocateForWrite(1);\n    target2[position3] = 193;\n  }\n}];\nfunction writeExtBuffer(typedArray, type, allocateForWrite, encode) {\n  let length = typedArray.byteLength;\n  if (length + 1 < 256) {\n    var { target: target2, position: position3 } = allocateForWrite(4 + length);\n    target2[position3++] = 199;\n    target2[position3++] = length + 1;\n  } else if (length + 1 < 65536) {\n    var { target: target2, position: position3 } = allocateForWrite(5 + length);\n    target2[position3++] = 200;\n    target2[position3++] = length + 1 >> 8;\n    target2[position3++] = length + 1 & 255;\n  } else {\n    var { target: target2, position: position3, targetView: targetView2 } = allocateForWrite(7 + length);\n    target2[position3++] = 201;\n    targetView2.setUint32(position3, length + 1);\n    position3 += 4;\n  }\n  target2[position3++] = 116;\n  target2[position3++] = type;\n  if (!typedArray.buffer)\n    typedArray = new Uint8Array(typedArray);\n  target2.set(new Uint8Array(typedArray.buffer, typedArray.byteOffset, typedArray.byteLength), position3);\n}\nfunction writeBuffer(buffer, allocateForWrite) {\n  let length = buffer.byteLength;\n  var target2, position3;\n  if (length < 256) {\n    var { target: target2, position: position3 } = allocateForWrite(length + 2);\n    target2[position3++] = 196;\n    target2[position3++] = length;\n  } else if (length < 65536) {\n    var { target: target2, position: position3 } = allocateForWrite(length + 3);\n    target2[position3++] = 197;\n    target2[position3++] = length >> 8;\n    target2[position3++] = length & 255;\n  } else {\n    var { target: target2, position: position3, targetView: targetView2 } = allocateForWrite(length + 5);\n    target2[position3++] = 198;\n    targetView2.setUint32(position3, length);\n    position3 += 4;\n  }\n  target2.set(buffer, position3);\n}\nfunction writeExtensionData(result, target2, position3, type) {\n  let length = result.length;\n  switch (length) {\n    case 1:\n      target2[position3++] = 212;\n      break;\n    case 2:\n      target2[position3++] = 213;\n      break;\n    case 4:\n      target2[position3++] = 214;\n      break;\n    case 8:\n      target2[position3++] = 215;\n      break;\n    case 16:\n      target2[position3++] = 216;\n      break;\n    default:\n      if (length < 256) {\n        target2[position3++] = 199;\n        target2[position3++] = length;\n      } else if (length < 65536) {\n        target2[position3++] = 200;\n        target2[position3++] = length >> 8;\n        target2[position3++] = length & 255;\n      } else {\n        target2[position3++] = 201;\n        target2[position3++] = length >> 24;\n        target2[position3++] = length >> 16 & 255;\n        target2[position3++] = length >> 8 & 255;\n        target2[position3++] = length & 255;\n      }\n  }\n  target2[position3++] = type;\n  target2.set(result, position3);\n  position3 += length;\n  return position3;\n}\nfunction insertIds(serialized, idsToInsert) {\n  let nextId;\n  let distanceToMove = idsToInsert.length * 6;\n  let lastEnd = serialized.length - distanceToMove;\n  while (nextId = idsToInsert.pop()) {\n    let offset = nextId.offset;\n    let id = nextId.id;\n    serialized.copyWithin(offset + distanceToMove, offset, lastEnd);\n    distanceToMove -= 6;\n    let position3 = offset + distanceToMove;\n    serialized[position3++] = 214;\n    serialized[position3++] = 105;\n    serialized[position3++] = id >> 24;\n    serialized[position3++] = id >> 16 & 255;\n    serialized[position3++] = id >> 8 & 255;\n    serialized[position3++] = id & 255;\n    lastEnd = offset;\n  }\n  return serialized;\n}\nfunction writeBundles(start, pack, incrementPosition) {\n  if (bundledStrings2.length > 0) {\n    targetView.setUint32(bundledStrings2.position + start, position2 + incrementPosition - bundledStrings2.position - start);\n    bundledStrings2.stringsPosition = position2 - start;\n    let writeStrings = bundledStrings2;\n    bundledStrings2 = null;\n    pack(writeStrings[0]);\n    pack(writeStrings[1]);\n  }\n}\nfunction prepareStructures(structures, packr) {\n  structures.isCompatible = (existingStructures) => {\n    let compatible = !existingStructures || (packr.lastNamedStructuresLength || 0) === existingStructures.length;\n    if (!compatible)\n      packr._mergeStructures(existingStructures);\n    return compatible;\n  };\n  return structures;\n}\nfunction setWriteStructSlots(writeSlots, makeStructures) {\n  writeStructSlots = writeSlots;\n  prepareStructures = makeStructures;\n}\nvar defaultPackr = new Packr({ useRecords: false });\nvar pack = defaultPackr.pack;\nvar encode = defaultPackr.pack;\nvar REUSE_BUFFER_MODE = 512;\nvar RESET_BUFFER_MODE = 1024;\nvar RESERVE_START_SPACE = 2048;\n// ../../node_modules/.bun/msgpackr@1.11.8/node_modules/msgpackr/struct.js\nvar ASCII = 3;\nvar NUMBER = 0;\nvar UTF8 = 2;\nvar OBJECT_DATA = 1;\nvar DATE = 16;\nvar TYPE_NAMES = [\"num\", \"object\", \"string\", \"ascii\"];\nTYPE_NAMES[DATE] = \"date\";\nvar float32Headers = [false, true, true, false, false, true, true, false];\nvar evalSupported;\ntry {\n  new Function(\"\");\n  evalSupported = true;\n} catch (error) {}\nvar updatedPosition;\nvar hasNodeBuffer2 = typeof Buffer !== \"undefined\";\nvar textEncoder2;\nvar currentSource;\ntry {\n  textEncoder2 = new TextEncoder;\n} catch (error) {}\nvar encodeUtf8 = hasNodeBuffer2 ? function(target2, string, position3) {\n  return target2.utf8Write(string, position3, target2.byteLength - position3);\n} : textEncoder2 && textEncoder2.encodeInto ? function(target2, string, position3) {\n  return textEncoder2.encodeInto(string, target2.subarray(position3)).written;\n} : false;\nvar TYPE = Symbol(\"type\");\nvar PARENT = Symbol(\"parent\");\nsetWriteStructSlots(writeStruct, prepareStructures2);\nfunction writeStruct(object, target2, encodingStart, position3, structures, makeRoom, pack2, packr) {\n  let typedStructs = packr.typedStructs || (packr.typedStructs = []);\n  let targetView2 = target2.dataView;\n  let refsStartPosition = (typedStructs.lastStringStart || 100) + position3;\n  let safeEnd2 = target2.length - 10;\n  let start = position3;\n  if (position3 > safeEnd2) {\n    target2 = makeRoom(position3);\n    targetView2 = target2.dataView;\n    position3 -= encodingStart;\n    start -= encodingStart;\n    refsStartPosition -= encodingStart;\n    encodingStart = 0;\n    safeEnd2 = target2.length - 10;\n  }\n  let refOffset, refPosition = refsStartPosition;\n  let transition = typedStructs.transitions || (typedStructs.transitions = Object.create(null));\n  let nextId = typedStructs.nextId || typedStructs.length;\n  let headerSize = nextId < 15 ? 1 : nextId < 240 ? 2 : nextId < 61440 ? 3 : nextId < 15728640 ? 4 : 0;\n  if (headerSize === 0)\n    return 0;\n  position3 += headerSize;\n  let queuedReferences = [];\n  let usedAscii0;\n  let keyIndex = 0;\n  for (let key in object) {\n    let value = object[key];\n    let nextTransition = transition[key];\n    if (!nextTransition) {\n      transition[key] = nextTransition = {\n        key,\n        parent: transition,\n        enumerationOffset: 0,\n        ascii0: null,\n        ascii8: null,\n        num8: null,\n        string16: null,\n        object16: null,\n        num32: null,\n        float64: null,\n        date64: null\n      };\n    }\n    if (position3 > safeEnd2) {\n      target2 = makeRoom(position3);\n      targetView2 = target2.dataView;\n      position3 -= encodingStart;\n      start -= encodingStart;\n      refsStartPosition -= encodingStart;\n      refPosition -= encodingStart;\n      encodingStart = 0;\n      safeEnd2 = target2.length - 10;\n    }\n    switch (typeof value) {\n      case \"number\":\n        let number = value;\n        if (nextId < 200 || !nextTransition.num64) {\n          if (number >> 0 === number && number < 536870912 && number > -520093696) {\n            if (number < 246 && number >= 0 && (nextTransition.num8 && !(nextId > 200 && nextTransition.num32) || number < 32 && !nextTransition.num32)) {\n              transition = nextTransition.num8 || createTypeTransition(nextTransition, NUMBER, 1);\n              target2[position3++] = number;\n            } else {\n              transition = nextTransition.num32 || createTypeTransition(nextTransition, NUMBER, 4);\n              targetView2.setUint32(position3, number, true);\n              position3 += 4;\n            }\n            break;\n          } else if (number < 4294967296 && number >= -2147483648) {\n            targetView2.setFloat32(position3, number, true);\n            if (float32Headers[target2[position3 + 3] >>> 5]) {\n              let xShifted;\n              if ((xShifted = number * mult10[(target2[position3 + 3] & 127) << 1 | target2[position3 + 2] >> 7]) >> 0 === xShifted) {\n                transition = nextTransition.num32 || createTypeTransition(nextTransition, NUMBER, 4);\n                position3 += 4;\n                break;\n              }\n            }\n          }\n        }\n        transition = nextTransition.num64 || createTypeTransition(nextTransition, NUMBER, 8);\n        targetView2.setFloat64(position3, number, true);\n        position3 += 8;\n        break;\n      case \"string\":\n        let strLength = value.length;\n        refOffset = refPosition - refsStartPosition;\n        if ((strLength << 2) + refPosition > safeEnd2) {\n          target2 = makeRoom((strLength << 2) + refPosition);\n          targetView2 = target2.dataView;\n          position3 -= encodingStart;\n          start -= encodingStart;\n          refsStartPosition -= encodingStart;\n          refPosition -= encodingStart;\n          encodingStart = 0;\n          safeEnd2 = target2.length - 10;\n        }\n        if (strLength > 65280 + refOffset >> 2) {\n          queuedReferences.push(key, value, position3 - start);\n          break;\n        }\n        let isNotAscii;\n        let strStart = refPosition;\n        if (strLength < 64) {\n          let i, c1, c2;\n          for (i = 0;i < strLength; i++) {\n            c1 = value.charCodeAt(i);\n            if (c1 < 128) {\n              target2[refPosition++] = c1;\n            } else if (c1 < 2048) {\n              isNotAscii = true;\n              target2[refPosition++] = c1 >> 6 | 192;\n              target2[refPosition++] = c1 & 63 | 128;\n            } else if ((c1 & 64512) === 55296 && ((c2 = value.charCodeAt(i + 1)) & 64512) === 56320) {\n              isNotAscii = true;\n              c1 = 65536 + ((c1 & 1023) << 10) + (c2 & 1023);\n              i++;\n              target2[refPosition++] = c1 >> 18 | 240;\n              target2[refPosition++] = c1 >> 12 & 63 | 128;\n              target2[refPosition++] = c1 >> 6 & 63 | 128;\n              target2[refPosition++] = c1 & 63 | 128;\n            } else {\n              isNotAscii = true;\n              target2[refPosition++] = c1 >> 12 | 224;\n              target2[refPosition++] = c1 >> 6 & 63 | 128;\n              target2[refPosition++] = c1 & 63 | 128;\n            }\n          }\n        } else {\n          refPosition += encodeUtf8(target2, value, refPosition);\n          isNotAscii = refPosition - strStart > strLength;\n        }\n        if (refOffset < 160 || refOffset < 246 && (nextTransition.ascii8 || nextTransition.string8)) {\n          if (isNotAscii) {\n            if (!(transition = nextTransition.string8)) {\n              if (typedStructs.length > 10 && (transition = nextTransition.ascii8)) {\n                transition.__type = UTF8;\n                nextTransition.ascii8 = null;\n                nextTransition.string8 = transition;\n                pack2(null, 0, true);\n              } else {\n                transition = createTypeTransition(nextTransition, UTF8, 1);\n              }\n            }\n          } else if (refOffset === 0 && !usedAscii0) {\n            usedAscii0 = true;\n            transition = nextTransition.ascii0 || createTypeTransition(nextTransition, ASCII, 0);\n            break;\n          } else if (!(transition = nextTransition.ascii8) && !(typedStructs.length > 10 && (transition = nextTransition.string8)))\n            transition = createTypeTransition(nextTransition, ASCII, 1);\n          target2[position3++] = refOffset;\n        } else {\n          transition = nextTransition.string16 || createTypeTransition(nextTransition, UTF8, 2);\n          targetView2.setUint16(position3, refOffset, true);\n          position3 += 2;\n        }\n        break;\n      case \"object\":\n        if (value) {\n          if (value.constructor === Date) {\n            transition = nextTransition.date64 || createTypeTransition(nextTransition, DATE, 8);\n            targetView2.setFloat64(position3, value.getTime(), true);\n            position3 += 8;\n          } else {\n            queuedReferences.push(key, value, keyIndex);\n          }\n          break;\n        } else {\n          nextTransition = anyType(nextTransition, position3, targetView2, -10);\n          if (nextTransition) {\n            transition = nextTransition;\n            position3 = updatedPosition;\n          } else\n            queuedReferences.push(key, value, keyIndex);\n        }\n        break;\n      case \"boolean\":\n        transition = nextTransition.num8 || nextTransition.ascii8 || createTypeTransition(nextTransition, NUMBER, 1);\n        target2[position3++] = value ? 249 : 248;\n        break;\n      case \"undefined\":\n        nextTransition = anyType(nextTransition, position3, targetView2, -9);\n        if (nextTransition) {\n          transition = nextTransition;\n          position3 = updatedPosition;\n        } else\n          queuedReferences.push(key, value, keyIndex);\n        break;\n      default:\n        queuedReferences.push(key, value, keyIndex);\n    }\n    keyIndex++;\n  }\n  for (let i = 0, l = queuedReferences.length;i < l; ) {\n    let key = queuedReferences[i++];\n    let value = queuedReferences[i++];\n    let propertyIndex = queuedReferences[i++];\n    let nextTransition = transition[key];\n    if (!nextTransition) {\n      transition[key] = nextTransition = {\n        key,\n        parent: transition,\n        enumerationOffset: propertyIndex - keyIndex,\n        ascii0: null,\n        ascii8: null,\n        num8: null,\n        string16: null,\n        object16: null,\n        num32: null,\n        float64: null\n      };\n    }\n    let newPosition;\n    if (value) {\n      let size;\n      refOffset = refPosition - refsStartPosition;\n      if (refOffset < 65280) {\n        transition = nextTransition.object16;\n        if (transition)\n          size = 2;\n        else if (transition = nextTransition.object32)\n          size = 4;\n        else {\n          transition = createTypeTransition(nextTransition, OBJECT_DATA, 2);\n          size = 2;\n        }\n      } else {\n        transition = nextTransition.object32 || createTypeTransition(nextTransition, OBJECT_DATA, 4);\n        size = 4;\n      }\n      newPosition = pack2(value, refPosition);\n      if (typeof newPosition === \"object\") {\n        refPosition = newPosition.position;\n        targetView2 = newPosition.targetView;\n        target2 = newPosition.target;\n        refsStartPosition -= encodingStart;\n        position3 -= encodingStart;\n        start -= encodingStart;\n        encodingStart = 0;\n      } else\n        refPosition = newPosition;\n      if (size === 2) {\n        targetView2.setUint16(position3, refOffset, true);\n        position3 += 2;\n      } else {\n        targetView2.setUint32(position3, refOffset, true);\n        position3 += 4;\n      }\n    } else {\n      transition = nextTransition.object16 || createTypeTransition(nextTransition, OBJECT_DATA, 2);\n      targetView2.setInt16(position3, value === null ? -10 : -9, true);\n      position3 += 2;\n    }\n    keyIndex++;\n  }\n  let recordId = transition[RECORD_SYMBOL];\n  if (recordId == null) {\n    recordId = packr.typedStructs.length;\n    let structure = [];\n    let nextTransition = transition;\n    let key, type;\n    while ((type = nextTransition.__type) !== undefined) {\n      let size = nextTransition.__size;\n      nextTransition = nextTransition.__parent;\n      key = nextTransition.key;\n      let property = [type, size, key];\n      if (nextTransition.enumerationOffset)\n        property.push(nextTransition.enumerationOffset);\n      structure.push(property);\n      nextTransition = nextTransition.parent;\n    }\n    structure.reverse();\n    transition[RECORD_SYMBOL] = recordId;\n    packr.typedStructs[recordId] = structure;\n    pack2(null, 0, true);\n  }\n  switch (headerSize) {\n    case 1:\n      if (recordId >= 16)\n        return 0;\n      target2[start] = recordId + 32;\n      break;\n    case 2:\n      if (recordId >= 256)\n        return 0;\n      target2[start] = 56;\n      target2[start + 1] = recordId;\n      break;\n    case 3:\n      if (recordId >= 65536)\n        return 0;\n      target2[start] = 57;\n      targetView2.setUint16(start + 1, recordId, true);\n      break;\n    case 4:\n      if (recordId >= 16777216)\n        return 0;\n      targetView2.setUint32(start, (recordId << 8) + 58, true);\n      break;\n  }\n  if (position3 < refsStartPosition) {\n    if (refsStartPosition === refPosition)\n      return position3;\n    target2.copyWithin(position3, refsStartPosition, refPosition);\n    refPosition += position3 - refsStartPosition;\n    typedStructs.lastStringStart = position3 - start;\n  } else if (position3 > refsStartPosition) {\n    if (refsStartPosition === refPosition)\n      return position3;\n    typedStructs.lastStringStart = position3 - start;\n    return writeStruct(object, target2, encodingStart, start, structures, makeRoom, pack2, packr);\n  }\n  return refPosition;\n}\nfunction anyType(transition, position3, targetView2, value) {\n  let nextTransition;\n  if (nextTransition = transition.ascii8 || transition.num8) {\n    targetView2.setInt8(position3, value, true);\n    updatedPosition = position3 + 1;\n    return nextTransition;\n  }\n  if (nextTransition = transition.string16 || transition.object16) {\n    targetView2.setInt16(position3, value, true);\n    updatedPosition = position3 + 2;\n    return nextTransition;\n  }\n  if (nextTransition = transition.num32) {\n    targetView2.setUint32(position3, 3758096640 + value, true);\n    updatedPosition = position3 + 4;\n    return nextTransition;\n  }\n  if (nextTransition = transition.num64) {\n    targetView2.setFloat64(position3, NaN, true);\n    targetView2.setInt8(position3, value);\n    updatedPosition = position3 + 8;\n    return nextTransition;\n  }\n  updatedPosition = position3;\n  return;\n}\nfunction createTypeTransition(transition, type, size) {\n  let typeName = TYPE_NAMES[type] + (size << 3);\n  let newTransition = transition[typeName] || (transition[typeName] = Object.create(null));\n  newTransition.__type = type;\n  newTransition.__size = size;\n  newTransition.__parent = transition;\n  return newTransition;\n}\nfunction onLoadedStructures2(sharedData) {\n  if (!(sharedData instanceof Map))\n    return sharedData;\n  let typed = sharedData.get(\"typed\") || [];\n  if (Object.isFrozen(typed))\n    typed = typed.map((structure) => structure.slice(0));\n  let named = sharedData.get(\"named\");\n  let transitions = Object.create(null);\n  for (let i = 0, l = typed.length;i < l; i++) {\n    let structure = typed[i];\n    let transition = transitions;\n    for (let [type, size, key] of structure) {\n      let nextTransition = transition[key];\n      if (!nextTransition) {\n        transition[key] = nextTransition = {\n          key,\n          parent: transition,\n          enumerationOffset: 0,\n          ascii0: null,\n          ascii8: null,\n          num8: null,\n          string16: null,\n          object16: null,\n          num32: null,\n          float64: null,\n          date64: null\n        };\n      }\n      transition = createTypeTransition(nextTransition, type, size);\n    }\n    transition[RECORD_SYMBOL] = i;\n  }\n  typed.transitions = transitions;\n  this.typedStructs = typed;\n  this.lastTypedStructuresLength = typed.length;\n  return named;\n}\nvar sourceSymbol = Symbol.for(\"source\");\nfunction readStruct2(src2, position3, srcEnd2, unpackr) {\n  let recordId = src2[position3++] - 32;\n  if (recordId >= 24) {\n    switch (recordId) {\n      case 24:\n        recordId = src2[position3++];\n        break;\n      case 25:\n        recordId = src2[position3++] + (src2[position3++] << 8);\n        break;\n      case 26:\n        recordId = src2[position3++] + (src2[position3++] << 8) + (src2[position3++] << 16);\n        break;\n      case 27:\n        recordId = src2[position3++] + (src2[position3++] << 8) + (src2[position3++] << 16) + (src2[position3++] << 24);\n        break;\n    }\n  }\n  let structure = unpackr.typedStructs && unpackr.typedStructs[recordId];\n  if (!structure) {\n    src2 = Uint8Array.prototype.slice.call(src2, position3, srcEnd2);\n    srcEnd2 -= position3;\n    position3 = 0;\n    if (!unpackr.getStructures)\n      throw new Error(`Reference to shared structure ${recordId} without getStructures method`);\n    unpackr._mergeStructures(unpackr.getStructures());\n    if (!unpackr.typedStructs)\n      throw new Error(\"Could not find any shared typed structures\");\n    unpackr.lastTypedStructuresLength = unpackr.typedStructs.length;\n    structure = unpackr.typedStructs[recordId];\n    if (!structure)\n      throw new Error(\"Could not find typed structure \" + recordId);\n  }\n  var construct = structure.construct;\n  var fullConstruct = structure.fullConstruct;\n  if (!construct) {\n    construct = structure.construct = function LazyObject() {};\n    fullConstruct = structure.fullConstruct = function LoadedObject() {};\n    fullConstruct.prototype = unpackr.structPrototype || {};\n    var prototype = construct.prototype = unpackr.structPrototype ? Object.create(unpackr.structPrototype) : {};\n    let properties = [];\n    let currentOffset = 0;\n    let lastRefProperty;\n    for (let i = 0, l = structure.length;i < l; i++) {\n      let definition = structure[i];\n      let [type, size, key, enumerationOffset] = definition;\n      if (key === \"__proto__\")\n        key = \"__proto_\";\n      let property = {\n        key,\n        offset: currentOffset\n      };\n      if (enumerationOffset)\n        properties.splice(i + enumerationOffset, 0, property);\n      else\n        properties.push(property);\n      let getRef;\n      switch (size) {\n        case 0:\n          getRef = () => 0;\n          break;\n        case 1:\n          getRef = (source, position4) => {\n            let ref = source.bytes[position4 + property.offset];\n            return ref >= 246 ? toConstant(ref) : ref;\n          };\n          break;\n        case 2:\n          getRef = (source, position4) => {\n            let src3 = source.bytes;\n            let dataView2 = src3.dataView || (src3.dataView = new DataView(src3.buffer, src3.byteOffset, src3.byteLength));\n            let ref = dataView2.getUint16(position4 + property.offset, true);\n            return ref >= 65280 ? toConstant(ref & 255) : ref;\n          };\n          break;\n        case 4:\n          getRef = (source, position4) => {\n            let src3 = source.bytes;\n            let dataView2 = src3.dataView || (src3.dataView = new DataView(src3.buffer, src3.byteOffset, src3.byteLength));\n            let ref = dataView2.getUint32(position4 + property.offset, true);\n            return ref >= 4294967040 ? toConstant(ref & 255) : ref;\n          };\n          break;\n      }\n      property.getRef = getRef;\n      currentOffset += size;\n      let get;\n      switch (type) {\n        case ASCII:\n          if (lastRefProperty && !lastRefProperty.next)\n            lastRefProperty.next = property;\n          lastRefProperty = property;\n          property.multiGetCount = 0;\n          get = function(source) {\n            let src3 = source.bytes;\n            let position4 = source.position;\n            let refStart = currentOffset + position4;\n            let ref = getRef(source, position4);\n            if (typeof ref !== \"number\")\n              return ref;\n            let end, next = property.next;\n            while (next) {\n              end = next.getRef(source, position4);\n              if (typeof end === \"number\")\n                break;\n              else\n                end = null;\n              next = next.next;\n            }\n            if (end == null)\n              end = source.bytesEnd - refStart;\n            if (source.srcString) {\n              return source.srcString.slice(ref, end);\n            }\n            return readString(src3, ref + refStart, end - ref);\n          };\n          break;\n        case UTF8:\n        case OBJECT_DATA:\n          if (lastRefProperty && !lastRefProperty.next)\n            lastRefProperty.next = property;\n          lastRefProperty = property;\n          get = function(source) {\n            let position4 = source.position;\n            let refStart = currentOffset + position4;\n            let ref = getRef(source, position4);\n            if (typeof ref !== \"number\")\n              return ref;\n            let src3 = source.bytes;\n            let end, next = property.next;\n            while (next) {\n              end = next.getRef(source, position4);\n              if (typeof end === \"number\")\n                break;\n              else\n                end = null;\n              next = next.next;\n            }\n            if (end == null)\n              end = source.bytesEnd - refStart;\n            if (type === UTF8) {\n              return src3.toString(\"utf8\", ref + refStart, end + refStart);\n            } else {\n              currentSource = source;\n              try {\n                return unpackr.unpack(src3, { start: ref + refStart, end: end + refStart });\n              } finally {\n                currentSource = null;\n              }\n            }\n          };\n          break;\n        case NUMBER:\n          switch (size) {\n            case 4:\n              get = function(source) {\n                let src3 = source.bytes;\n                let dataView2 = src3.dataView || (src3.dataView = new DataView(src3.buffer, src3.byteOffset, src3.byteLength));\n                let position4 = source.position + property.offset;\n                let value = dataView2.getInt32(position4, true);\n                if (value < 536870912) {\n                  if (value > -520093696)\n                    return value;\n                  if (value > -536870912)\n                    return toConstant(value & 255);\n                }\n                let fValue = dataView2.getFloat32(position4, true);\n                let multiplier = mult10[(src3[position4 + 3] & 127) << 1 | src3[position4 + 2] >> 7];\n                return (multiplier * fValue + (fValue > 0 ? 0.5 : -0.5) >> 0) / multiplier;\n              };\n              break;\n            case 8:\n              get = function(source) {\n                let src3 = source.bytes;\n                let dataView2 = src3.dataView || (src3.dataView = new DataView(src3.buffer, src3.byteOffset, src3.byteLength));\n                let value = dataView2.getFloat64(source.position + property.offset, true);\n                if (isNaN(value)) {\n                  let byte = src3[source.position + property.offset];\n                  if (byte >= 246)\n                    return toConstant(byte);\n                }\n                return value;\n              };\n              break;\n            case 1:\n              get = function(source) {\n                let src3 = source.bytes;\n                let value = src3[source.position + property.offset];\n                return value < 246 ? value : toConstant(value);\n              };\n              break;\n          }\n          break;\n        case DATE:\n          get = function(source) {\n            let src3 = source.bytes;\n            let dataView2 = src3.dataView || (src3.dataView = new DataView(src3.buffer, src3.byteOffset, src3.byteLength));\n            return new Date(dataView2.getFloat64(source.position + property.offset, true));\n          };\n          break;\n      }\n      property.get = get;\n    }\n    if (evalSupported) {\n      let objectLiteralProperties = [];\n      let args = [];\n      let i = 0;\n      let hasInheritedProperties;\n      for (let property of properties) {\n        if (unpackr.alwaysLazyProperty && unpackr.alwaysLazyProperty(property.key)) {\n          hasInheritedProperties = true;\n          continue;\n        }\n        Object.defineProperty(prototype, property.key, { get: withSource(property.get), enumerable: true });\n        let valueFunction = \"v\" + i++;\n        args.push(valueFunction);\n        objectLiteralProperties.push(\"o[\" + JSON.stringify(property.key) + \"]=\" + valueFunction + \"(s)\");\n      }\n      if (hasInheritedProperties) {\n        objectLiteralProperties.push(\"__proto__:this\");\n      }\n      let toObject = new Function(...args, \"var c=this;return function(s){var o=new c();\" + objectLiteralProperties.join(\";\") + \";return o;}\").apply(fullConstruct, properties.map((prop) => prop.get));\n      Object.defineProperty(prototype, \"toJSON\", {\n        value(omitUnderscoredProperties) {\n          return toObject.call(this, this[sourceSymbol]);\n        }\n      });\n    } else {\n      Object.defineProperty(prototype, \"toJSON\", {\n        value(omitUnderscoredProperties) {\n          let resolved = {};\n          for (let i = 0, l = properties.length;i < l; i++) {\n            let key = properties[i].key;\n            resolved[key] = this[key];\n          }\n          return resolved;\n        }\n      });\n    }\n  }\n  var instance = new construct;\n  instance[sourceSymbol] = {\n    bytes: src2,\n    position: position3,\n    srcString: \"\",\n    bytesEnd: srcEnd2\n  };\n  return instance;\n}\nfunction toConstant(code) {\n  switch (code) {\n    case 246:\n      return null;\n    case 247:\n      return;\n    case 248:\n      return false;\n    case 249:\n      return true;\n  }\n  throw new Error(\"Unknown constant\");\n}\nfunction withSource(get) {\n  return function() {\n    return get(this[sourceSymbol]);\n  };\n}\nfunction saveState2() {\n  if (currentSource) {\n    currentSource.bytes = Uint8Array.prototype.slice.call(currentSource.bytes, currentSource.position, currentSource.bytesEnd);\n    currentSource.position = 0;\n    currentSource.bytesEnd = currentSource.bytes.length;\n  }\n}\nfunction prepareStructures2(structures, packr) {\n  if (packr.typedStructs) {\n    let structMap = new Map;\n    structMap.set(\"named\", structures);\n    structMap.set(\"typed\", packr.typedStructs);\n    structures = structMap;\n  }\n  let lastTypedStructuresLength = packr.lastTypedStructuresLength || 0;\n  structures.isCompatible = (existing) => {\n    let compatible = true;\n    if (existing instanceof Map) {\n      let named = existing.get(\"named\") || [];\n      if (named.length !== (packr.lastNamedStructuresLength || 0))\n        compatible = false;\n      let typed = existing.get(\"typed\") || [];\n      if (typed.length !== lastTypedStructuresLength)\n        compatible = false;\n    } else if (existing instanceof Array || Array.isArray(existing)) {\n      if (existing.length !== (packr.lastNamedStructuresLength || 0))\n        compatible = false;\n    }\n    if (!compatible)\n      packr._mergeStructures(existing);\n    return compatible;\n  };\n  packr.lastTypedStructuresLength = packr.typedStructs && packr.typedStructs.length;\n  return structures;\n}\nsetReadStruct(readStruct2, onLoadedStructures2, saveState2);\n// ../../node_modules/.bun/msgpackr@1.11.8/node_modules/msgpackr/node-index.js\nvar nativeAccelerationDisabled = process.env.MSGPACKR_NATIVE_ACCELERATION_DISABLED !== undefined && process.env.MSGPACKR_NATIVE_ACCELERATION_DISABLED.toLowerCase() === \"true\";\nif (!nativeAccelerationDisabled) {\n  let extractor;\n  try {\n    if (true)\n      extractor = require_msgpackr_extract();\n    else\n      ;\n    if (extractor)\n      setExtractor(extractor.extractStrings);\n  } catch (error) {}\n}\n\n// src/infrastructure/serialization/msgpack-serializer.ts\nclass MsgpackSerializer {\n  encode(value) {\n    return pack(value);\n  }\n  decode(data) {\n    return unpack(data);\n  }\n}\n\n// src/infrastructure/serialization/binary-event-batch-serializer.ts\nvar BATCH_MAGIC2 = 1397773378;\nvar BATCH_VERSION = 1;\nvar BATCH_HEADER_SIZE3 = 12;\nvar NULL_LENGTH = 4294967295;\nvar textEncoder3 = new TextEncoder;\nvar textDecoder = new TextDecoder;\nfunction isStoredEvent(value) {\n  if (!value || typeof value !== \"object\")\n    return false;\n  const record = value;\n  return typeof record[\"streamId\"] === \"string\" && typeof record[\"type\"] === \"string\" && typeof record[\"revision\"] === \"number\" && typeof record[\"globalPosition\"] === \"number\" && typeof record[\"timestamp\"] === \"number\";\n}\nfunction ensureSafeUint(value, label) {\n  if (!Number.isSafeInteger(value) || value < 0) {\n    throw new Error(`${label} must be a non-negative safe integer`);\n  }\n}\nfunction encodeEventBatch(events, packr) {\n  const encoded = events.map((event) => {\n    const streamId = textEncoder3.encode(event.streamId);\n    const type = textEncoder3.encode(event.type);\n    const tenantId = event.tenantId !== undefined ? textEncoder3.encode(event.tenantId) : null;\n    const metadata = event.metadata !== undefined ? packr.pack(event.metadata) : null;\n    const data = packr.pack(event.data);\n    ensureSafeUint(event.revision, \"Revision\");\n    ensureSafeUint(event.globalPosition, \"Global position\");\n    ensureSafeUint(event.timestamp, \"Timestamp\");\n    return {\n      event,\n      streamId,\n      type,\n      tenantId,\n      metadata,\n      data\n    };\n  });\n  let totalSize = BATCH_HEADER_SIZE3;\n  for (const entry of encoded) {\n    totalSize += 4 + entry.streamId.length;\n    totalSize += 4 + entry.type.length;\n    totalSize += 4;\n    totalSize += 8;\n    totalSize += 8;\n    totalSize += 4 + (entry.tenantId ? entry.tenantId.length : 0);\n    totalSize += 4 + (entry.metadata ? entry.metadata.length : 0);\n    totalSize += 4 + entry.data.length;\n  }\n  const buffer = new Uint8Array(totalSize);\n  const view = new DataView(buffer.buffer, buffer.byteOffset, buffer.byteLength);\n  view.setUint32(0, BATCH_MAGIC2, false);\n  view.setUint16(4, BATCH_VERSION, false);\n  view.setUint16(6, 0, false);\n  view.setUint32(8, events.length, false);\n  let offset = BATCH_HEADER_SIZE3;\n  for (const entry of encoded) {\n    view.setUint32(offset, entry.streamId.length, false);\n    offset += 4;\n    buffer.set(entry.streamId, offset);\n    offset += entry.streamId.length;\n    view.setUint32(offset, entry.type.length, false);\n    offset += 4;\n    buffer.set(entry.type, offset);\n    offset += entry.type.length;\n    view.setUint32(offset, entry.event.revision, false);\n    offset += 4;\n    view.setBigUint64(offset, BigInt(entry.event.globalPosition), false);\n    offset += 8;\n    view.setBigUint64(offset, BigInt(entry.event.timestamp), false);\n    offset += 8;\n    if (entry.tenantId) {\n      view.setUint32(offset, entry.tenantId.length, false);\n      offset += 4;\n      buffer.set(entry.tenantId, offset);\n      offset += entry.tenantId.length;\n    } else {\n      view.setUint32(offset, NULL_LENGTH, false);\n      offset += 4;\n    }\n    if (entry.metadata) {\n      view.setUint32(offset, entry.metadata.length, false);\n      offset += 4;\n      buffer.set(entry.metadata, offset);\n      offset += entry.metadata.length;\n    } else {\n      view.setUint32(offset, NULL_LENGTH, false);\n      offset += 4;\n    }\n    view.setUint32(offset, entry.data.length, false);\n    offset += 4;\n    buffer.set(entry.data, offset);\n    offset += entry.data.length;\n  }\n  return buffer;\n}\nfunction decodeEventBatch(data, unpackr) {\n  if (data.length < BATCH_HEADER_SIZE3) {\n    throw new Error(\"Batch payload is too small\");\n  }\n  const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n  const magic = view.getUint32(0, false);\n  if (magic !== BATCH_MAGIC2) {\n    throw new Error(\"Invalid batch payload magic\");\n  }\n  const version = view.getUint16(4, false);\n  if (version !== BATCH_VERSION) {\n    throw new Error(`Unsupported batch payload version: ${version}`);\n  }\n  const eventCount = view.getUint32(8, false);\n  let offset = BATCH_HEADER_SIZE3;\n  const events = [];\n  for (let i = 0;i < eventCount; i += 1) {\n    if (offset + 4 > data.length) {\n      throw new Error(\"Batch payload truncated (streamId length)\");\n    }\n    const streamIdLength = view.getUint32(offset, false);\n    offset += 4;\n    const streamIdEnd = offset + streamIdLength;\n    if (streamIdEnd > data.length) {\n      throw new Error(\"Batch payload truncated (streamId)\");\n    }\n    const streamId = textDecoder.decode(data.subarray(offset, streamIdEnd));\n    offset = streamIdEnd;\n    if (offset + 4 > data.length) {\n      throw new Error(\"Batch payload truncated (type length)\");\n    }\n    const typeLength = view.getUint32(offset, false);\n    offset += 4;\n    const typeEnd = offset + typeLength;\n    if (typeEnd > data.length) {\n      throw new Error(\"Batch payload truncated (type)\");\n    }\n    const type = textDecoder.decode(data.subarray(offset, typeEnd));\n    offset = typeEnd;\n    if (offset + 20 > data.length) {\n      throw new Error(\"Batch payload truncated (position fields)\");\n    }\n    const revision = view.getUint32(offset, false);\n    offset += 4;\n    const globalPosition = Number(view.getBigUint64(offset, false));\n    offset += 8;\n    const timestamp = Number(view.getBigUint64(offset, false));\n    offset += 8;\n    if (!Number.isSafeInteger(globalPosition) || !Number.isSafeInteger(timestamp)) {\n      throw new Error(\"Batch payload contains unsafe integer positions\");\n    }\n    if (offset + 4 > data.length) {\n      throw new Error(\"Batch payload truncated (tenant length)\");\n    }\n    const tenantLength = view.getUint32(offset, false);\n    offset += 4;\n    let tenantId;\n    if (tenantLength !== NULL_LENGTH) {\n      const tenantEnd = offset + tenantLength;\n      if (tenantEnd > data.length) {\n        throw new Error(\"Batch payload truncated (tenant id)\");\n      }\n      tenantId = textDecoder.decode(data.subarray(offset, tenantEnd));\n      offset = tenantEnd;\n    }\n    if (offset + 4 > data.length) {\n      throw new Error(\"Batch payload truncated (metadata length)\");\n    }\n    const metadataLength = view.getUint32(offset, false);\n    offset += 4;\n    let metadata;\n    if (metadataLength !== NULL_LENGTH) {\n      const metadataEnd = offset + metadataLength;\n      if (metadataEnd > data.length) {\n        throw new Error(\"Batch payload truncated (metadata)\");\n      }\n      metadata = unpackr.unpack(data.subarray(offset, metadataEnd));\n      offset = metadataEnd;\n    }\n    if (offset + 4 > data.length) {\n      throw new Error(\"Batch payload truncated (data length)\");\n    }\n    const dataLength = view.getUint32(offset, false);\n    offset += 4;\n    const dataEnd = offset + dataLength;\n    if (dataEnd > data.length) {\n      throw new Error(\"Batch payload truncated (data)\");\n    }\n    const payload = unpackr.unpack(data.subarray(offset, dataEnd));\n    offset = dataEnd;\n    events.push({\n      streamId,\n      type,\n      data: payload,\n      metadata,\n      revision,\n      globalPosition,\n      timestamp,\n      tenantId: tenantId ?? \"default\"\n    });\n  }\n  return events;\n}\n\nclass BinaryEventBatchSerializer {\n  packr = new Packr({ useRecords: true });\n  unpackr = new Unpackr({ useRecords: true });\n  encode(value) {\n    if (!Array.isArray(value)) {\n      throw new Error(\"BinaryEventBatchSerializer only supports event batches\");\n    }\n    if (value.length > 0 && !isStoredEvent(value[0])) {\n      throw new Error(\"BinaryEventBatchSerializer expects StoredEvent[]\");\n    }\n    return encodeEventBatch(value, this.packr);\n  }\n  decode(data) {\n    if (data.length < BATCH_HEADER_SIZE3) {\n      throw new Error(\"Batch payload is too small\");\n    }\n    const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n    if (view.getUint32(0, false) !== BATCH_MAGIC2) {\n      throw new Error(\"Invalid batch payload magic\");\n    }\n    return decodeEventBatch(data, this.unpackr);\n  }\n}\n\n// src/infrastructure/serialization/noop-compressor.ts\nclass NoopCompressor {\n  compress(data) {\n    return data;\n  }\n  decompress(data) {\n    return data;\n  }\n}\n\n// src/spitedb.ts\nclass SpiteDB {\n  eventStore;\n  coordinator;\n  dataDir;\n  projectionsStarted = false;\n  backpressure;\n  constructor(eventStore, coordinator, dataDir, backpressure) {\n    this.eventStore = eventStore;\n    this.coordinator = coordinator;\n    this.dataDir = dataDir;\n    this.backpressure = backpressure;\n  }\n  static async open(path, options = {}) {\n    const fs = new BunFileSystem;\n    const clock = new BunClock;\n    const eventSerializer = options.eventSerializer ?? new BinaryEventBatchSerializer;\n    const projectionSerializer = options.projectionSerializer ?? new MsgpackSerializer;\n    const compressor = options.compressor ?? new NoopCompressor;\n    const eventStoreConfig = {\n      fs,\n      clock,\n      serializer: eventSerializer,\n      compressor\n    };\n    if (options.maxSegmentSize !== undefined) {\n      eventStoreConfig.maxSegmentSize = options.maxSegmentSize;\n    }\n    if (options.indexCacheSize !== undefined) {\n      eventStoreConfig.indexCacheSize = options.indexCacheSize;\n    }\n    if (options.autoFlushCount !== undefined) {\n      eventStoreConfig.autoFlushCount = options.autoFlushCount;\n    }\n    if (options.readProfiler !== undefined) {\n      eventStoreConfig.readProfiler = options.readProfiler;\n    }\n    const eventStore = new EventStore(eventStoreConfig);\n    const eventsDir = `${path}/events`;\n    await eventStore.open(eventsDir);\n    const coordinatorConfig = {\n      eventStore,\n      fs,\n      serializer: projectionSerializer,\n      clock,\n      dataDir: `${path}/projections`\n    };\n    if (options.projectionPollingIntervalMs !== undefined) {\n      coordinatorConfig.pollingIntervalMs = options.projectionPollingIntervalMs;\n    }\n    if (options.projectionCheckpointIntervalMs !== undefined) {\n      coordinatorConfig.defaultCheckpointIntervalMs = options.projectionCheckpointIntervalMs;\n    }\n    if (options.projectionBatchSize !== undefined) {\n      coordinatorConfig.defaultBatchSize = options.projectionBatchSize;\n    }\n    if (options.projectionSharedReader !== undefined) {\n      coordinatorConfig.sharedReader = options.projectionSharedReader;\n    }\n    const coordinator = new ProjectionCoordinator(coordinatorConfig);\n    const backpressure = resolveBackpressure(options.projectionBackpressure);\n    return new SpiteDB(eventStore, coordinator, path, backpressure);\n  }\n  async close() {\n    if (!this.eventStore.isOpen()) {\n      return;\n    }\n    await this.stopProjections();\n    await this.eventStore.close();\n  }\n  isOpen() {\n    return this.eventStore.isOpen();\n  }\n  getDataDir() {\n    return this.dataDir;\n  }\n  async append(streamId, events, options) {\n    this.ensureOpen();\n    await this.applyProjectionBackpressure();\n    return this.eventStore.append(streamId, events, options);\n  }\n  async appendBatch(operations) {\n    this.ensureOpen();\n    await this.applyProjectionBackpressure();\n    return this.eventStore.appendBatch(operations);\n  }\n  async readStream(streamId, options) {\n    this.ensureOpen();\n    return this.eventStore.readStream(streamId, options);\n  }\n  async readGlobal(fromPosition = 0, options) {\n    this.ensureOpen();\n    return this.eventStore.readGlobal(fromPosition, options);\n  }\n  async* streamGlobal(fromPosition = 0) {\n    this.ensureOpen();\n    yield* this.eventStore.streamGlobal(fromPosition);\n  }\n  getStreamRevision(streamId) {\n    this.ensureOpen();\n    return this.eventStore.getStreamRevision(streamId);\n  }\n  hasStream(streamId) {\n    this.ensureOpen();\n    return this.eventStore.hasStream(streamId);\n  }\n  async getStreamIds() {\n    this.ensureOpen();\n    return this.eventStore.getStreamIds();\n  }\n  getGlobalPosition() {\n    this.ensureOpen();\n    return this.eventStore.getGlobalPosition();\n  }\n  async flush() {\n    this.ensureOpen();\n    await this.eventStore.flush();\n  }\n  registerProjection(registration, options) {\n    this.ensureOpen();\n    this.coordinator.getRegistry().register(registration, options);\n  }\n  async startProjections() {\n    this.ensureOpen();\n    if (this.projectionsStarted) {\n      return;\n    }\n    await this.coordinator.start();\n    this.projectionsStarted = true;\n  }\n  async stopProjections() {\n    if (!this.projectionsStarted) {\n      return;\n    }\n    await this.coordinator.stop();\n    this.projectionsStarted = false;\n  }\n  getProjection(name) {\n    this.ensureOpen();\n    return this.coordinator.getProjection(name);\n  }\n  requireProjection(name) {\n    this.ensureOpen();\n    if (!this.projectionsStarted) {\n      throw new ProjectionsNotStartedError;\n    }\n    return this.coordinator.requireProjection(name);\n  }\n  async waitForProjections(timeoutMs = 30000) {\n    this.ensureOpen();\n    if (!this.projectionsStarted) {\n      throw new ProjectionsNotStartedError;\n    }\n    await this.coordinator.waitForCatchUp(timeoutMs);\n  }\n  async forceProjectionCheckpoint() {\n    this.ensureOpen();\n    if (!this.projectionsStarted) {\n      throw new ProjectionsNotStartedError;\n    }\n    await this.coordinator.forceCheckpoint();\n  }\n  getProjectionStatus() {\n    this.ensureOpen();\n    return this.coordinator.getStatus();\n  }\n  projectionsRunning() {\n    return this.projectionsStarted;\n  }\n  ensureOpen() {\n    if (!this.eventStore.isOpen()) {\n      throw new SpiteDBNotOpenError;\n    }\n  }\n  async applyProjectionBackpressure() {\n    if (!this.backpressure || !this.projectionsStarted) {\n      return;\n    }\n    const {\n      maxLag,\n      maxWaitMs,\n      pollIntervalMs,\n      mode\n    } = this.backpressure;\n    const start = Date.now();\n    while (true) {\n      const status = this.coordinator.getStatus();\n      if (status.projections.length === 0) {\n        return;\n      }\n      let slowest = status.projections[0];\n      for (const projection of status.projections) {\n        if (projection.currentPosition < slowest.currentPosition) {\n          slowest = projection;\n        }\n      }\n      const current = slowest.currentPosition < 0 ? 0 : slowest.currentPosition;\n      const lag = status.globalPosition > current ? status.globalPosition - current : 0;\n      if (lag <= maxLag) {\n        return;\n      }\n      if (mode === \"fail\") {\n        throw new ProjectionBackpressureError(slowest.name, lag, maxLag);\n      }\n      const waitedMs = Date.now() - start;\n      if (waitedMs >= maxWaitMs) {\n        throw new ProjectionBackpressureTimeoutError(slowest.name, lag, maxLag, waitedMs, maxWaitMs);\n      }\n      await new Promise((resolve) => {\n        setTimeout(resolve, pollIntervalMs);\n      });\n    }\n  }\n}\nfunction resolveBackpressure(options) {\n  if (options === false) {\n    return;\n  }\n  return {\n    maxLag: options?.maxLag ?? 200000,\n    maxWaitMs: options?.maxWaitMs ?? 5000,\n    pollIntervalMs: options?.pollIntervalMs ?? 25,\n    mode: options?.mode ?? \"block\"\n  };\n}\n// src/domain/value-objects/stream-id.ts\nvar MAX_STREAM_ID_LENGTH = 256;\nvar STREAM_ID_PATTERN = /^[a-zA-Z0-9_\\-:.]+$/;\n\nclass StreamId {\n  value;\n  constructor(value) {\n    this.value = value;\n  }\n  static from(value) {\n    if (!value) {\n      throw new InvalidStreamIdError(\"StreamId cannot be empty\");\n    }\n    if (value.length > MAX_STREAM_ID_LENGTH) {\n      throw new InvalidStreamIdError(`StreamId cannot exceed ${MAX_STREAM_ID_LENGTH} characters`);\n    }\n    if (!STREAM_ID_PATTERN.test(value)) {\n      throw new InvalidStreamIdError(\"StreamId can only contain alphanumeric characters, underscores, hyphens, colons, and dots\");\n    }\n    return new StreamId(value);\n  }\n  toString() {\n    return this.value;\n  }\n  equals(other) {\n    return this.value === other.value;\n  }\n  hash() {\n    let hash = 2166136261;\n    for (let i = 0;i < this.value.length; i++) {\n      hash ^= this.value.charCodeAt(i);\n      hash = hash * 16777619 >>> 0;\n    }\n    return hash;\n  }\n}\n// src/domain/value-objects/global-position.ts\nclass GlobalPosition {\n  value;\n  constructor(value) {\n    this.value = value;\n  }\n  static from(value) {\n    if (!Number.isSafeInteger(value)) {\n      throw new InvalidPositionError(\"GlobalPosition must be a safe integer\");\n    }\n    if (value < 0) {\n      throw new InvalidPositionError(\"GlobalPosition cannot be negative\");\n    }\n    return new GlobalPosition(value);\n  }\n  static BEGINNING = GlobalPosition.from(0);\n  toNumber() {\n    return this.value;\n  }\n  next() {\n    return new GlobalPosition(this.value + 1);\n  }\n  advance(count) {\n    return new GlobalPosition(this.value + count);\n  }\n  equals(other) {\n    return this.value === other.value;\n  }\n  compareTo(other) {\n    if (this.value < other.value)\n      return -1;\n    if (this.value > other.value)\n      return 1;\n    return 0;\n  }\n  isAfter(other) {\n    return this.value > other.value;\n  }\n  isBefore(other) {\n    return this.value < other.value;\n  }\n}\n// src/domain/value-objects/revision.ts\nclass Revision {\n  value;\n  constructor(value) {\n    this.value = value;\n  }\n  static from(value) {\n    if (!Number.isInteger(value)) {\n      throw new Error(\"Revision must be an integer\");\n    }\n    return new Revision(value);\n  }\n  static NONE = new Revision(-1);\n  static ANY = new Revision(-2);\n  toNumber() {\n    return this.value;\n  }\n  next() {\n    return new Revision(this.value + 1);\n  }\n  isNone() {\n    return this.value === -1;\n  }\n  isAny() {\n    return this.value === -2;\n  }\n  equals(other) {\n    return this.value === other.value;\n  }\n  compareTo(other) {\n    if (this.value < other.value)\n      return -1;\n    if (this.value > other.value)\n      return 1;\n    return 0;\n  }\n}\n// src/domain/value-objects/tenant-id.ts\nvar MAX_TENANT_ID_LENGTH = 128;\nvar TENANT_ID_PATTERN = /^[a-zA-Z0-9_\\-]+$/;\n\nclass TenantId {\n  value;\n  constructor(value) {\n    this.value = value;\n  }\n  static from(value) {\n    if (!value) {\n      throw new Error(\"TenantId cannot be empty\");\n    }\n    if (value.length > MAX_TENANT_ID_LENGTH) {\n      throw new Error(`TenantId cannot exceed ${MAX_TENANT_ID_LENGTH} characters`);\n    }\n    if (!TENANT_ID_PATTERN.test(value)) {\n      throw new Error(\"TenantId can only contain alphanumeric characters, underscores, and hyphens\");\n    }\n    return new TenantId(value);\n  }\n  static DEFAULT = new TenantId(\"default\");\n  toString() {\n    return this.value;\n  }\n  equals(other) {\n    return this.value === other.value;\n  }\n  isDefault() {\n    return this.value === \"default\";\n  }\n}\n// src/domain/value-objects/batch-id.ts\nclass BatchId {\n  value;\n  constructor(value) {\n    this.value = value;\n  }\n  static from(value) {\n    return new BatchId(BigInt(value));\n  }\n  static generate() {\n    const timestamp = BigInt(Date.now()) << 20n;\n    const random = BigInt(Math.floor(Math.random() * 1048575));\n    return new BatchId(timestamp | random);\n  }\n  toBigInt() {\n    return this.value;\n  }\n  toString() {\n    return this.value.toString(16);\n  }\n  equals(other) {\n    return this.value === other.value;\n  }\n}\n// src/domain/value-objects/command-id.ts\nclass CommandId {\n  value;\n  constructor(value) {\n    this.value = value;\n  }\n  static from(value) {\n    if (!value) {\n      throw new Error(\"CommandId cannot be empty\");\n    }\n    if (value.length > 256) {\n      throw new Error(\"CommandId cannot exceed 256 characters\");\n    }\n    return new CommandId(value);\n  }\n  static generate() {\n    return new CommandId(crypto.randomUUID());\n  }\n  toString() {\n    return this.value;\n  }\n  equals(other) {\n    return this.value === other.value;\n  }\n}\n// src/infrastructure/serialization/fast-event-serializer.ts\nvar BATCH_MARKER = \"__spite_batch\";\nvar BATCH_VERSION2 = 1;\nfunction isStoredEvent2(value) {\n  if (!value || typeof value !== \"object\")\n    return false;\n  const record = value;\n  return typeof record[\"streamId\"] === \"string\" && typeof record[\"type\"] === \"string\" && typeof record[\"revision\"] === \"number\" && typeof record[\"globalPosition\"] === \"number\" && typeof record[\"timestamp\"] === \"number\";\n}\nfunction toColumnarBatch(events) {\n  const len = events.length;\n  const streamIds = new Array(len);\n  const types = new Array(len);\n  const data = new Array(len);\n  const metadata = new Array(len);\n  const revisions = new Array(len);\n  const globalPositions = new Array(len);\n  const timestamps = new Array(len);\n  const tenantIds = new Array(len);\n  for (let i = 0;i < len; i++) {\n    const event = events[i];\n    streamIds[i] = event.streamId;\n    types[i] = event.type;\n    data[i] = event.data;\n    metadata[i] = event.metadata;\n    revisions[i] = event.revision;\n    globalPositions[i] = event.globalPosition;\n    timestamps[i] = event.timestamp;\n    tenantIds[i] = event.tenantId;\n  }\n  return {\n    [BATCH_MARKER]: BATCH_VERSION2,\n    streamIds,\n    types,\n    data,\n    metadata,\n    revisions,\n    globalPositions,\n    timestamps,\n    tenantIds\n  };\n}\nfunction fromColumnarBatch(batch) {\n  const len = batch.streamIds.length;\n  const events = new Array(len);\n  for (let i = 0;i < len; i++) {\n    events[i] = {\n      streamId: batch.streamIds[i] ?? \"\",\n      type: batch.types[i] ?? \"\",\n      data: batch.data[i],\n      metadata: batch.metadata[i],\n      revision: batch.revisions[i] ?? 0,\n      globalPosition: batch.globalPositions[i] ?? 0,\n      timestamp: batch.timestamps[i] ?? 0,\n      tenantId: batch.tenantIds[i] ?? \"default\"\n    };\n  }\n  return events;\n}\n\nclass FastEventSerializer {\n  packr = new Packr({ useRecords: true });\n  unpackr = new Unpackr({ useRecords: true });\n  encode(value) {\n    if (Array.isArray(value) && value.length > 0 && isStoredEvent2(value[0])) {\n      return this.packr.pack(toColumnarBatch(value));\n    }\n    return this.packr.pack(value);\n  }\n  decode(data) {\n    const decoded = this.unpackr.unpack(data);\n    if (decoded && typeof decoded === \"object\" && decoded[BATCH_MARKER] === BATCH_VERSION2) {\n      return fromColumnarBatch(decoded);\n    }\n    return decoded;\n  }\n}\n// src/infrastructure/serialization/zstd-compressor.ts\nclass ZstdCompressor {\n  static DEFAULT_LEVEL = 3;\n  compress(data, level = ZstdCompressor.DEFAULT_LEVEL) {\n    return Bun.zstdCompressSync(data, { level });\n  }\n  decompress(data) {\n    return Bun.zstdDecompressSync(data);\n  }\n}\n// src/infrastructure/projections/stores/disk-kv-store.ts\nvar TOMBSTONE_MARKER = 4294967295;\n\nclass DiskKeyValueStore {\n  fs;\n  serializer;\n  path;\n  index = new Map;\n  writePosition = 0;\n  liveBytes = 0;\n  totalBytes = 0;\n  writeHandle = null;\n  opened = false;\n  constructor(config) {\n    this.fs = config.fs;\n    this.serializer = config.serializer;\n    this.path = config.path;\n  }\n  async open() {\n    if (this.opened) {\n      return;\n    }\n    const dir = this.path.substring(0, this.path.lastIndexOf(\"/\"));\n    if (dir && !await this.fs.exists(dir)) {\n      await this.fs.mkdir(dir, { recursive: true });\n    }\n    const exists = await this.fs.exists(this.path);\n    if (exists) {\n      await this.rebuildIndex();\n    }\n    this.writeHandle = await this.fs.open(this.path, exists ? \"append\" : \"write\");\n    this.opened = true;\n  }\n  async close() {\n    if (!this.opened) {\n      return;\n    }\n    if (this.writeHandle) {\n      await this.fs.sync(this.writeHandle);\n      await this.fs.close(this.writeHandle);\n      this.writeHandle = null;\n    }\n    this.opened = false;\n  }\n  async get(key) {\n    const entry = this.index.get(key);\n    if (!entry) {\n      return;\n    }\n    const record = await this.fs.readFileSlice(this.path, entry.offset, entry.offset + entry.length);\n    const parsed = this.parseRecord(record);\n    if (!parsed || parsed.isDeleted) {\n      return;\n    }\n    return parsed.value;\n  }\n  async set(key, value) {\n    this.ensureOpen();\n    const valueData = this.serializer.encode(value);\n    const record = this.buildRecord(key, valueData);\n    const existingEntry = this.index.get(key);\n    if (existingEntry) {\n      this.liveBytes -= existingEntry.length;\n    }\n    const offset = this.writePosition;\n    await writeAllBytes(this.fs, this.writeHandle, record);\n    this.writePosition += record.length;\n    const newEntry = { offset, length: record.length };\n    this.index.set(key, newEntry);\n    this.liveBytes += record.length;\n    this.totalBytes = this.writePosition;\n  }\n  async delete(key) {\n    const existingEntry = this.index.get(key);\n    if (!existingEntry) {\n      return false;\n    }\n    this.ensureOpen();\n    const record = this.buildTombstone(key);\n    await writeAllBytes(this.fs, this.writeHandle, record);\n    this.writePosition += record.length;\n    this.liveBytes -= existingEntry.length;\n    this.totalBytes = this.writePosition;\n    this.index.delete(key);\n    return true;\n  }\n  has(key) {\n    return this.index.has(key);\n  }\n  keys() {\n    return this.index.keys();\n  }\n  get size() {\n    return this.index.size;\n  }\n  async sync() {\n    if (this.writeHandle) {\n      await this.fs.sync(this.writeHandle);\n    }\n  }\n  getGarbageRatio() {\n    if (this.totalBytes === 0) {\n      return 0;\n    }\n    return 1 - this.liveBytes / this.totalBytes;\n  }\n  getTotalBytes() {\n    return this.totalBytes;\n  }\n  getLiveBytes() {\n    return this.liveBytes;\n  }\n  async compact() {\n    this.ensureOpen();\n    const tempPath = this.path + \".compact\";\n    try {\n      if (this.writeHandle) {\n        await this.fs.sync(this.writeHandle);\n        await this.fs.close(this.writeHandle);\n        this.writeHandle = null;\n      }\n      const tempHandle = await this.fs.open(tempPath, \"write\");\n      const newIndex = new Map;\n      let newPosition = 0;\n      for (const [key, entry] of this.index) {\n        const record = await this.fs.readFileSlice(this.path, entry.offset, entry.offset + entry.length);\n        await writeAllBytes(this.fs, tempHandle, record);\n        newIndex.set(key, { offset: newPosition, length: record.length });\n        newPosition += record.length;\n      }\n      await this.fs.sync(tempHandle);\n      await this.fs.close(tempHandle);\n      await this.fs.rename(tempPath, this.path);\n      this.index = newIndex;\n      this.writePosition = newPosition;\n      this.liveBytes = newPosition;\n      this.totalBytes = newPosition;\n      this.writeHandle = await this.fs.open(this.path, \"append\");\n    } catch (error) {\n      try {\n        if (await this.fs.exists(tempPath)) {\n          await this.fs.unlink(tempPath);\n        }\n      } catch {}\n      this.writeHandle = await this.fs.open(this.path, \"append\");\n      throw error;\n    }\n  }\n  async rebuildIndex() {\n    const data = await this.fs.readFile(this.path);\n    let offset = 0;\n    let corruptionDetected = false;\n    this.index.clear();\n    this.liveBytes = 0;\n    while (offset < data.length) {\n      if (offset + 8 > data.length) {\n        corruptionDetected = true;\n        break;\n      }\n      const view = new DataView(data.buffer, data.byteOffset + offset, data.length - offset);\n      const keyLen = view.getUint32(0, true);\n      if (keyLen === 0 || keyLen > 65536) {\n        corruptionDetected = true;\n        break;\n      }\n      if (offset + 4 + keyLen + 4 > data.length) {\n        corruptionDetected = true;\n        break;\n      }\n      const valueLen = view.getUint32(4 + keyLen, true);\n      const isDeleted = valueLen === TOMBSTONE_MARKER;\n      const actualValueLen = isDeleted ? 0 : valueLen;\n      const recordLen = 4 + keyLen + 4 + actualValueLen + 4;\n      if (offset + recordLen > data.length) {\n        corruptionDetected = true;\n        break;\n      }\n      const recordData = data.subarray(offset, offset + recordLen - 4);\n      const storedCrc = view.getUint32(recordLen - 4, true);\n      const calculatedCrc = crc32(recordData);\n      if (calculatedCrc !== storedCrc) {\n        corruptionDetected = true;\n        break;\n      }\n      const keyBytes = data.subarray(offset + 4, offset + 4 + keyLen);\n      const key = new TextDecoder().decode(keyBytes);\n      if (isDeleted) {\n        const existing = this.index.get(key);\n        if (existing) {\n          this.liveBytes -= existing.length;\n        }\n        this.index.delete(key);\n      } else {\n        const existing = this.index.get(key);\n        if (existing) {\n          this.liveBytes -= existing.length;\n        }\n        this.index.set(key, { offset, length: recordLen });\n        this.liveBytes += recordLen;\n      }\n      offset += recordLen;\n    }\n    if (corruptionDetected && offset < data.length) {\n      console.warn(`[DiskKeyValueStore] Corruption detected at offset ${offset}, ` + `truncating file from ${data.length} to ${offset} bytes`);\n      const handle = await this.fs.open(this.path, \"readwrite\");\n      try {\n        await this.fs.truncate(handle, offset);\n        await this.fs.sync(handle);\n      } finally {\n        await this.fs.close(handle);\n      }\n    }\n    this.writePosition = offset;\n    this.totalBytes = offset;\n  }\n  buildRecord(key, valueData) {\n    const encoder = new TextEncoder;\n    const keyData = encoder.encode(key);\n    const recordLen = 4 + keyData.length + 4 + valueData.length + 4;\n    const buffer = new Uint8Array(recordLen);\n    const view = new DataView(buffer.buffer);\n    view.setUint32(0, keyData.length, true);\n    buffer.set(keyData, 4);\n    view.setUint32(4 + keyData.length, valueData.length, true);\n    buffer.set(valueData, 4 + keyData.length + 4);\n    const dataForCrc = buffer.subarray(0, recordLen - 4);\n    const checksum = crc32(dataForCrc);\n    view.setUint32(recordLen - 4, checksum, true);\n    return buffer;\n  }\n  buildTombstone(key) {\n    const encoder = new TextEncoder;\n    const keyData = encoder.encode(key);\n    const recordLen = 4 + keyData.length + 4 + 4;\n    const buffer = new Uint8Array(recordLen);\n    const view = new DataView(buffer.buffer);\n    view.setUint32(0, keyData.length, true);\n    buffer.set(keyData, 4);\n    view.setUint32(4 + keyData.length, TOMBSTONE_MARKER, true);\n    const dataForCrc = buffer.subarray(0, recordLen - 4);\n    const checksum = crc32(dataForCrc);\n    view.setUint32(recordLen - 4, checksum, true);\n    return buffer;\n  }\n  parseRecord(data) {\n    if (data.length < 8) {\n      return null;\n    }\n    const view = new DataView(data.buffer, data.byteOffset, data.byteLength);\n    const keyLen = view.getUint32(0, true);\n    if (data.length < 4 + keyLen + 4) {\n      return null;\n    }\n    const valueLen = view.getUint32(4 + keyLen, true);\n    const isDeleted = valueLen === TOMBSTONE_MARKER;\n    const actualValueLen = isDeleted ? 0 : valueLen;\n    const recordLen = 4 + keyLen + 4 + actualValueLen + 4;\n    if (data.length < recordLen) {\n      return null;\n    }\n    const recordData = data.subarray(0, recordLen - 4);\n    const storedCrc = view.getUint32(recordLen - 4, true);\n    const calculatedCrc = crc32(recordData);\n    if (calculatedCrc !== storedCrc) {\n      return null;\n    }\n    const keyBytes = data.subarray(4, 4 + keyLen);\n    const key = new TextDecoder().decode(keyBytes);\n    if (isDeleted) {\n      return { key, value: undefined, isDeleted: true };\n    }\n    const valueBytes = data.subarray(4 + keyLen + 4, 4 + keyLen + 4 + valueLen);\n    const value = this.serializer.decode(valueBytes);\n    return { key, value, isDeleted: false };\n  }\n  ensureOpen() {\n    if (!this.opened) {\n      throw new Error(\"DiskKeyValueStore not open. Call open() first.\");\n    }\n  }\n}\nexport {\n  crc32,\n  ZstdCompressor,\n  TenantId,\n  StreamId,\n  SpiteDBNotOpenError,\n  SpiteDBError,\n  SpiteDB,\n  SortedIndex,\n  SegmentWriter,\n  SegmentReader,\n  SegmentManager,\n  SegmentIndex,\n  SEGMENT_VERSION,\n  SEGMENT_MAGIC,\n  SEGMENT_HEADER_SIZE,\n  Revision,\n  ProjectionsNotStartedError,\n  ProjectionRunner,\n  ProjectionNotFoundError,\n  ProjectionError,\n  ProjectionDisabledError,\n  ProjectionCoordinatorError,\n  ProjectionCoordinator,\n  ProjectionCatchUpTimeoutError,\n  ProjectionBuildError,\n  ProjectionBackpressureTimeoutError,\n  ProjectionBackpressureError,\n  ProjectionAlreadyRegisteredError,\n  NoopCompressor,\n  MsgpackSerializer,\n  InvalidStreamIdError,\n  InvalidSegmentHeaderError,\n  InvalidPositionError,\n  InvalidBatchError,\n  IndexCollection,\n  GlobalPosition,\n  FastEventSerializer,\n  EventStore,\n  EqualityIndex,\n  DiskKeyValueStore,\n  DenormalizedViewStore,\n  DefaultProjectionRegistry,\n  ConcurrencyError,\n  CommandId,\n  CheckpointWriteError,\n  CheckpointVersionError,\n  CheckpointManager,\n  CheckpointLoadError,\n  CheckpointCorruptionError,\n  CRC32Calculator,\n  CHECKPOINT_VERSION,\n  CHECKPOINT_MAGIC,\n  CHECKPOINT_HEADER_SIZE,\n  BunFileSystem,\n  BunClock,\n  BinaryEventBatchSerializer,\n  BatchId,\n  BatchChecksumError,\n  BATCH_MAGIC,\n  BATCH_HEADER_SIZE,\n  AggregatorStore\n};\n\n//# debugId=5CDDB4466536CA6564756E2164756E21\n";

// SpiteDB type declarations (3.3 KB)
export const SPITEDB_DTS = "/**\n * SpiteDB Runtime Type Declarations\n *\n * Auto-generated - do not edit manually.\n */\n\n/**\n * Input event for appending (without position info).\n */\nexport interface InputEvent {\n  /** Event type name */\n  type: string;\n  /** Event data payload */\n  data: unknown;\n  /** Optional event metadata */\n  metadata?: unknown;\n}\n\n/**\n * Options for appending events.\n */\nexport interface AppendOptions {\n  /**\n   * Expected stream revision (for optimistic concurrency).\n   * - undefined: No check (append unconditionally)\n   * - -1: Stream must not exist\n   * - >= 0: Stream must be at this revision\n   */\n  expectedRevision?: number;\n  /** Tenant ID for multi-tenancy (default: 'default') */\n  tenantId?: string;\n}\n\n/**\n * Result of appending events.\n */\nexport interface AppendResult {\n  /** New stream revision after append */\n  streamRevision: number;\n  /** Global position of first event in batch */\n  globalPosition: number;\n  /** Number of events appended */\n  eventCount: number;\n}\n\n/**\n * Options for reading streams.\n */\nexport interface ReadStreamOptions {\n  /** Start reading from this revision (inclusive, default: 0) */\n  fromRevision?: number;\n  /** Stop reading at this revision (inclusive) */\n  toRevision?: number;\n  /** Maximum number of events to return */\n  maxCount?: number;\n  /** Read direction (default: 'forward') */\n  direction?: 'forward' | 'backward';\n}\n\n/**\n * A stored event with all position and metadata.\n */\nexport interface StoredEvent {\n  /** Stream this event belongs to */\n  streamId: string;\n  /** Event type name */\n  type: string;\n  /** Event data payload */\n  data: unknown;\n  /** Optional event metadata */\n  metadata?: unknown;\n  /** Revision within the stream (0-based) */\n  revision: number;\n  /** Global position across all streams */\n  globalPosition: number;\n  /** Timestamp when the event was stored */\n  timestamp: number;\n  /** Tenant ID */\n  tenantId: string;\n}\n\n/**\n * Thrown when optimistic concurrency check fails.\n */\nexport declare class ConcurrencyError extends Error {\n  readonly streamId: string;\n  readonly expected: number;\n  readonly actual: number;\n  constructor(streamId: string, expected: number, actual: number);\n}\n\n/**\n * Configuration options for SpiteDB.\n */\nexport interface SpiteDBOptions {\n  /** Maximum segment size in bytes (default: 128MB) */\n  maxSegmentSize?: number;\n  /** Number of segment indexes to cache (default: 10) */\n  indexCacheSize?: number;\n  /** Auto-flush after this many events (default: 1000, 0 = disabled) */\n  autoFlushCount?: number;\n}\n\n/**\n * The main SpiteDB class.\n */\nexport declare class SpiteDB {\n  /**\n   * Open a SpiteDB instance at the given path.\n   */\n  static open(path: string, options?: SpiteDBOptions): Promise<SpiteDB>;\n\n  /**\n   * Append events to a stream.\n   */\n  append(\n    streamId: string,\n    events: InputEvent[],\n    options?: AppendOptions\n  ): Promise<AppendResult>;\n\n  /**\n   * Read events from a stream.\n   */\n  readStream(\n    streamId: string,\n    options?: ReadStreamOptions\n  ): Promise<StoredEvent[]>;\n\n  /**\n   * Flush pending events to disk.\n   */\n  flush(): Promise<void>;\n\n  /**\n   * Close the database.\n   */\n  close(): Promise<void>;\n\n  /**\n   * Check if the database is open.\n   */\n  isOpen(): boolean;\n\n  /**\n   * Get the current stream revision.\n   */\n  getStreamRevision(streamId: string): number;\n}\n";
